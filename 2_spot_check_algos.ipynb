{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A0XMzfB5_EtE"
   },
   "source": [
    "# Sentiment analysis of movie (IMDB) reviews \n",
    "\n",
    "using dataset provided by the ACL 2011 paper, see http://ai.stanford.edu/~amaas/data/sentiment/.\n",
    "\n",
    "Dataset can be downloaded separately from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz, but wont be necessary as the download process has been embedded in the notebook and source file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents<a class=\"anchor\" id=\"table\"></a>\n",
    "* [Data exploration](#data_e)\n",
    "* [Modelling](#model)\n",
    "    * [Compare vectorisation techniques](#compare)\n",
    "    * [Logistic Regression](#logis)\n",
    "        * [Logistic Regression with bag of word](#logis_bag)\n",
    "        * [Logistic Regression with TFIDF](#logis_tfidf)\n",
    "    * [Random Forest](#randfor)\n",
    "        * [Random Forest with Hashing](#rand_for_bag)\n",
    "        * [Random Forest with TFIDF](#rand_for_tfidf)\n",
    "    * [Linear SVC](#linearsvc)\n",
    "        * [Random Forest with Hashing](#linear_bag)\n",
    "        * [Random Forest with TFIDF](#linear_tfidf)\n",
    "* [Display scores for all trained models](#disp_sco)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "colab_type": "code",
    "id": "CML_IG6z-iwM",
    "outputId": "d9301f36-c1cf-4b3f-e639-a731880ed036"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /anaconda3/lib/python3.7/site-packages (3.4)\n",
      "Requirement already satisfied: wget in /anaconda3/lib/python3.7/site-packages (3.2)\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.7/site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: singledispatch in /anaconda3/lib/python3.7/site-packages (from nltk) (3.4.0.3)\n",
      "Requirement already up-to-date: gensim in /anaconda3/lib/python3.7/site-packages (3.7.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /anaconda3/lib/python3.7/site-packages (from gensim) (1.16.2)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /anaconda3/lib/python3.7/site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.7.0 in /anaconda3/lib/python3.7/site-packages (from gensim) (1.8.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /anaconda3/lib/python3.7/site-packages (from gensim) (1.2.1)\n",
      "Requirement already satisfied, skipping upgrade: boto>=2.32 in /anaconda3/lib/python3.7/site-packages (from smart-open>=1.7.0->gensim) (2.49.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /anaconda3/lib/python3.7/site-packages (from smart-open>=1.7.0->gensim) (2.21.0)\n",
      "Requirement already satisfied, skipping upgrade: bz2file in /anaconda3/lib/python3.7/site-packages (from smart-open>=1.7.0->gensim) (0.98)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in /anaconda3/lib/python3.7/site-packages (from smart-open>=1.7.0->gensim) (1.9.108)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim) (2018.11.29)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim) (1.24.1)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.7.0->gensim) (0.9.4)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in /anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.7.0->gensim) (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.13.0,>=1.12.108 in /anaconda3/lib/python3.7/site-packages (from boto3->smart-open>=1.7.0->gensim) (1.12.108)\n",
      "Requirement already satisfied, skipping upgrade: docutils>=0.10 in /anaconda3/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.108->boto3->smart-open>=1.7.0->gensim) (0.14)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /anaconda3/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.108->boto3->smart-open>=1.7.0->gensim) (2.8.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/swami/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# uncomment these for Google collab, will have already been installed in local environment \n",
    "# if 'pip install -r requirements.txt' has been run\n",
    "# !pip install nltk wget\n",
    "# !pip install --upgrade gensim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import nltk\n",
    "\n",
    "\n",
    "import glob\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "lfr3bXOgXNJJ",
    "outputId": "cc06dd0d-e886-4090-c972-cd05520adaf3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FJiWamI00hBp",
    "outputId": "e3c105d5-037f-4521-b8e1-a83cb7a5edeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory exists, taking no action\n"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "import tarfile\n",
    "\n",
    "# By checking if the directory exists first, we allow people to delete the tarfile without the notebook re-downloading it\n",
    "if os.path.isdir('aclImdb'):\n",
    "    print(\"Dataset directory exists, taking no action\")\n",
    "else:    \n",
    "    if not os.path.isfile('aclImdb_v1.tar.gz'):\n",
    "        print(\"Downloading dataset\")\n",
    "        #!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "        wget.download('http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz')\n",
    "    else:\n",
    "        print(\"Dataset already downloaded\")\n",
    "    \n",
    "    print(\"Unpacking dataset\")\n",
    "    #!tar -xf aclImdb_v1.tar.gz \n",
    "    tar = tarfile.open(\"aclImdb_v1.tar.gz\")\n",
    "    tar.extractall()\n",
    "    tar.close()\n",
    "    print(\"Dataset unpacked in aclImdb\")\n",
    "\n",
    "import re\n",
    "\n",
    "# load doc into memory\n",
    "# regex to clean markup elements \n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', encoding='utf8')\n",
    "    # read all text\n",
    "    text = re.sub('<[^>]*>', ' ', file.read())\n",
    "    #text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook configuration\n",
    "# SAMPLE_SIZE=12500\n",
    "SAMPLE_SIZE=50000\n",
    "# SAMPLE_SIZE=1000\n",
    "# TRAIN_TEST_RATIO=0.75\n",
    "TRAIN_TEST_RATIO=0.50\n",
    "TRAINING_SET_SIZE=int(SAMPLE_SIZE * (1 - TRAIN_TEST_RATIO))\n",
    "VALIDATION_SET_SIZE=int(SAMPLE_SIZE * TRAIN_TEST_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bcTVSZK-i07N"
   },
   "outputs": [],
   "source": [
    "dataframes={}\n",
    "for type in [\"train\",\"test\"]:\n",
    "    if type==\"train\":\n",
    "        SLICE = int(TRAINING_SET_SIZE / 2)\n",
    "    elif type==\"test\":\n",
    "        SLICE = int(VALIDATION_SET_SIZE / 2)\n",
    "\n",
    "    positive_file_list = glob.glob(os.path.join('aclImdb/'+type+'/pos', \"*.txt\"))\n",
    "    positive_sample_file_list = positive_file_list[:SLICE]\n",
    "    df_positive = pd.DataFrame({'reviews':[load_doc(x) for x in positive_sample_file_list], 'sentiment': np.ones(SLICE)})\n",
    "    \n",
    "    negative_file_list = glob.glob(os.path.join('aclImdb/'+type+'/neg', \"*.txt\"))\n",
    "    negative_sample_file_list = negative_file_list[:SLICE]\n",
    "    df_negative = pd.DataFrame({'reviews':[load_doc(x) for x in negative_sample_file_list], 'sentiment': np.zeros(SLICE)})\n",
    "\n",
    "    dataframes[type]=pd.concat([df_positive,df_negative])\n",
    "    dataframes[type]=shuffle(dataframes[type])\n",
    "\n",
    "df=pd.concat([dataframes[\"train\"],dataframes[\"test\"]])\n",
    "    \n",
    "X_train=dataframes[\"train\"]['reviews']\n",
    "y_train=dataframes[\"train\"]['sentiment']\n",
    "\n",
    "X_test=dataframes[\"test\"]['reviews']\n",
    "y_test=dataframes[\"test\"]['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "(25000,)\n",
      "[0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dnu21deYOkDE"
   },
   "source": [
    "<a href='#table'>Back</a>\n",
    "# Modelling <a class=\"anchor\" id=\"model\"></a> \n",
    "\n",
    "We have decided to do the use the below models and vectorisation techniques to test our their accuracy / score, the idea is to use a one model and one vectorization technique and plot a score.\n",
    "\n",
    "**Vectorisation techniques**\n",
    "- Bag of Words\n",
    "- TFIDF (weighted bag of words) http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "- mapping words to plain integers\n",
    "- mapping words to embedding vectors\n",
    "\n",
    "\n",
    "**Simple models**\n",
    "\n",
    "- Logistic Regression\n",
    "- Random Forst\n",
    "- LSTM\n",
    "- GRU\n",
    "- CNN\n",
    "\n",
    "**Embeddings to try**\n",
    "- Word2Vec\n",
    "- FastText\n",
    "- Glove\n",
    "\n",
    "**APIs we used**\n",
    "- Introducing Pipeline: http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "- Introducing cross_val_score http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q-BsaFCwXNJQ"
   },
   "source": [
    "## Compare vectorisation techniques<a class=\"anchor\" id=\"compare\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<50000x101888 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 6797241 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVec = CountVectorizer()\n",
    "# Creating the BoW with the set of all the documents and transforming the documents in feature vectors\n",
    "bag_of_words=CountVec.fit_transform(df['reviews'])\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<50000x101888 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6793600 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashing = HashingVectorizer(strip_accents='ascii', lowercase=True, preprocessor=None,n_features=bag_of_words.shape[1])\n",
    "reviews_hashing=hashing.fit_transform(df['reviews'])\n",
    "reviews_hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<50000x101888 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6797188 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents='ascii', lowercase=True, preprocessor=None,max_features=bag_of_words.shape[1])\n",
    "reviews_tfidf=tfidf.fit_transform(df['reviews'])\n",
    "reviews_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mGh1Vqp3XNJI"
   },
   "source": [
    "## Logistic Regression <a class=\"anchor\" id=\"logis\"></a> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q-BsaFCwXNJQ"
   },
   "source": [
    "### Logistic Regress model using Bag of Words vectorisation technique<a class=\"anchor\" id=\"logis_bag\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "colab_type": "code",
    "id": "XtV8N-0zXNJ4",
    "outputId": "d76d5eea-9796-4eb7-a158-1f6ccdcd3db9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 0.88008\n",
      "Test accuracy 0.85996\n",
      "Time to run: 20.193513870239258\n"
     ]
    }
   ],
   "source": [
    "time_beginning_of_training = time.time()\n",
    "\n",
    "model=LogisticRegression(random_state=0)\n",
    "model.fit(hashing.fit_transform(X_train), y_train)\n",
    "train_acc=model.score(hashing.transform(X_train), y_train)\n",
    "print('Train accuracy {}'.format(train_acc))\n",
    "test_acc=model.score(hashing.transform(X_test), y_test)\n",
    "print('Test accuracy {}'.format(test_acc))\n",
    "\n",
    "time_end_of_training = time.time()\n",
    "print('Time to run: {}'.format(time_end_of_training-time_beginning_of_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             title  sample_size      acc  val_acc\n",
      "0  log reg Hashing        50000  0.88008  0.85996\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({})\n",
    "df.reset_index(inplace=True)\n",
    "df[\"title\"]=[\"log reg Hashing\"]\n",
    "df[\"sample_size\"]=[SAMPLE_SIZE]\n",
    "df[\"acc\"]=train_acc\n",
    "df[\"val_acc\"]=test_acc\n",
    "df.drop(labels=\"index\",axis=1,inplace=True)\n",
    "print(df)\n",
    "results=pd.concat([df,results])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "# # Trying with cross_val_score\n",
    "lr = LogisticRegression()\n",
    "k_folds = 10\n",
    "X_train_CV = CountVec.fit_transform(X_train)\n",
    "type(X_train_CV)\n",
    "print('Train accuracy list {} '.format(cross_val_score(lr, X_train_CV, y_train, cv= k_folds))) \n",
    "print('Train accuracy mean {} '.format(cross_val_score(lr, X_train_CV, y_train, cv= k_folds).mean()))\n",
    "\n",
    "scores.append([\"Logistic Reg with BoW\",train_acc,test_acc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YeUltp6gXNJd"
   },
   "source": [
    "### Logistic Regress model using TfidfVectorizer vectorisation technique<a class=\"anchor\" id=\"logis_tfidf\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "colab_type": "code",
    "id": "XtV8N-0zXNJ4",
    "outputId": "d76d5eea-9796-4eb7-a158-1f6ccdcd3db9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 0.9332\n",
      "Test accuracy 0.883\n",
      "Time to run: 26.110776901245117\n"
     ]
    }
   ],
   "source": [
    "time_beginning_of_training = time.time()\n",
    "\n",
    "model=LogisticRegression(random_state=0)\n",
    "model.fit(tfidf.fit_transform(X_train), y_train)\n",
    "train_acc=model.score(tfidf.transform(X_train), y_train)\n",
    "print('Train accuracy {}'.format(train_acc))\n",
    "test_acc=model.score(tfidf.transform(X_test), y_test)\n",
    "print('Test accuracy {}'.format(test_acc))\n",
    "\n",
    "time_end_of_training = time.time()\n",
    "print('Time to run: {}'.format(time_end_of_training-time_beginning_of_training))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           title  sample_size     acc  val_acc\n",
      "0  log reg Tfidf        50000  0.9332    0.883\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({})\n",
    "df.reset_index(inplace=True)\n",
    "df[\"title\"]=[\"log reg Tfidf\"]\n",
    "df[\"sample_size\"]=[SAMPLE_SIZE]\n",
    "df[\"acc\"]=train_acc\n",
    "df[\"val_acc\"]=test_acc\n",
    "df.drop(labels=\"index\",axis=1,inplace=True)\n",
    "print(df)\n",
    "results=pd.concat([df,results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P0YIRBaroMeM"
   },
   "source": [
    "## RandomForestClassifer<a class=\"anchor\" id=\"rand_for\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BM0YI71dnpZE"
   },
   "source": [
    "### RandomForestClassifer with hashing<a class=\"anchor\" id=\"rand_for_bag\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "colab_type": "code",
    "id": "XtV8N-0zXNJ4",
    "outputId": "d76d5eea-9796-4eb7-a158-1f6ccdcd3db9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 0.99252\n",
      "Test accuracy 0.72524\n",
      "Time to run: 29.536765098571777\n"
     ]
    }
   ],
   "source": [
    "time_beginning_of_training = time.time()\n",
    "\n",
    "model=RandomForestClassifier(random_state=0)\n",
    "model.fit(hashing.fit_transform(X_train), y_train)\n",
    "train_acc=model.score(hashing.transform(X_train), y_train)\n",
    "print('Train accuracy {}'.format(train_acc))\n",
    "test_acc=model.score(hashing.transform(X_test), y_test)\n",
    "print('Test accuracy {}'.format(test_acc))\n",
    "\n",
    "time_end_of_training = time.time()\n",
    "print('Time to run: {}'.format(time_end_of_training-time_beginning_of_training))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              title  sample_size      acc  val_acc\n",
      "0  Rand for hashing        50000  0.99252  0.72524\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({})\n",
    "df.reset_index(inplace=True)\n",
    "df[\"title\"]=[\"Rand for hashing\"]\n",
    "df[\"sample_size\"]=[SAMPLE_SIZE]\n",
    "df[\"acc\"]=train_acc\n",
    "df[\"val_acc\"]=test_acc\n",
    "df.drop(labels=\"index\",axis=1,inplace=True)\n",
    "print(df)\n",
    "results=pd.concat([df,results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BM0YI71dnpZE"
   },
   "source": [
    "### RandomForestClassifer with TFIDF<a class=\"anchor\" id=\"rand_for_tfidf\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "colab_type": "code",
    "id": "XtV8N-0zXNJ4",
    "outputId": "d76d5eea-9796-4eb7-a158-1f6ccdcd3db9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 0.9934\n",
      "Test accuracy 0.73332\n",
      "Time to run: 31.585236072540283\n"
     ]
    }
   ],
   "source": [
    "time_beginning_of_training = time.time()\n",
    "\n",
    "model=RandomForestClassifier(random_state=0)\n",
    "model.fit(tfidf.fit_transform(X_train), y_train)\n",
    "train_acc=model.score(tfidf.transform(X_train), y_train)\n",
    "print('Train accuracy {}'.format(train_acc))\n",
    "test_acc=model.score(tfidf.transform(X_test), y_test)\n",
    "print('Test accuracy {}'.format(test_acc))\n",
    "\n",
    "time_end_of_training = time.time()\n",
    "print('Time to run: {}'.format(time_end_of_training-time_beginning_of_training))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            title  sample_size     acc  val_acc\n",
      "0  Rand for tfidf        50000  0.9934  0.73332\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({})\n",
    "df.reset_index(inplace=True)\n",
    "df[\"title\"]=[\"Rand for tfidf\"]\n",
    "df[\"sample_size\"]=[SAMPLE_SIZE]\n",
    "df[\"acc\"]=train_acc\n",
    "df[\"val_acc\"]=test_acc\n",
    "df.drop(labels=\"index\",axis=1,inplace=True)\n",
    "print(df)\n",
    "results=pd.concat([df,results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P0YIRBaroMeM"
   },
   "source": [
    "## Linear SVC <a class=\"anchor\" id=\"linearsvc\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BM0YI71dnpZE"
   },
   "source": [
    "### Linear SVC with hashing<a class=\"anchor\" id=\"linear_bag\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "colab_type": "code",
    "id": "XtV8N-0zXNJ4",
    "outputId": "d76d5eea-9796-4eb7-a158-1f6ccdcd3db9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 0.9408\n",
      "Test accuracy 0.88344\n",
      "Time to run: 18.58791208267212\n"
     ]
    }
   ],
   "source": [
    "time_beginning_of_training = time.time()\n",
    "\n",
    "model=LinearSVC(random_state=0)\n",
    "model.fit(hashing.fit_transform(X_train), y_train)\n",
    "train_acc=model.score(hashing.transform(X_train), y_train)\n",
    "print('Train accuracy {}'.format(train_acc))\n",
    "test_acc=model.score(hashing.transform(X_test), y_test)\n",
    "print('Test accuracy {}'.format(test_acc))\n",
    "\n",
    "time_end_of_training = time.time()\n",
    "print('Time to run: {}'.format(time_end_of_training-time_beginning_of_training))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               title  sample_size     acc  val_acc\n",
      "0  LinearSVC hashing        50000  0.9408  0.88344\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({})\n",
    "df.reset_index(inplace=True)\n",
    "df[\"title\"]=[\"LinearSVC hashing\"]\n",
    "df[\"sample_size\"]=[SAMPLE_SIZE]\n",
    "df[\"acc\"]=train_acc\n",
    "df[\"val_acc\"]=test_acc\n",
    "df.drop(labels=\"index\",axis=1,inplace=True)\n",
    "print(df)\n",
    "results=pd.concat([df,results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BM0YI71dnpZE"
   },
   "source": [
    "### Linear SVC with TFIDF<a class=\"anchor\" id=\"linearsvc_tfidf\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "colab_type": "code",
    "id": "XtV8N-0zXNJ4",
    "outputId": "d76d5eea-9796-4eb7-a158-1f6ccdcd3db9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 0.99008\n",
      "Test accuracy 0.87716\n",
      "Time to run: 22.858220100402832\n"
     ]
    }
   ],
   "source": [
    "time_beginning_of_training = time.time()\n",
    "\n",
    "model=LinearSVC(random_state=0)\n",
    "model.fit(tfidf.fit_transform(X_train), y_train)\n",
    "train_acc=model.score(tfidf.transform(X_train), y_train)\n",
    "print('Train accuracy {}'.format(train_acc))\n",
    "test_acc=model.score(tfidf.transform(X_test), y_test)\n",
    "print('Test accuracy {}'.format(test_acc))\n",
    "\n",
    "time_end_of_training = time.time()\n",
    "print('Time to run: {}'.format(time_end_of_training-time_beginning_of_training))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({})\n",
    "df.reset_index(inplace=True)\n",
    "df[\"title\"]=[\"LinearSVC tfidf\"]\n",
    "df[\"sample_size\"]=[SAMPLE_SIZE]\n",
    "df[\"acc\"]=train_acc\n",
    "df[\"val_acc\"]=test_acc\n",
    "df.drop(labels=\"index\",axis=1,inplace=True)\n",
    "print(df)\n",
    "results=pd.concat([df,results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BM0YI71dnpZE"
   },
   "source": [
    "# Display scores for all trained models<a class=\"anchor\" id=\"disp_sco\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! apt-get install tk-dev || true\n",
    "! brew install tk-dev\n",
    "! pip3 --yes uninstall matplotlib\n",
    "! pip3 --no-cache-dir install -U matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc\tbatch_size\tcategorical_accuracy\tloss\tnb_epochs\tsample_size\ttitle\ttitle_short\ttrain_acc\ttrain_test_ratio\ttraining_duration_in_secs\ttraining_set_size\tval_acc\tval_categorical_accuracy\tval_loss\tvalidation_set_size\tvocab_size\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results.title,results.acc)\n",
    "plt.plot(results.title,results.val_acc)\n",
    "plt.rcParams[\"figure.figsize\"] = [17,2]\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(path_or_buf=\"reports/simple_ml_models.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Sentiment analysis of movies (IMDB).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
