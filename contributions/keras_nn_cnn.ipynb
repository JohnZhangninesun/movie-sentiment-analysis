{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Sentiment Analysis with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebok is based on the following article: https://machinelearningmastery.com/predict-sentiment-movie-reviews-using-deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keras.datasets.imdb.load_data() allows you to load the dataset in a format that is ready for use in neural network and deep learning models.\n",
    "The words have been replaced by integers that indicate the absolute popularity of the word in the dataset. The sentences in each review therefore consist of a sequence of integers.\n",
    "This contrasts with how we have been loading the data set in other notebooks, where we have to prepare the data before it can be split into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP for the IMDB problem\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.preprocessing import sequence\n",
    "from matplotlib import pyplot\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data()\n",
    "X = numpy.concatenate((X_train, X_test), axis=0)\n",
    "y = numpy.concatenate((y_train, y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: \n",
      "(50000,)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "# summarize size\n",
    "print(\"Training data: \")\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: \n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "# Summarize number of classes\n",
    "print(\"Classes: \")\n",
    "print(numpy.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: \n",
      "88585\n"
     ]
    }
   ],
   "source": [
    "# Summarize number of words\n",
    "print(\"Number of words: \")\n",
    "print(len(numpy.unique(numpy.hstack(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review length: \n",
      "Mean 234.76 words (172.911495)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summarize review length\n",
    "print(\"Review length: \")\n",
    "result = [len(x) for x in X]\n",
    "print(\"Mean %.2f words (%f)\" % (numpy.mean(result), numpy.std(result)))\n",
    "# plot review length\n",
    "pyplot.boxplot(result)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "Word Embedding is a technique where words are encoded as real-valued vectors in a high-dimensional space, where the similarity between words in terms of meaning translates to closeness in the vector space.\n",
    "Keras provides a convenient way to convert positive integer representations of words into a word embedding by an Embedding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify what follows the IMDB dataset is reloaded, but only the top 5,000 words will be loaded.\n",
    "The dataset is into training and test based a 50%/50% split, but it is probably more usual to use an 80/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bound reviews at 500 words, truncating longer reviews and zero-padding shorter reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model below uses an Embedding layer as the input layer, setting the vocabulary to 5,000, the word vector size to 32 dimensions and the input_length to 500. The output of this first layer will be a 32×500 sized matrix.\n",
    "\n",
    "Then the Embedded layers are flattened to produce an output of one dimension, which then uses a dense hidden layer of 250 units with a rectifier activation function. The output layer has one neuron and uses a sigmoid activation to output values of 0 and 1 as predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               4000250   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 4,160,501\n",
      "Trainable params: 4,160,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      " - 10s - loss: 0.5102 - acc: 0.7109 - val_loss: 0.3090 - val_acc: 0.8667\n",
      "Epoch 2/2\n",
      " - 10s - loss: 0.1903 - acc: 0.9276 - val_loss: 0.3003 - val_acc: 0.8731\n",
      "Accuracy: 87.31%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0) \n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better result (0.8825) can be obtained if the output layer is changed to softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 250)               4000250   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 4,160,501\n",
      "Trainable params: 4,160,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='softmax'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      " - 11s - loss: 0.6513 - acc: 0.6832 - val_loss: 0.5690 - val_acc: 0.8662\n",
      "Epoch 2/2\n",
      " - 10s - loss: 0.5153 - acc: 0.8902 - val_loss: 0.4873 - val_acc: 0.8825\n",
      "Accuracy: 88.25%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model again\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0) \n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 250)               2000250   \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 2,163,605\n",
      "Trainable params: 2,163,605\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='softmax'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      " - 12s - loss: 0.6443 - acc: 0.7017 - val_loss: 0.5711 - val_acc: 0.8591\n",
      "Epoch 2/2\n",
      " - 12s - loss: 0.5208 - acc: 0.8920 - val_loss: 0.4956 - val_acc: 0.8846\n",
      "Accuracy: 88.46%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why did the CNN improve the accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it only improved it by a tiny amount. Perhaps the CNNs are better at capturing the spatial relationships or the LSTM needs to be larger and trained for longer to achieve the same accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the denser layer with 250 neurons reduces the number of parameters needed to be trained drastically with an increased accuracy of about 1% over 5 epochs. Maybe the 2 dense layers after flatten layer should not be there."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
