{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A0XMzfB5_EtE"
   },
   "source": [
    "### Sentiment analysis of movie (IMDB) reviews using dataset provided by the ACL 2011 paper, see http://ai.stanford.edu/~amaas/data/sentiment/.\n",
    "\n",
    "#### Dataset can be downloaded separately from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz, but wont be necessary as the download process has been embedded in the notebook and source file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "colab_type": "code",
    "id": "CML_IG6z-iwM",
    "outputId": "d9301f36-c1cf-4b3f-e639-a731880ed036"
   },
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install --upgrade gensim\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import nltk\n",
    "\n",
    "\n",
    "import glob\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FJiWamI00hBp",
    "outputId": "e3c105d5-037f-4521-b8e1-a83cb7a5edeb"
   },
   "outputs": [],
   "source": [
    "# MacOSX: See https://www.mkyong.com/mac/wget-on-mac-os-x/ for wget\n",
    "if not os.path.isdir('aclImdb'):\n",
    "    if not os.path.isfile('aclImdb_v1.tar.gz'):\n",
    "      !wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz \n",
    "\n",
    "    if not os.path.isdir('aclImdb'):  \n",
    "      !tar -xf aclImdb_v1.tar.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U5Tnmoh-Dpfk"
   },
   "outputs": [],
   "source": [
    "time_beginning_of_notebook = time.time()\n",
    "SAMPLE_SIZE=1000\n",
    "positive_sample_file_list = glob.glob(os.path.join('aclImdb/train/pos', \"*.txt\"))\n",
    "positive_sample_file_list = positive_sample_file_list[:SAMPLE_SIZE]\n",
    "\n",
    "negative_sample_file_list = glob.glob(os.path.join('aclImdb/train/neg', \"*.txt\"))\n",
    "negative_sample_file_list = negative_sample_file_list[:SAMPLE_SIZE]\n",
    "\n",
    "import re\n",
    "\n",
    "# load doc into memory\n",
    "# regex to clean markup elements \n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', encoding='utf8')\n",
    "    # read all text\n",
    "    text = re.sub('<[^>]*>', ' ', file.read())\n",
    "    #text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vMYBkcdIB9uc"
   },
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cz5eJi7AGSqR"
   },
   "outputs": [],
   "source": [
    "positive_strings = [load_doc(x) for x in positive_sample_file_list]\n",
    "negative_strings = [load_doc(x) for x in negative_sample_file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C0WiHTr7I4CN"
   },
   "outputs": [],
   "source": [
    "positive_tokenized = [word_tokenize(s) for s in positive_strings]\n",
    "negative_tokenized = [word_tokenize(s) for s in negative_strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "6bgN1KJRMPpq",
    "outputId": "aa69f5b0-246f-4805-bc36-9944c1f36b3a"
   },
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "with open('aclImdb/imdb.vocab', encoding='utf8') as f:\n",
    "    #content = f.readlines()\n",
    "    universe_vocabulary = [x.strip() for x in f.readlines()]\n",
    "\n",
    "print(\"Word count across all reviews (before stripping tokens):\", sum([len(token) for token in positive_tokenized]))\n",
    "\n",
    "#Checking the not alphanumeric characters in vocabulary\n",
    "non_alphanumeric_set = set()\n",
    "for word in universe_vocabulary:\n",
    "    non_alphanumeric_set |= set(re.findall('\\W', word))\n",
    "print('Non alphanumeric characters found in universe vocabulary', non_alphanumeric_set)\n",
    "\n",
    "\n",
    "stripped_positive_tokenized = []\n",
    "for tokens in positive_tokenized:\n",
    "  stripped_positive_tokenized.append([token.lower() for token in tokens if token.lower() in universe_vocabulary])\n",
    "\n",
    "print(\"Word count across all reviews (after stripping tokens):\", sum([len(token) for token in stripped_positive_tokenized]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "DSFWrZInMueS",
    "outputId": "4ae1eab3-3e09-41a5-9778-22aa72ce5cdb"
   },
   "outputs": [],
   "source": [
    "print(\"Word count across all reviews (before stripping tokens):\", sum([len(token) for token in positive_tokenized]))\n",
    "stripped_negative_tokenized = []\n",
    "for tokens in negative_tokenized:\n",
    "  stripped_negative_tokenized.append([token.lower() for token in tokens if token.lower() in universe_vocabulary])\n",
    "\n",
    "print(\"Word count across all reviews (after stripping tokens):\", sum([len(token) for token in stripped_negative_tokenized]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dnu21deYOkDE"
   },
   "source": [
    "## Modelling \n",
    "\n",
    "We have decided to do the use the below models and vectorisation techniques to test our their accuracy / score, the idea is to use a one model and one vectorization technique and plot a score.\n",
    "\n",
    "**Simple models**\n",
    "\n",
    "- Logistic Regression\n",
    "- Random Forst\n",
    "- LSTM\n",
    "- GRU\n",
    "- CNN\n",
    "\n",
    "**Vectorisation techniques**\n",
    "- Bag of Words\n",
    "- Word2Vec\n",
    "- TFIDF (probability scores)\n",
    "- FastText\n",
    "- Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mGh1Vqp3XNJI"
   },
   "source": [
    "## Logistic Regression\n",
    "### Introducing Pipeline: \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "### Introducing TfdfVectorizer: \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "### Introducing cross_val_score \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "lfr3bXOgXNJJ",
    "outputId": "cc06dd0d-e886-4090-c972-cd05520adaf3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df_positives = pd.DataFrame({'reviews':[load_doc(x) for x in positive_sample_file_list], 'sentiment': np.ones(SAMPLE_SIZE)})\n",
    "df_negatives = pd.DataFrame({'reviews':[load_doc(x) for x in negative_sample_file_list], 'sentiment': np.zeros(SAMPLE_SIZE)})\n",
    "\n",
    "print(\"Positive review(s):\", df_positives['reviews'][1][:100])\n",
    "print(\"Negative review(s):\", df_negatives['reviews'][1][:100])\n",
    "\n",
    "df = pd.concat([df_positives, df_negatives], ignore_index=True)\n",
    "\n",
    "df = shuffle(df)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['reviews'], df['sentiment'], test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the BoW with the set of all the documents and transforming the documents in feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CountVec = CountVectorizer()\n",
    "bag_of_words = CountVec.fit_transform(df['reviews'])\n",
    "print(type(bag_of_words))\n",
    "print('\\n Number of rows {} (documents) -- Number of columns {} (vocabulary) \\n'.format(bag_of_words.shape[0], bag_of_words.shape[1]))\n",
    "print('\\n Type of bag_of_words {} \\n'.format(type(bag_of_words)))\n",
    "sparsity = 1.0 - bag_of_words.nnz / (bag_of_words.shape[0] * bag_of_words.shape[1])\n",
    "print('\\n Sparsity {} \\n'.format(sparsity))\n",
    "\n",
    "print('\\n Type of bag_of_words.toarray {} \\n'.format(type(bag_of_words.toarray())))\n",
    "\n",
    "print('\\n Type of CountVec.vocabulary {} \\n'.format(type(CountVec.vocabulary_)))\n",
    "print('A sample of CountVec.vocabulary_ {}'.format([(k, v) for k, v in CountVec.vocabulary_.items() if v < 1000]))\n",
    "print('\\n Type of bag_of_words.toarray() {} \\n')\n",
    "print('\\n First feature vector, representing the first document \\n', bag_of_words[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "DIF9pizTcpWm"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(bag_of_words.toarray(), df['sentiment'].values, test_size=0.25)\n",
    "\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "results = [(predicted, actual) for predicted, actual in zip(clf.predict(X_test),  y_test) \n",
    "           if  predicted == actual]\n",
    "\n",
    "print('Percentage of correct predicted values{}'.format(len(results)/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q-BsaFCwXNJQ"
   },
   "source": [
    "## Logistic Regress model using Bag of Words vectorisation technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "-S-I4Fc2XNJS",
    "outputId": "7158b946-8734-47df-d4f4-9318b9d2a0c2"
   },
   "outputs": [],
   "source": [
    "countvec = CountVectorizer()\n",
    "lr_CV = Pipeline([('vect', countvec), ('tfidf', TfidfTransformer()), ('clf', LogisticRegression(random_state=0))])\n",
    "lr_CV.fit(X_train, y_train)\n",
    "print('Train accuracy {}'.format(lr_CV.score(X_train, y_train)))\n",
    "print('Test accuracy {}'.format(lr_CV.score(X_test, y_test)))\n",
    "\n",
    "# Trying with cross_val_score\n",
    "lr = LogisticRegression()\n",
    "k_folds = 10\n",
    "X_train_CV = countvec.fit_transform(X_train)\n",
    "type(X_train_CV)\n",
    "print('Train accuracy list {} '.format(cross_val_score(lr, X_train_CV, y_train, cv= k_folds))) \n",
    "print('Train accuracy mean {} '.format(cross_val_score(lr, X_train_CV, y_train, cv= k_folds).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YeUltp6gXNJd"
   },
   "source": [
    "## Logistic Regress model using TfidfVectorizer vectorisation technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "s16mkhktXNJf",
    "outputId": "6fce5e0d-1d45-432e-e7cb-f1fd5972bc42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 0.97\n",
      "Test accuracy 0.826\n",
      "Train accuracy list [0.8410596  0.81456954 0.80794702 0.84768212 0.75333333 0.84\n",
      " 0.7852349  0.83892617 0.80536913 0.81879195] \n",
      "Train accuracy mean 0.8152913759130035 \n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf), ('clf', LogisticRegression(random_state=0))])\n",
    "lr_tfidf.fit(X_train, y_train)\n",
    "print('Train accuracy {}'.format(lr_tfidf.score(X_train, y_train)))\n",
    "print('Test accuracy {}'.format(lr_tfidf.score(X_test, y_test)))\n",
    "\n",
    "# Trying with cross_val_score\n",
    "lr = LogisticRegression()\n",
    "k_folds = 10\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "print('Train accuracy list {} '.format(cross_val_score(lr, X_train_tfidf, y_train, cv= k_folds))) \n",
    "print('Train accuracy mean {} '.format(cross_val_score(lr, X_train_tfidf, y_train, cv= k_folds).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e2lMqoGKXNJl"
   },
   "source": [
    "## Logistic Regress model using TfidfVectorizer and different values for C hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "zZZOXbA4XNJm",
    "outputId": "518f52ab-2ccd-4bca-9105-5fdcf5c797ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_value 1.0 Test Score 0.832 Train_score 0.9686666666666667\n",
      "C_value 1.1 Test Score 0.834 Train_score 0.9733333333333334\n",
      "C_value 1.2000000000000002 Test Score 0.834 Train_score 0.9786666666666667\n",
      "C_value 1.3000000000000003 Test Score 0.834 Train_score 0.98\n",
      "C_value 1.4000000000000004 Test Score 0.834 Train_score 0.9826666666666667\n",
      "C_value 1.5000000000000004 Test Score 0.834 Train_score 0.9846666666666667\n",
      "C_value 1.6000000000000005 Test Score 0.832 Train_score 0.9853333333333333\n",
      "C_value 1.7000000000000006 Test Score 0.834 Train_score 0.9866666666666667\n",
      "C_value 1.8000000000000007 Test Score 0.836 Train_score 0.988\n",
      "C_value 1.9000000000000008 Test Score 0.836 Train_score 0.988\n"
     ]
    }
   ],
   "source": [
    "C_values = np.arange(1,2,0.1)\n",
    "results = []\n",
    "\n",
    "for value in C_values:   \n",
    "    lr_tfidf = Pipeline([('vect', tfidf), ('clf', LogisticRegression(random_state=0, C=value))])\n",
    "    lr_tfidf.fit(X_train, y_train)\n",
    "    train_score = lr_tfidf.score(X_train, y_train)\n",
    "    score = lr_tfidf.score(X_test, y_test)\n",
    "    print('C_value {} Test Score {} Train_score {}'.format(value, score, train_score))\n",
    "    results.append(score)\n",
    "\n",
    "time_end_of_notebook = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 186
    },
    "colab_type": "code",
    "id": "6siRLHQU79F7",
    "outputId": "dbc21ee2-1066-4b68-c8a0-ad47ca57f4b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 1000\n",
      "Full notebook execution duration: 6434.627096891403 seconds\n",
      "Full notebook execution duration: 107.24378494819005 minutes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>Vectorisation techniques</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>0.836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>Pending</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>TFIDF</td>\n",
       "      <td>0.988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Models Vectorisation techniques    Score\n",
       "0  Logistic Regression             Bag of Words    0.836\n",
       "1  Logistic Regression                 Word2Vec  Pending\n",
       "2  Logistic Regression                    TFIDF    0.988"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_models_vectorization = pd.DataFrame(\n",
    "     {'Models':                   [\"Logistic Regression\", \"Logistic Regression\", \"Logistic Regression\"], \n",
    "      'Vectorisation techniques': [\"Bag of Words\",        \"Word2Vec\", \"TFIDF\"], \n",
    "      'Score':                    [score,                 \"Pending\", lr_tfidf.score(X_train, y_train) ]},\n",
    "    columns=['Models','Vectorisation techniques','Score']\n",
    ")\n",
    "print(\"Sample size:\", SAMPLE_SIZE)\n",
    "\n",
    "duration = time_end_of_notebook - time_beginning_of_notebook\n",
    "\n",
    "print(\"Full notebook execution duration:\", duration, \"seconds\")\n",
    "print(\"Full notebook execution duration:\", duration / 60, \"minutes\")\n",
    "\n",
    "table_models_vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G-28yfXCkgTe"
   },
   "source": [
    "**The below two code blocks replaces the original/inital BoW implementation using Scikit-learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "colab_type": "code",
    "id": "XtV8N-0zXNJ4",
    "outputId": "d76d5eea-9796-4eb7-a158-1f6ccdcd3db9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "\n",
      " Number of raws 2000 (documents) -- Number of columns 25644 (vocabulary) \n",
      "\n",
      "\n",
      " Type of bag_of_words <class 'scipy.sparse.csr.csr_matrix'> \n",
      "\n",
      "\n",
      " Sparsity 0.9946127554203712 \n",
      "\n",
      "\n",
      " Type of bag_of_words.toarray <class 'numpy.ndarray'> \n",
      "\n",
      "\n",
      " Type of CountVec.vocabulary <class 'dict'> \n",
      "\n",
      "A sample of CountVec.vocabulary_ [('after', 710), ('45', 244), ('50', 253), ('acting', 512), ('all', 885), ('absurd', 400), ('1922', 87), ('40', 237), ('also', 949), ('1939', 102), ('already', 946), ('1935', 98), ('africa', 706), ('1930', 92), ('allah', 886), ('alan', 823), ('135', 33), ('1970s', 136), ('aliens', 877), ('agents', 733), ('about', 380), ('10', 12), ('absolutely', 392), ('ability', 361), ('act', 510), ('acts', 527), ('1983', 149), ('age', 727), ('although', 967), ('advise', 660), ('able', 365), ('40s', 239), ('adrenalin', 620), ('actors', 524), ('666', 273), ('60', 266), ('always', 976), ('add', 547), ('ah', 765), ('alcoholic', 842), ('alcohol', 841), ('48', 247), ('advertized', 657), ('almost', 928), ('15', 42), ('alright', 948), ('against', 723), ('amazing', 990), ('action', 514), ('1980', 145), ('accident', 434), ('alex', 851), ('70s', 280), ('80s', 295), ('aint', 791), ('20', 173), ('30', 214), ('ago', 751), ('acknowledgements', 493), ('allows', 913), ('adds', 565), ('adaptation', 542), ('accounted', 461), ('actual', 528), ('adapt', 541), ('adventure', 640), ('along', 933), ('11', 25), ('actually', 531), ('advantage', 636), ('adults', 630), ('adult', 626), ('again', 722), ('ahead', 767), ('agonising', 753), ('accent', 420), ('agent', 732), ('00', 0), ('addict', 550), ('actresses', 526), ('accents', 422), ('90', 308), ('aimed', 784), ('affirming', 690), ('100', 13), ('24', 197), ('6ft', 276), ('2in', 210), ('affair', 673), ('admit', 598), ('am', 977), ('adding', 556), ('addendum', 549), ('1991', 158), ('addiction', 552), ('alienate', 871), ('adam', 536), ('adolescent', 605), ('acutely', 534), ('admits', 599), ('admirable', 587), ('2002', 178), ('acidity', 489), ('al', 818), ('adams', 539), ('accounts', 463), ('acclaimed', 439), ('accept', 424), ('1930s', 93), ('afloat', 700), ('actor', 523), ('actress', 525), ('admire', 591), ('advertisement', 653), ('aftereffects', 712), ('affect', 675), ('afterwards', 719), ('1970', 135), ('77', 288), ('2005', 181), ('akin', 812), ('30lbs', 218), ('across', 509), ('83', 298), ('advanced', 632), ('2000s', 176), ('address', 562), ('affairs', 674), ('air', 792), ('actingwise', 513), ('adversaries', 648), ('adventurously', 647), ('1996', 163), ('1990', 156), ('600', 267), ('93', 314), ('300', 215), ('aimee', 785), ('accidentally', 436), ('acceptable', 425), ('activist', 519), ('95', 316), ('29', 206), ('allow', 909), ('ages', 735), ('12', 27), ('adorable', 614), ('alone', 931), ('affected', 677), ('advice', 659), ('additional', 559), ('abandonment', 346), ('acid', 488), ('alps', 945), ('abandoned', 344), ('afore', 701), ('acted', 511), ('1994', 161), ('4am', 250), ('9am', 323), ('afternoon', 715), ('90s', 310), ('00s', 3), ('alredy', 947), ('2003', 179), ('185', 64), ('145', 37), ('150', 43), ('aerodynamics', 667), ('300c', 217), ('168', 50), ('70', 278), ('aerodynamic', 666), ('aim', 782), ('ahem', 768), ('1547', 45), ('amazingly', 991), ('abroad', 385), ('accounting', 462), ('abilities', 360), ('addition', 558), ('allowed', 911), ('alert', 848), ('1932', 95), ('ackland', 490), ('1962', 127), ('ambiguous', 999), ('agatha', 726), ('1967', 132), ('1973', 139), ('1976', 142), ('adventures', 643), ('alas', 829), ('alive', 884), ('ain', 790), ('according', 455), ('aired', 797), ('adrian', 622), ('amazed', 988), ('accapella', 419), ('88', 303), ('agree', 758), ('18th', 77), ('albert', 833), ('afterward', 718), ('1960s', 125), ('above', 381), ('1975', 141), ('1969', 134), ('adolescents', 606), ('addressed', 563), ('actions', 516), ('alien', 870), ('1988', 154), ('aims', 789), ('altantis', 954), ('alienation', 875), ('alike', 880), ('absurdity', 402), ('adjani', 578), ('101', 17), ('78', 289), ('aforementioned', 702), ('abuse', 406), ('1968', 133), ('alba', 830), ('added', 548), ('activities', 521), ('affection', 679), ('2004', 180), ('afraid', 705), ('250', 199), ('56', 261), ('aftermath', 714), ('abraham', 382), ('alot', 939), ('1960', 124), ('1985', 151), ('1800', 59), ('advertises', 655), ('adele', 567), ('accorsi', 458), ('400', 238), ('80', 292), ('abercrombie', 355), ('13', 30), ('altaira', 953), ('absence', 389), ('50s', 256), ('35', 227), ('alessandra', 850), ('adama', 537), ('1971', 137), ('affecting', 678), ('aire', 796), ('4th', 252), ('alternative', 965), ('adore', 615), ('_sung_', 335), ('absorbed', 395), ('_discussing_', 330), ('_discuss_', 329), ('91', 311), ('accosted', 459), ('alongside', 935), ('afterthought', 717), ('adores', 617), ('abetted', 356), ('affectionate', 680), ('alarm', 826), ('actively', 518), ('absent', 390), ('19th', 167), ('20th', 190), ('agnes', 747), ('1950s', 115), ('18', 57), ('1318', 32), ('14', 35), ('1876', 71), ('1881', 72), ('1933', 96), ('30s', 219), ('3000', 216), ('agar', 725), ('1957', 121), ('agrees', 762), ('1954', 119), ('1956', 120), ('1958', 122), ('1941', 105), ('1943', 107), ('1964', 129), ('1972', 138), ('28', 205), ('account', 460), ('adapting', 545), ('accurate', 469), ('adversary', 649), ('1989', 155), ('aja', 807), ('5th', 265), ('absolute', 391), ('1st', 170), ('2nd', 212), ('52', 258), ('admittedly', 602), ('1980s', 146), ('adultery', 628), ('abiding', 359), ('16', 47), ('3d', 234), ('ace', 478), ('25yrs', 200), ('allen', 893), ('2007', 183), ('13th', 34), ('afterwords', 720), ('aid', 771), ('1946', 110), ('addresses', 564), ('amber', 995), ('75', 285), ('activity', 522), ('alec', 844), ('adopt', 608), ('81', 296), ('adapted', 544), ('1940s', 104), ('abc', 352), ('agency', 729), ('abundant', 405), ('abysmal', 414), ('achieve', 480), ('admires', 595), ('agendas', 731), ('60s', 268), ('african', 707), ('africans', 708), ('agenda', 730), ('activists', 520), ('album', 835), ('21', 191), ('accompany', 445), ('17', 55), ('200', 174), ('aiden', 774), ('adherence', 573), ('amateurish', 982), ('almighty', 926), ('1924', 88), ('advani', 635), ('alones', 932), ('accepts', 430), ('affections', 682), ('albeit', 832), ('1997', 164), ('25', 198), ('accused', 474), ('500', 254), ('allergic', 895), ('adverts', 658), ('alienating', 874), ('alternates', 963), ('academy', 418), ('alanis', 824), ('achilles', 486), ('alexander', 853), ('57', 262), ('747', 284), ('aircraft', 795), ('8000', 294), ('admission', 597), ('aesthetic', 668), ('access', 431), ('aggression', 738), ('allusions', 919), ('1950', 114), ('achieves', 484), ('abduct', 353), ('abusive', 412), ('accuracy', 468), ('02', 5), ('1995', 162), ('1984', 150), ('airlines', 801), ('adaptations', 543), ('accepted', 428), ('3rd', 236), ('airport', 804), ('airliners', 800), ('adolescence', 604), ('afford', 693), ('800', 293), ('210', 192), ('alexandre', 855), ('abused', 407), ('aborted', 376), ('aforesaid', 703), ('aimlessly', 788), ('amateur', 981), ('35mm', 229), ('1936', 99), ('almghandi', 925), ('agreed', 759), ('agnostic', 749), ('agnosticism', 750), ('academic', 417), ('agreement', 761), ('98', 319), ('alligator', 904), ('afro', 709), ('1979', 144), ('alice', 867), ('2019', 187), ('alliances', 901), ('1963', 128), ('86', 301), ('achievement', 482), ('85', 300), ('19', 78), ('absoluter', 393), ('ambiguity', 998), ('aids', 776), ('admirers', 594), ('1993', 160), ('1992', 159), ('ailing', 781), ('absorbing', 396), ('allan', 887), ('1999', 166), ('absurdly', 403), ('adventurer', 641), ('advocate', 662), ('96', 317), ('alienator', 876), ('allegedly', 889), ('accomplished', 450), ('aging', 744), ('26', 201), ('alter', 956), ('aakrosh', 340), ('adjust', 581), ('allowance', 910), ('alvin', 974), ('allowing', 912), ('alison', 882), ('allison', 905), ('adored', 616), ('2001', 177), ('accompanies', 443), ('14a', 39), ('adventists', 639), ('1938', 101), ('1931', 94), ('1937', 100), ('1982', 148), ('2008', 184), ('ali', 864), ('ala', 819), ('55', 260), ('6th', 277), ('adequately', 571), ('aka', 810), ('accords', 457), ('1000', 14), ('36', 230), ('admiration', 590), ('achieved', 481), ('alfredo', 862), ('admirer', 593), ('aggressive', 739), ('acquaintance', 499), ('acquired', 503), ('abruptly', 387), ('amalgam', 979), ('accentuated', 423), ('alik', 879), ('amazonas', 993), ('amateurs', 985), ('advertisements', 654), ('99', 321), ('alerted', 849), ('aged', 728), ('accuses', 475), ('actuality', 530), ('1965', 130), ('altered', 959), ('accordingly', 456), ('1492', 38), ('abrupt', 386), ('adaption', 546), ('1927', 89), ('1928', 90), ('372', 232), ('21st', 193), ('accolades', 440), ('afghanistan', 695), ('aides', 775), ('admired', 592), ('accepting', 429), ('42nd', 242), ('advances', 634), ('acquaintances', 500), ('alternate', 961), ('ambiance', 996), ('1978', 143), ('abrasively', 383), ('addy', 566), ('accuse', 473), ('abysmally', 415), ('adulterous', 627), ('aide', 772), ('affects', 684), ('amazonian', 994), ('amazon', 992), ('akira', 813), ('16th', 53), ('000', 1), ('1998', 165), ('acquire', 502), ('ablaze', 364), ('accuracies', 467), ('altogether', 969), ('affleck', 691), ('accomplish', 449), ('1940', 103), ('23', 195), ('2010', 186), ('aladdin', 821), ('adjoining', 579), ('aficionados', 698), ('alta', 951), ('adelle', 568), ('1959', 123), ('affinities', 685), ('1929', 91), ('1947', 111), ('airplane', 803), ('abundance', 404), ('ambient', 997), ('acceptance', 427), ('01', 4), ('afilm', 699), ('accidental', 435), ('abbots', 350), ('affective', 683), ('1987', 153), ('affirm', 687), ('adulthood', 629), ('20c', 188), ('alley', 897), ('acknowledges', 494), ('abnormal', 367), ('abominable', 371), ('1986', 152), ('alain', 822), ('akimoto', 811), ('abuser', 408), ('allegory', 892), ('accessible', 433), ('1981', 147), ('accurately', 470), ('adopter', 610), ('alibi', 866), ('1944', 108), ('38', 233), ('afb', 671), ('alcoholism', 843), ('allied', 902), ('alliance', 900), ('195', 113), ('acre', 508), ('akshay', 814), ('89', 304), ('agreeing', 760), ('66', 272), ('2006', 182), ('alos', 938), ('alfie', 858), ('06', 7), ('73', 283), ('alterations', 957), ('aloof', 937), ('advised', 661), ('700', 279), ('administrator', 586), ('abound', 378), ('administered', 585), ('abby', 351), ('acquainted', 501), ('94', 315), ('achieving', 485), ('ad', 535), ('accompanied', 442), ('abounds', 379), ('alfred', 861), ('1920s', 86), ('adopted', 609), ('49th', 249), ('1934', 97), ('abuses', 410), ('aided', 773), ('adorned', 618), ('64', 270), ('accomplice', 447), ('altman', 968), ('alfre', 860), ('aloud', 940), ('alongs', 934), ('advertising', 656), ('acquisition', 504), ('accompaniment', 444), ('aiello', 777), ('acceptably', 426), ('ally', 920), ('admitted', 601), ('alight', 878), ('adoption', 612), ('180', 58), ('1h30', 169), ('afoul', 704), ('alberto', 834), ('alejandra', 846), ('adriana', 623), ('agile', 743), ('76', 287), ('adopts', 613), ('agonized', 754), ('abyss', 416), ('afterall', 711), ('akward', 817), ('accorded', 454), ('2000', 175), ('agustí', 764), ('aloy', 941), ('1913', 82), ('afforded', 694), ('addictive', 554), ('allies', 903), ('47', 246), ('106', 20), ('ager', 734), ('44', 243), ('aime', 783), ('16ieme', 51), ('14th', 40), ('agnew', 748), ('87', 302), ('aisle', 805), ('1961', 126), ('1966', 131), ('afflicted', 692), ('affectionately', 681), ('agitprop', 746), ('accrued', 464), ('180d', 61), ('altair', 952), ('abvious', 413), ('accumulated', 466), ('____', 326), ('1800s', 60), ('1853', 66), ('admiral', 589), ('advancement', 633), ('32', 224), ('4hrs', 251), ('aiming', 786), ('adrienne', 624), ('79', 290), ('350', 228), ('alternatives', 966), ('abducted', 354), ('alcatraz', 838), ('actioner', 515), ('120', 28), ('achievements', 483), ('1890', 74), ('acurately', 532), ('aesthetically', 669), ('72nd', 282), ('aislinn', 806), ('alchemy', 839), ('admiring', 596), ('adequate', 570), ('advantages', 637), ('_plan', 333), ('_a', 327), ('3199', 222), ('admittance', 600), ('abortion', 377), ('adheres', 574), ('alexa', 852), ('1953', 118), ('alarmingly', 828), ('911', 312), ('0r', 11), ('abstained', 398), ('alphaville', 944), ('addicts', 555), ('altering', 960), ('ahahahahahaaaaa', 766), ('40th', 240), ('alt', 950), ('16ème', 54), ('84', 299), ('abs', 388), ('17th', 56), ('2009', 185), ('absolutly', 394), ('16mm', 52), ('alleys', 898), ('addicted', 551), ('9mm', 324), ('almanesque', 923), ('alicia', 868), ('admirably', 588), ('31st', 223), ('altar', 955), ('42', 241), ('alludes', 917), ('900', 309), ('abhijeet', 357), ('airbag', 793), ('alloted', 908), ('alias', 865), ('adventurers', 642), ('adventuring', 645), ('amateurist', 984), ('almoust', 929), ('alton', 970), ('allocation', 907), ('100b', 16), ('accrutements', 465), ('allllllll', 906), ('alienates', 873), ('aggravated', 736), ('3k', 235), ('accusation', 471), ('22', 194), ('adamson', 540), ('alumni', 973), ('aluminum', 972), ('12a', 29), ('2inch', 211), ('accomplices', 448), ('aimless', 787), ('allegorical', 891), ('amazes', 989), ('admitting', 603), ('65', 271), ('alistair', 883), ('altruism', 971), ('_there', 336), ('_so_much_', 334), ('_they_', 337), ('accompanying', 446), ('acmetropolis', 497), ('2772', 203), ('79th', 291), ('_other_', 332), ('67', 274), ('33', 225), ('adjuncts', 580), ('altercations', 958), ('abode', 370), ('34', 226), ('advertised', 652), ('acknowledging', 495), ('acknowledgement', 492), ('almeida', 924), ('alphabetti', 943), ('alejandro', 847), ('accusing', 476), ('absurdist', 401), ('accidents', 437), ('30th', 221), ('adolf', 607), ('agis', 745), ('112', 26), ('ag', 721), ('8p', 306), ('10mil', 22), ('affectations', 676), ('afi', 696), ('103', 18), ('92', 313), ('abandons', 347), ('advance', 631), ('aghast', 742), ('ackroyd', 496), ('accented', 421), ('affirmations', 688), ('alexei', 856), ('alcaine', 837), ('27', 202), ('alfonso', 859), ('agony', 756), ('14ème', 41), ('1942', 106), ('20ft', 189), ('2s', 213), ('aftertaste', 716), ('agriculture', 763), ('ak', 809), ('1952', 117), ('aboard', 369), ('abject', 363), ('alida', 869), ('72', 281), ('adulation', 625), ('1990s', 157), ('ahh', 769), ('abusing', 411), ('150m', 44), ('accomplishments', 452), ('adopting', 611), ('451', 245), ('alecia', 845), ('addison', 557), ('alcides', 840), ('albaladejo', 831), ('actualities', 529), ('abishai', 362), ('2fast', 207), ('2furious', 208), ('alluring', 918), ('aaaugh', 338), ('abandon', 343), ('afar', 670), ('amanda', 980), ('_extremeley_', 331), ('allying', 921), ('abstract', 399), ('8½', 307), ('acoustic', 498), ('alleged', 888), ('aggressors', 741), ('51', 257), ('8mm', 305), ('albums', 836), ('27th', 204), ('10th', 24), ('58', 263), ('airing', 798), ('49', 248), ('1001', 15), ('97', 318), ('additionally', 560), ('acute', 533), ('allyson', 922), ('advent', 638), ('007', 2), ('akshaye', 816), ('akshaya', 815), ('aag', 339), ('abusers', 409), ('alternations', 964), ('alanrickmaniac', 825), ('accustomed', 477), ('1948', 112), ('alternately', 962), ('achterbusch', 487), ('68', 275), ('alpha', 942), ('15th', 46), ('affinity', 686), ('allende', 894), ('abandoning', 345), ('active', 517), ('_am_', 328), ('1tv', 171), ('aames', 341), ('alls', 914), ('aeons', 665), ('1911', 81), ('1918', 84), ('08', 9), ('1920', 85), ('aborigins', 375), ('aboriginal', 374), ('aborigin', 373), ('acquits', 506), ('ae', 664), ('airman', 802), ('affable', 672), ('30something', 220), ('alarming', 827), ('adjusted', 582), ('9th', 325), ('alger', 863), ('adjutant', 584), ('agamemnon', 724), ('acknowledge', 491), ('adventuresome', 644), ('abre', 384), ('adjusting', 583), ('ai', 770), ('afterlife', 713), ('ably', 366), ('1300', 31), ('1900s', 79), ('1974', 140), ('agonizingly', 755), ('adamant', 538), ('165', 49), ('agonies', 752), ('alabama', 820), ('abbas', 348), ('achad', 479), ('109', 21), ('99cents', 322), ('airborne', 794), ('accusations', 472), ('accomplishment', 451), ('advert', 651), ('alleviate', 896), ('acclaim', 438), ('1951', 116), ('aditya', 576), ('alienated', 872), ('adversity', 650), ('1ç', 172), ('1840', 63), ('alleyways', 899), ('accord', 453), ('allegiance', 890), ('987', 320), ('abo', 368), ('adventurous', 646), ('140', 36), ('accommodation', 441), ('233', 196), ('alexandra', 854), ('amadeus', 978), ('2hrs', 209), ('aardman', 342), ('160', 48), ('10pm', 23), ('absoulely', 397), ('750', 286), ('1895', 75), ('adept', 569), ('500db', 255), ('aggressively', 740), ('1863', 70), ('1901', 80), ('1852', 65), ('1886', 73), ('1898', 76), ('5million', 264), ('agoraphobic', 757), ('1830', 62), ('1861', 69), ('alonzo', 936), ('1854', 67), ('1859', 68), ('61', 269), ('allude', 916), ('adgth', 572), ('abbot', 349), ('ajnabi', 808), ('alok', 930), ('aileen', 779), ('abhorrent', 358), ('1914', 83), ('amateurishly', 983), ('almodovar', 927), ('adjacent', 577), ('aficionado', 697), ('addictions', 553), ('amaze', 987), ('alway', 975), ('allthough', 915), ('adien', 575), ('acquitted', 507), ('additions', 561), ('affirmed', 689), ('alexis', 857), ('abomination', 372), ('advocates', 663), ('82', 297), ('ailes', 780), ('1945', 109), ('aiken', 778), ('aline', 881), ('adrenaline', 621), ('adorns', 619), ('aggravates', 737), ('104', 19), ('05', 6), ('53', 259), ('09', 10), ('07', 8), ('360', 231), ('acquit', 505), ('amati', 986), ('1and', 168), ('accessability', 432), ('airline', 799)]\n",
      "\n",
      " Type of bag_of_words.toarray() {} \n",
      "\n",
      "\n",
      " First feature vector, representing the first document \n",
      " \n"
     ]
    }
   ],
   "source": [
    "CountVec = CountVectorizer()\n",
    "# Creating the BoW with the set of all the documents and transforming the documents in feature vectors\n",
    "bag_of_words = CountVec.fit_transform(df['reviews'])\n",
    "\n",
    "print(type(bag_of_words))\n",
    "print('\\n Number of raws {} (documents) -- Number of columns {} (vocabulary) \\n'.format(bag_of_words.shape[0], bag_of_words.shape[1]))\n",
    "\n",
    "# https://machinelearningmastery.com/sparse-matrices-for-machine-learning/\n",
    "# This is a sparse matrix \n",
    "print('\\n Type of bag_of_words {} \\n'.format(type(bag_of_words)))\n",
    "sparsity = 1.0 - bag_of_words.nnz / (bag_of_words.shape[0] * bag_of_words.shape[1])\n",
    "print('\\n Sparsity {} \\n'.format(sparsity))\n",
    "\n",
    "# This is a  \n",
    "print('\\n Type of bag_of_words.toarray {} \\n'.format(type(bag_of_words.toarray())))\n",
    "\n",
    "# In CountVec we have the vocabulary as an attribute\n",
    "print('\\n Type of CountVec.vocabulary {} \\n'.format(type(CountVec.vocabulary_)))\n",
    "print('A sample of CountVec.vocabulary_ {}'.format([(k, v) for k, v in CountVec.vocabulary_.items() if v < 1000]))\n",
    "\n",
    "# In bag_of_words we have the vector features representing each single document \n",
    "print('\\n Type of bag_of_words.toarray() {} \\n')\n",
    "print('\\n First feature vector, representing the first document \\n', bag_of_words[0, :2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c6A2pKzOmxzT"
   },
   "outputs": [],
   "source": [
    "# Lets get our training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(bag_of_words.toarray(), df['sentiment'].values, test_size=0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "pqbPNSGUg4Rj",
    "outputId": "b5976680-a41d-4736-99b7-e5e17ce473f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using score function: 0.824\n",
      "Percentage of correct predicted values: 0.824\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "print('Using score function: {}'.format(clf.score(X_test, y_test)))\n",
    "results = [(predicted, actual) for predicted, actual in zip(clf.predict(X_test),  y_test) \n",
    "           if  predicted == actual]\n",
    "print('Percentage of correct predicted values: {}'.format(len(results)/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P0YIRBaroMeM"
   },
   "source": [
    "### RandomForestClassifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "nMUUsMLkkyLO",
    "outputId": "8b2096de-ded8-4e02-b75a-046b1861f529"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using score function: 0.828\n",
      "Percentage of correct predicted values: 0.828\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "print('Using score function: {}'.format(clf.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "results = [(predicted, actual) for predicted, actual in zip(clf.predict(X_test),  y_test) \n",
    "           if  predicted == actual]\n",
    "\n",
    "print('Percentage of correct predicted values: {}'.format(len(results)/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BM0YI71dnpZE"
   },
   "source": [
    "### RandomForestClassifer with TFIDF (Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "HNluLDB4nayq",
    "outputId": "2ee1d453-891e-45d9-ad20-2d8bd81c3853"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 1.0\n",
      "Test accuracy 0.816\n",
      "Train accuracy list [0.84768212 0.86092715 0.82119205 0.74834437 0.83333333 0.83333333\n",
      " 0.85234899 0.8590604  0.79194631 0.80536913] \n",
      "Train accuracy mean 0.825353719424567 \n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['reviews'], df['sentiment'], test_size=0.25)\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf), ('clf', RandomForestClassifier(n_estimators=1000))])\n",
    "lr_tfidf.fit(X_train, y_train)\n",
    "print('Train accuracy {}'.format(lr_tfidf.score(X_train, y_train)))\n",
    "print('Test accuracy {}'.format(lr_tfidf.score(X_test, y_test)))\n",
    "\n",
    "# Trying with cross_val_score\n",
    "lr = LogisticRegression()\n",
    "k_folds = 10\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "print('Train accuracy list {} '.format(cross_val_score(lr, X_train_tfidf, y_train, cv= k_folds))) \n",
    "print('Train accuracy mean {} '.format(cross_val_score(lr, X_train_tfidf, y_train, cv= k_folds).mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fjDcOMEUoToE"
   },
   "source": [
    "### LSTM with Keras (Sequential model)\n",
    "\n",
    "\n",
    "Please note that the below code is executed on GPU instances on Colab, this wont work on your local machine, use the flag to enable/disable running in CPU or GPU mode, set `run_in_GPU_mode_on_colab=false` in order to be able to run in CPU mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "fVN5C_34oSgO",
    "outputId": "03f65fd1-c9fb-47dc-8025-afa10d4f68d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1350 samples, validate on 150 samples\n",
      "Epoch 1/2\n",
      "1350/1350 [==============================] - 62s 46ms/step - loss: 0.6947 - acc: 0.4822 - val_loss: 0.6932 - val_acc: 0.4933\n",
      "Epoch 2/2\n",
      "1350/1350 [==============================] - 59s 43ms/step - loss: 0.6934 - acc: 0.4919 - val_loss: 0.6928 - val_acc: 0.5333\n",
      "500/500 [==============================] - 5s 10ms/step\n",
      "Test score: 0.6931333312988281\n",
      "Test accuracy: 0.4959999995231628\n",
      "Duration on the CPU: 128.34940028190613\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def lstm_keras():\n",
    "  from keras.models import Sequential\n",
    "  from keras.layers import Dense, Activation, Embedding, LSTM\n",
    "  from keras.preprocessing.text import Tokenizer\n",
    "  from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(df['reviews'], df['sentiment'], test_size=0.25)\n",
    "\n",
    "  vocab_size = 1000\n",
    "  tokenize = Tokenizer(num_words=vocab_size)\n",
    "  tokenize.fit_on_texts(X_train)\n",
    "\n",
    "  encoded_X_train = tokenize.texts_to_matrix(X_train)\n",
    "  encoded_X_test = tokenize.texts_to_matrix(X_test)\n",
    "\n",
    "  encoder = LabelBinarizer()\n",
    "  encoder.fit(y_train)\n",
    "  encoded_y_train = encoder.transform(y_train)\n",
    "  encoded_y_test = encoder.transform(y_test)\n",
    "\n",
    "  max_features = 1000\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(max_features, 128, dropout=0.2))\n",
    "  model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))  # try using a GRU instead, for fun\n",
    "  model.add(Dense(1))\n",
    "  model.add(Activation('sigmoid'))\n",
    "\n",
    "  model.compile(loss='binary_crossentropy',\n",
    "               optimizer='adam',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "  batch_size=64\n",
    "  epochs=10\n",
    "  history = model.fit(encoded_X_train, encoded_y_train, \n",
    "                  batch_size=batch_size, \n",
    "                  epochs=epochs, \n",
    "                  verbose=1, \n",
    "                  validation_split=0.1)\n",
    "\n",
    "  score = model.evaluate(encoded_X_test, encoded_y_test, \n",
    "                         batch_size=batch_size, verbose=1)\n",
    "  print('Test score:', score[0])\n",
    "  print('Test accuracy:', score[1])\n",
    "\n",
    "\n",
    "run_in_GPU_mode_on_colab=False\n",
    "\n",
    "if run_in_GPU_mode_on_colab:  \n",
    "  config = tf.ConfigProto()\n",
    "  config.gpu_options.allow_growth = True\n",
    "\n",
    "  with tf.device('/gpu:0'):\n",
    "    session_gpu = tf.Session(config=config)\n",
    "    session_gpu.run(tf.global_variables_initializer())\n",
    "    session_gpu.run(tf.tables_initializer())\n",
    "    start = time.time()\n",
    "    session_gpu.run(lstm_keras())\n",
    "    end = time.time()\n",
    "    gpu_time = end - start\n",
    "    print('Duration on the GPU: {} seconds'.format(gpu_time))\n",
    "else:\n",
    "  start = time.time()\n",
    "  lstm_keras()\n",
    "  end = time.time()\n",
    "  cpu_time = end - start\n",
    "  print('Duration on the CPU: {} seconds'.format(cpu_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gvChJ9kdot4q"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Sentiment analysis of movies (IMDB).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
