{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A0XMzfB5_EtE"
   },
   "source": [
    "### Sentiment analysis of movie (IMDB) reviews using dataset provided by the ACL 2011 paper, see http://ai.stanford.edu/~amaas/data/sentiment/.\n",
    "\n",
    "#### Dataset can be downloaded separately from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz, but wont be necessary as the download process has been embedded in the notebook and source file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "colab_type": "code",
    "id": "CML_IG6z-iwM",
    "outputId": "ed907d98-b14f-42a8-c0a4-5b8a3082c9f5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rodolfo\\Anaconda3\\envs\\flasky\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk\n",
    "# !pip install --upgrade gensim\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "#nltk.download('punkt')>>> import nltk\n",
    "\n",
    "\n",
    "import glob\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FJiWamI00hBp",
    "outputId": "177c454e-cf13-49a5-91dd-74c40c2365fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On the MacOSX, you will need to install wget, see https://www.mkyong.com/mac/wget-on-mac-os-x/\n"
     ]
    }
   ],
   "source": [
    "# MacOSX: See https://www.mkyong.com/mac/wget-on-mac-os-x/ for wget\n",
    "print('On the MacOSX, you will need to install wget, see https://www.mkyong.com/mac/wget-on-mac-os-x/')\n",
    "\n",
    "if not os.path.isfile('aclImdb_v1.tar.gz'):\n",
    "  !wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz \n",
    "\n",
    "if not os.path.isfile('aclImdb'):  \n",
    "  !tar -xf aclImdb_v1.tar.gz \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U5Tnmoh-Dpfk"
   },
   "outputs": [],
   "source": [
    "time_beginning_of_notebook = time.time()\n",
    "SAMPLE_SIZE=600\n",
    "positive_sample_file_list = glob.glob(os.path.join('aclImdb/train/pos', \"*.txt\"))\n",
    "positive_sample_file_list = positive_sample_file_list[:SAMPLE_SIZE]\n",
    "\n",
    "negative_sample_file_list = glob.glob(os.path.join('aclImdb/train/neg', \"*.txt\"))\n",
    "negative_sample_file_list = negative_sample_file_list[:SAMPLE_SIZE]\n",
    "\n",
    "import re\n",
    "\n",
    "# load doc into memory\n",
    "# regex to clean markup elements \n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', encoding='utf8')\n",
    "    # read all text\n",
    "    text = re.sub('<[^>]*>', ' ', file.read())\n",
    "    #text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vMYBkcdIB9uc"
   },
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "cz5eJi7AGSqR",
    "outputId": "854b06de-6285-44bb-8f53-49f32565c770"
   },
   "outputs": [],
   "source": [
    "positive_strings = [load_doc(x) for x in positive_sample_file_list]\n",
    "#print('\\n Positive reviews \\n ',positive_strings[:5])\n",
    "\n",
    "negative_strings = [load_doc(x) for x in negative_sample_file_list]\n",
    "#print('\\n Negative reviews \\n ', negative_strings[:5])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "C0WiHTr7I4CN",
    "outputId": "238e7430-78f7-46b1-9a21-c00abac09d56"
   },
   "outputs": [],
   "source": [
    "positive_tokenized = [word_tokenize(s) for s in positive_strings]\n",
    "#print('\\n Positive tokenized 1 \\n {} \\n\\n Positive tokenized 2 \\n {}'. format(positive_tokenized[1], positive_tokenized[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "YDP-eqAGIq5R",
    "outputId": "6e254ed0-15ae-420b-d9d5-0340d73a9de2"
   },
   "outputs": [],
   "source": [
    "negative_tokenized = [word_tokenize(s) for s in negative_strings]\n",
    "#print('\\n Negative tokenized 1 \\n {} \\n\\n  Negative tokenized 2 \\n {}'. format(negative_tokenized[1], negative_tokenized[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "6bgN1KJRMPpq",
    "outputId": "9ebc93dd-a3d4-45f2-f8c0-e4ba957c8ce7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count across all reviews (before stripping tokens): 161165\n",
      "Non alphanumeric characters found in universe vocabulary {'=', ')', ';', '[', '-', \"'\", ':', '(', '}', '!', '?', ']'}\n",
      "Word count across all reviews (after stripping tokens): 139772\n"
     ]
    }
   ],
   "source": [
    "# load doc into memory\n",
    "with open('aclImdb/imdb.vocab', encoding='utf8') as f:\n",
    "    #content = f.readlines()\n",
    "    universe_vocabulary = [x.strip() for x in f.readlines()]\n",
    "\n",
    "print(\"Word count across all reviews (before stripping tokens):\", sum([len(token) for token in positive_tokenized]))\n",
    "\n",
    "#Checking the not alphanumeric characters in vocabulary\n",
    "non_alphanumeric_set = set()\n",
    "for word in universe_vocabulary:\n",
    "    non_alphanumeric_set |= set(re.findall('\\W', word))\n",
    "print('Non alphanumeric characters found in universe vocabulary', non_alphanumeric_set)\n",
    "\n",
    "\n",
    "stripped_positive_tokenized = []\n",
    "for tokens in positive_tokenized:\n",
    "  stripped_positive_tokenized.append([token.lower() for token in tokens if token.lower() in universe_vocabulary])\n",
    "\n",
    "print(\"Word count across all reviews (after stripping tokens):\", sum([len(token) for token in stripped_positive_tokenized]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "DSFWrZInMueS",
    "outputId": "56c46895-8165-416e-eebe-099aeb9b719e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count across all reviews (before stripping tokens): 161165\n",
      "Word count across all reviews (after stripping tokens): 135014\n"
     ]
    }
   ],
   "source": [
    "print(\"Word count across all reviews (before stripping tokens):\", sum([len(token) for token in positive_tokenized]))\n",
    "stripped_negative_tokenized = []\n",
    "for tokens in negative_tokenized:\n",
    "  stripped_negative_tokenized.append([token.lower() for token in tokens if token.lower() in universe_vocabulary])\n",
    "\n",
    "print(\"Word count across all reviews (after stripping tokens):\", sum([len(token) for token in stripped_negative_tokenized]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dnu21deYOkDE"
   },
   "source": [
    "## Modelling \n",
    "\n",
    "We have decided to do the use the below models and vectorisation techniques to test our their accuracy / score, the idea is to use a one model and one vectorization technique and plot a score.\n",
    "\n",
    "**Simple models**\n",
    "\n",
    "- Logistic Regression\n",
    "- Random Forst\n",
    "- LSTM\n",
    "- GRU\n",
    "- CNN\n",
    "\n",
    "**Vectorisation techniques**\n",
    "- Bag of Words\n",
    "- Word2Vec\n",
    "- TFIDF (probability scores)\n",
    "- FastText\n",
    "- Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression.\n",
    "## Introducing Pipeline: http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "\n",
    "## Introducing TfdfVectorizer: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "## Introducing cross_val_score http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\n",
    "\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df_positives = pd.DataFrame({'reviews':[load_doc(x) for x in positive_sample_file_list], 'sentiment': np.ones(SAMPLE_SIZE)})\n",
    "df_negatives = pd.DataFrame({'reviews':[load_doc(x) for x in negative_sample_file_list], 'sentiment': np.zeros(SAMPLE_SIZE)})\n",
    "\n",
    "df = pd.concat([df_positives, df_negatives], ignore_index=True)\n",
    "\n",
    "df = shuffle(df)\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(df['reviews'], df['sentiment'], test_size=0.25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regress model using Bag of Words vectorisation technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "\n",
      " Number of raws 1200 (documents) -- Number of columns 17940 (vocabulary) \n",
      "\n",
      "\n",
      " Type of bag_of_words <class 'scipy.sparse.csr.csr_matrix'> \n",
      "\n",
      "\n",
      " Sparsity 0.992391397250093 \n",
      "\n",
      "\n",
      " Type of bag_of_words.toarray <class 'numpy.ndarray'> \n",
      "\n",
      "\n",
      " Type of CountVec.vocabulary <class 'dict'> \n",
      "\n",
      "A sample of CountVec.vocabulary_ [('and', 853), ('alike', 698), ('about', 304), ('above', 305), ('any', 951), ('1941', 81), ('apophis', 974), ('1928', 68), ('anderson', 856), ('along', 731), ('24', 167), ('alliances', 714), ('advanced', 505), ('antagonists', 928), ('alien', 690), ('anubis', 946), ('another', 922), ('also', 740), ('accompanies', 359), ('an', 830), ('ancients', 852), ('absolutely', 312), ('allow', 719), ('abducted', 286), ('appalling', 977), ('achievement', 384), ('afford', 548), ('against', 571), ('all', 703), ('alec', 676), ('aplomb', 967), ('annals', 905), ('airplay', 634), ('akshay', 651), ('add', 437), ('anything', 955), ('advertising', 522), ('antics', 940), ('actually', 424), ('10', 12), ('85', 251), ('amount', 813), ('alright', 739), ('anyone', 954), ('accompanied', 358), ('across', 402), ('ages', 578), ('adored', 491), ('acting', 406), ('adventures', 513), ('actors', 417), ('after', 562), ('1966', 103), ('1963', 100), ('always', 759), ('ape', 965), ('again', 570), ('anime', 900), ('2010', 157), ('30', 181), ('appearance', 985), ('al', 653), ('anna', 904), ('2nd', 179), ('america', 793), ('airborne', 623), ('affects', 540), ('apparently', 980), ('alan', 658), ('already', 738), ('anne', 906), ('amy', 829), ('american', 794), ('actual', 423), ('60', 224), ('annoyed', 917), ('1993', 132), ('2002', 147), ('alone', 730), ('appear', 984), ('adult', 498), ('admit', 476), ('almost', 729), ('alert', 677), ('amnesia', 808), ('among', 809), ('alcoholic', 671), ('apparent', 979), ('amateur', 770), ('_everything_', 271), ('actresses', 421), ('17', 44), ('actress', 420), ('acted', 404), ('agonizing', 590), ('account', 365), ('70s', 236), ('admittedly', 480), ('aired', 627), ('2006', 151), ('ann', 903), ('1920', 65), ('animals', 893), ('annoyance', 916), ('andrews', 861), ('admires', 473), ('amazing', 777), ('allowed', 720), ('angered', 875), ('ago', 586), ('ah', 598), ('accept', 339), ('act', 403), ('angry', 884), ('am', 762), ('admire', 469), ('almighty', 728), ('aniston', 902), ('adams', 429), ('afraid', 557), ('able', 293), ('actra', 418), ('although', 753), ('anthems', 931), ('70', 234), ('2am', 175), ('36', 191), ('aimed', 615), ('animated', 895), ('according', 364), ('animators', 899), ('anxious', 949), ('air', 621), ('advances', 506), ('annisten', 909), ('aliens', 695), ('1976', 114), ('23', 166), ('anywhere', 959), ('answer', 923), ('apologize', 972), ('advance', 504), ('110', 24), ('appeared', 987), ('amongst', 810), ('90', 256), ('action', 407), ('adventists', 510), ('ancestors', 845), ('anyway', 957), ('adhere', 456), ('ambiance', 782), ('1979', 117), ('20', 143), ('32', 186), ('age', 573), ('adapt', 430), ('advantage', 508), ('40', 200), ('accolades', 356), ('adults', 502), ('anti', 935), ('ancestry', 847), ('adjusting', 462), ('anew', 868), ('80', 248), ('abandoned', 279), ('50', 214), ('animation', 897), ('appears', 989), ('annoying', 918), ('1997', 136), ('aftermath', 565), ('aim', 614), ('aka', 646), ('1971', 109), ('abreast', 306), ('allison', 718), ('aircraft', 625), ('150', 39), ('anachronisms', 832), ('anyways', 958), ('appearing', 988), ('adequate', 454), ('albert', 665), ('90s', 259), ('aged', 574), ('adventurous', 515), ('abu', 321), ('ahmad', 603), ('absolute', 311), ('answering', 925), ('adaptation', 431), ('accent', 337), ('aback', 277), ('appeal', 982), ('antagonist', 927), ('amusing', 827), ('adaptations', 432), ('ads', 497), ('alive', 700), ('1978', 116), ('2003', 148), ('abilities', 291), ('almightly', 727), ('adventure', 511), ('amrapurkar', 819), ('amrapurkars', 820), ('actor', 416), ('5th', 222), ('3rd', 198), ('18', 46), ('adaption', 435), ('1940', 79), ('absurd', 318), ('applied', 998), ('anthony', 933), ('accapella', 334), ('andress', 859), ('50s', 217), ('ahead', 601), ('affected', 535), ('applaud', 993), ('agrees', 597), ('adulterous', 499), ('anthology', 932), ('ain', 618), ('3d', 197), ('35', 188), ('anniston', 910), ('anymore', 953), ('anchorman', 850), ('abuse', 325), ('added', 438), ('afternoon', 566), ('1990s', 129), ('80s', 249), ('1970s', 108), ('4th', 213), ('aircrafts', 626), ('12', 28), ('1990', 128), ('allen', 709), ('101', 16), ('15', 38), ('99', 266), ('ability', 292), ('67', 231), ('33', 187), ('alarmed', 660), ('affection', 537), ('apart', 961), ('achieved', 383), ('anticipated', 937), ('2008', 153), ('akbar', 647), ('acharya', 379), ('ali', 686), ('anil', 890), ('amps', 817), ('alpha', 737), ('adversaries', 516), ('ace', 377), ('advice', 523), ('13', 32), ('16', 41), ('accepted', 342), ('abbot', 282), ('afterlives', 564), ('angular', 888), ('amid', 799), ('affluence', 546), ('addition', 446), ('accident', 350), ('apologies', 971), ('abundance', 323), ('anytime', 956), ('alexander', 680), ('1950', 89), ('19', 57), ('1980', 118), ('amelie', 791), ('amazed', 775), ('1996', 135), ('alignment', 697), ('annual', 920), ('95', 263), ('000', 1), ('afterward', 568), ('abc', 285), ('accessible', 348), ('1969', 106), ('apartment', 962), ('1864', 52), ('aforementioned', 556), ('apocalyptic', 969), ('anybody', 952), ('14', 34), ('aboard', 296), ('1912', 63), ('animal', 891), ('absorbing', 314), ('achieve', 382), ('adjust', 461), ('alternative', 749), ('answered', 924), ('8th', 255), ('accomplish', 362), ('aluminum', 758), ('105', 18), ('alienating', 693), ('anar', 842), ('aggressive', 581), ('angrily', 883), ('agony', 591), ('akin', 648), ('abruptly', 308), ('1954', 92), ('ailing', 612), ('25', 169), ('agent', 576), ('1998', 137), ('100', 13), ('announcers', 914), ('admission', 475), ('accurate', 371), ('airlines', 630), ('accidentally', 351), ('analytical', 839), ('angora', 882), ('accused', 374), ('antoine', 942), ('angie', 878), ('alas', 661), ('1951', 91), ('adriana', 496), ('amore', 812), ('aida', 607), ('79', 246), ('2007', 152), ('absent', 310), ('agree', 593), ('andrew', 860), ('adam', 426), ('ai', 605), ('ably', 294), ('acceptance', 341), ('angels', 873), ('abound', 302), ('adversity', 518), ('aloof', 733), ('actions', 410), ('accepting', 343), ('andy', 865), ('anaconda', 833), ('alfred', 683), ('affirms', 542), ('120', 29), ('annna', 911), ('altho', 752), ('abominable', 297), ('alias', 687), ('algae', 684), ('afterthought', 567), ('ala', 654), ('acceptable', 340), ('akshaye', 652), ('aditya', 458), ('1931', 72), ('accepts', 344), ('abashed', 280), ('amann', 767), ('abrupt', 307), ('agonizes', 589), ('affair', 531), ('active', 412), ('1930s', 71), ('anxiety', 948), ('ahmed', 604), ('allan', 704), ('accomplice', 361), ('1924', 67), ('1937', 76), ('1938', 77), ('1942', 82), ('1947', 87), ('1964', 101), ('39', 196), ('adrian', 495), ('2004', 149), ('acts', 422), ('45', 206), ('anemic', 867), ('35mins', 190), ('1800', 48), ('angles', 880), ('affect', 533), ('20th', 160), ('1st', 142), ('amita', 804), ('12th', 31), ('allport', 723), ('1985', 123), ('1700', 45), ('activity', 415), ('abusive', 327), ('98', 265), ('accessibility', 347), ('30s', 184), ('40s', 204), ('11', 23), ('addictive', 443), ('adolescent', 481), ('acne', 394), ('admirably', 466), ('afterwards', 569), ('16s', 43), ('afghanistan', 551), ('admiral', 467), ('alcohol', 670), ('anchor', 848), ('advise', 524), ('absorbed', 313), ('allows', 722), ('31', 185), ('ambiguous', 785), ('21st', 164), ('102', 17), ('airphone', 631), ('ambitious', 788), ('anger', 874), ('addressed', 450), ('86', 252), ('african', 559), ('alabama', 655), ('alex', 678), ('140', 35), ('22', 165), ('05', 7), ('apathy', 964), ('ancient', 851), ('accompany', 360), ('angel', 870), ('afro', 561), ('americans', 797), ('angeles', 871), ('60th', 226), ('answers', 926), ('amazon', 779), ('agape', 572), ('address', 449), ('altman', 755), ('adler', 464), ('aladdin', 656), ('ailes', 611), ('amidst', 800), ('annoyingly', 919), ('2001', 146), ('appease', 990), ('admirers', 472), ('affairs', 532), ('ambitions', 787), ('1960s', 98), ('ally', 726), ('absence', 309), ('amazingly', 778), ('1972', 110), ('68', 232), ('abstract', 316), ('abstraction', 317), ('6th', 233), ('anorexic', 921), ('amuses', 826), ('16mm', 42), ('adding', 444), ('01', 5), ('americanized', 796), ('altogether', 756), ('09', 11), ('alicia', 689), ('adapted', 433), ('advertisements', 521), ('ancestor', 844), ('alexa', 679), ('1994', 133), ('amish', 802), ('altitude', 754), ('abydos', 329), ('activated', 411), ('addresses', 451), ('announces', 915), ('alternate', 746), ('alliance', 713), ('ahamad', 599), ('ahamd', 600), ('accumulates', 369), ('achieving', 387), ('accuracy', 370), ('alives', 701), ('1977', 115), ('1960', 97), ('appealing', 983), ('acclaimed', 354), ('adoptive', 486), ('ailments', 613), ('21', 162), ('06', 8), ('64', 229), ('addicting', 442), ('111', 25), ('360', 192), ('1956', 94), ('1980s', 119), ('apocalypse', 968), ('alledgedly', 705), ('1880', 53), ('apologized', 973), ('88', 253), ('aclear', 392), ('academy', 333), ('1900', 59), ('aldrin', 675), ('_have_', 272), ('117', 26), ('1930', 70), ('accidently', 352), ('agreed', 594), ('aerial', 526), ('amused', 824), ('1948', 88), ('admirer', 471), ('andre', 857), ('altar', 742), ('9ers', 268), ('alleys', 712), ('africa', 558), ('3000', 183), ('appearances', 986), ('1991', 130), ('acknowledge', 390), ('13th', 33), ('agonia', 587), ('aint', 620), ('aesthetic', 527), ('abuelita', 322), ('altagracia', 741), ('apartments', 963), ('5x5', 223), ('amateurish', 771), ('applicability', 996), ('alarm', 659), ('animalplanet', 892), ('albeit', 664), ('94', 262), ('amu', 822), ('antithetical', 941), ('ambiguity', 784), ('60s', 225), ('admitted', 479), ('alter', 743), ('27', 172), ('amanda', 764), ('aggressors', 582), ('2005', 150), ('alongside', 732), ('214', 163), ('amassed', 769), ('1989', 127), ('airbag', 622), ('4kids', 212), ('2000', 145), ('1929', 69), ('adds', 452), ('admired', 470), ('alki', 702), ('72', 239), ('afghans', 552), ('akki', 650), ('affections', 539), ('amazement', 776), ('1958', 95), ('andalucia', 854), ('ang', 869), ('200', 144), ('alot', 736), ('abducting', 287), ('accomplished', 363), ('abbott', 283), ('400', 201), ('1970', 107), ('anticipate', 936), ('accents', 338), ('achievements', 385), ('additional', 447), ('1974', 112), ('amityville', 806), ('1987', 125), ('81', 250), ('55th', 219), ('adventist', 509), ('26th', 171), ('amoral', 811), ('airplanes', 633), ('acquire', 397), ('1939', 78), ('applause', 995), ('71', 238), ('ambersons', 781), ('acrobatics', 401), ('alba', 663), ('akira', 649), ('airplane', 632), ('amuse', 823), ('aiming', 616), ('2fast', 177), ('2furious', 178), ('airport', 635), ('77', 245), ('747', 241), ('108', 19), ('airline', 629), ('1840', 49), ('47', 208), ('ample', 816), ('adorned', 494), ('08', 10), ('_apocalyptically', 270), ('amusingly', 828), ('abysmal', 330), ('addict', 440), ('1940s', 80), ('amaze', 774), ('achieves', 386), ('alleged', 707), ('alterior', 745), ('admittadly', 478), ('allowing', 721), ('96', 264), ('adultery', 500), ('ambassador', 780), ('1900s', 60), ('20perr', 158), ('20widow', 161), ('1943', 83), ('1920s', 66), ('activities', 414), ('angers', 876), ('1933', 74), ('aishwarya', 644), ('aishu', 643), ('analyze', 840), ('aince', 619), ('1000', 14), ('addicted', 441), ('achingly', 388), ('agents', 577), ('addled', 448), ('adores', 492), ('1907', 61), ('accusations', 373), ('applauded', 994), ('album', 668), ('76', 244), ('amateurishness', 772), ('appalled', 976), ('algerians', 685), ('aide', 608), ('adorble', 489), ('18th', 56), ('adamant', 428), ('adjoining', 460), ('adopts', 487), ('ahem', 602), ('angelina', 872), ('actioner', 408), ('10p', 20), ('albiet', 666), ('4am', 210), ('9am', 267), ('00s', 4), ('70th', 237), ('alienation', 694), ('1967', 104), ('afilm', 553), ('1999', 138), ('00', 0), ('alternates', 748), ('adopted', 485), ('accessory', 349), ('abuzz', 328), ('acquitted', 400), ('allegations', 706), ('1995', 134), ('amigos', 801), ('73', 240), ('aging', 584), ('adopt', 484), ('adversary', 517), ('adequately', 455), ('appallingly', 978), ('75', 242), ('altered', 744), ('48', 209), ('actioners', 409), ('aline', 699), ('aaja', 275), ('56', 220), ('ammo', 807), ('anand', 841), ('alcoholism', 672), ('adjective', 459), ('amusement', 825), ('1968', 105), ('accolade', 355), ('1961', 99), ('affluent', 547), ('1973', 111), ('allegedly', 708), ('anthropomorphic', 934), ('anticlimax', 939), ('90ish', 258), ('abducts', 288), ('12m', 30), ('40mins', 203), ('alfre', 682), ('adapting', 434), ('28', 173), ('agnes', 585), ('absurdist', 319), ('admirable', 465), ('abby', 284), ('addison', 445), ('alight', 696), ('appetites', 992), ('accurately', 372), ('29', 174), ('1988', 126), ('1982', 121), ('1986', 124), ('anticipation', 938), ('alters', 751), ('airwaves', 640), ('accumulated', 368), ('abused', 326), ('aids', 610), ('abandon', 278), ('alaska', 662), ('adorable', 488), ('aish', 642), ('africans', 560), ('afoot', 555), ('addendum', 439), ('alos', 735), ('ao', 960), ('8bit', 254), ('10th', 21), ('accelerated', 335), ('announce', 912), ('anecdotes', 866), ('angus', 889), ('airspace', 637), ('38k', 195), ('admiring', 474), ('abomination', 298), ('angle', 879), ('ad', 425), ('albums', 669), ('antoinette', 943), ('1946', 86), ('1862', 51), ('alienated', 692), ('apotheosis', 975), ('announced', 913), ('aisle', 645), ('absurdity', 320), ('abbey', 281), ('aesthetically', 528), ('adolph', 483), ('agrandizement', 592), ('1992', 131), ('alternatives', 750), ('007', 2), ('amounts', 814), ('afterall', 563), ('6200', 227), ('1860', 50), ('abe', 289), ('acquainted', 395), ('acquaints', 396), ('200ft', 155), ('93', 261), ('ambient', 783), ('aakrosh', 276), ('1983', 122), ('actelone', 405), ('angharad', 877), ('adjustments', 463), ('adaptor', 436), ('4eva', 211), ('affectionate', 538), ('37', 193), ('500', 215), ('analysing', 837), ('190', 58), ('allergic', 710), ('18s', 55), ('14s', 36), ('1944', 84), ('1945', 85), ('1919', 64), ('anguished', 887), ('alternately', 747), ('alibi', 688), ('1959', 96), ('angst', 885), ('150k', 40), ('ache', 380), ('afghanastan', 550), ('afl', 554), ('afganistan', 549), ('24th', 168), ('401k', 202), ('1981', 120), ('aghast', 583), ('11m', 27), ('02', 6), ('alda', 673), ('advancing', 507), ('amiss', 803), ('application', 997), ('aggravates', 579), ('airy', 641), ('agreements', 596), ('admits', 477), ('adieu', 457), ('65', 230), ('alienate', 691), ('andres', 858), ('amen', 792), ('amrish', 821), ('acknowledging', 391), ('abundant', 324), ('amateurs', 773), ('aided', 609), ('anguish', 886), ('anchored', 849), ('americanised', 795), ('agonisingly', 588), ('accommodation', 357), ('00am', 3), ('3yrs', 199), ('anu', 945), ('antes', 930), ('amanhecer', 766), ('adore', 490), ('_very_', 273), ('adama', 427), ('anders', 855), ('acid', 389), ('amitabh', 805), ('amar', 768), ('animate', 894), ('alluring', 725), ('abo', 295), ('advertised', 519), ('anally', 834), ('acclaim', 353), ('access', 345), ('aboriginal', 300), ('anglican', 881), ('airing', 628), ('acceleration', 336), ('1975', 113), ('19th', 139), ('apparition', 981), ('actreesess', 419), ('anxiously', 950), ('appeased', 991), ('andromeda', 864), ('antonellina', 944), ('anatomical', 843), ('adventuresome', 514), ('1965', 102), ('allied', 715), ('adolf', 482), ('altruistic', 757), ('actively', 413), ('apolitical', 970), ('abets', 290), ('abominations', 299), ('agenda', 575), ('abyss', 331), ('57', 221), ('adulthood', 501), ('91', 260), ('affirming', 541), ('55', 218), ('affixed', 543), ('afflict', 545), ('albinoni', 667), ('affable', 530), ('airtight', 639), ('accrued', 367), ('alley', 711), ('admiration', 468), ('amputees', 818), ('acquit', 398), ('applies', 999), ('android', 863), ('ancestral', 846), ('aces', 378), ('adventurers', 512), ('animator', 898), ('63rd', 228), ('affleck', 544), ('amair', 763), ('accounts', 366), ('07', 9), ('alain', 657), ('1950s', 90), ('antedote', 929), ('anway', 947), ('alyn', 760), ('ambling', 789), ('accusers', 375), ('707', 235), ('airbrushed', 624), ('adelaide', 453), ('animater', 896), ('alligator', 717), ('amandola', 765), ('adviser', 525), ('acheaology', 381), ('90c', 257), ('44c', 205), ('75c', 243), ('35c', 189), ('50c', 216), ('451', 207), ('affectations', 534), ('aimless', 617), ('1932', 73), ('2d', 176), ('300', 182), ('10yr', 22), ('aid', 606), ('aborigine', 301), ('aloofness', 734), ('amoured', 815), ('ambushing', 790), ('agreement', 595), ('abounds', 303), ('affecting', 536), ('acquits', 399), ('annihilates', 908), ('26', 170), ('2009', 154), ('annie', 907), ('1889', 54), ('apes', 966), ('analyse', 836), ('7th', 247), ('9th', 269), ('acme', 393), ('ambition', 786), ('allure', 724), ('aesthetics', 529), ('alderich', 674), ('advertisement', 520), ('1955', 93), ('analysis', 838), ('airstrike', 638), ('1935', 75), ('alzheimer', 761), ('aggression', 580), ('378', 194), ('adoring', 493), ('100th', 15), ('200th', 156), ('20s', 159), ('1910s', 62), ('14th', 37), ('aaa', 274), ('accessability', 346), ('advan', 503), ('animie', 901), ('absoulely', 315), ('alfonso', 681), ('androgynous', 862), ('amiable', 798), ('airs', 636), ('accustomed', 376), ('180', 47), ('1h30', 140), ('allies', 716), ('analogies', 835), ('academic', 332), ('1s', 141), ('2s', 180), ('anachronism', 831)]\n",
      "\n",
      " Type of bag_of_words.toarray() {} \n",
      "\n",
      "\n",
      " First feature vector, representing the first document \n",
      "   (0, 8867)\t1\n",
      "  (0, 8775)\t1\n",
      "  (0, 81)\t1\n",
      "  (0, 1577)\t1\n",
      "  (0, 1207)\t1\n",
      "  (0, 15731)\t1\n",
      "  (0, 1407)\t1\n",
      "  (0, 16100)\t1\n",
      "  (0, 6169)\t1\n",
      "  (0, 14838)\t1\n",
      "  (0, 7606)\t1\n",
      "  (0, 6142)\t1\n",
      "  (0, 16069)\t1\n",
      "  (0, 8781)\t1\n",
      "  (0, 9594)\t1\n",
      "  (0, 15085)\t1\n",
      "  (0, 11683)\t1\n",
      "  (0, 10593)\t2\n",
      "  (0, 17392)\t1\n",
      "  (0, 11522)\t1\n",
      "  (0, 1761)\t1\n",
      "  (0, 9987)\t1\n",
      "  (0, 12164)\t1\n",
      "  (0, 2397)\t1\n",
      "  (0, 8127)\t1\n",
      "  :\t:\n",
      "  (0, 8648)\t1\n",
      "  (0, 15292)\t3\n",
      "  (0, 6658)\t2\n",
      "  (0, 14327)\t1\n",
      "  (0, 11526)\t1\n",
      "  (0, 5667)\t1\n",
      "  (0, 10591)\t1\n",
      "  (0, 8575)\t5\n",
      "  (0, 7423)\t2\n",
      "  (0, 6631)\t3\n",
      "  (0, 6800)\t2\n",
      "  (0, 2352)\t1\n",
      "  (0, 3074)\t1\n",
      "  (0, 9818)\t1\n",
      "  (0, 6666)\t1\n",
      "  (0, 698)\t1\n",
      "  (0, 9552)\t1\n",
      "  (0, 7643)\t2\n",
      "  (0, 853)\t3\n",
      "  (0, 8644)\t2\n",
      "  (0, 8703)\t2\n",
      "  (0, 1127)\t4\n",
      "  (0, 15155)\t1\n",
      "  (0, 13581)\t5\n",
      "  (0, 13667)\t2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8566666666666667\n",
      "Percentage of correct predicted values0.8566666666666667\n"
     ]
    }
   ],
   "source": [
    "CountVec = CountVectorizer()\n",
    "# Creating the BoW with the set of all the documents and transforming the documents in feature vectors\n",
    "bag_of_words = CountVec.fit_transform(df['reviews'])\n",
    "\n",
    "print(type(bag_of_words))\n",
    "print('\\n Number of raws {} (documents) -- Number of columns {} (vocabulary) \\n'.format(bag_of_words.shape[0], bag_of_words.shape[1]))\n",
    "\n",
    "# https://machinelearningmastery.com/sparse-matrices-for-machine-learning/\n",
    "# This is a sparse matrix \n",
    "print('\\n Type of bag_of_words {} \\n'.format(type(bag_of_words)))\n",
    "sparsity = 1.0 - bag_of_words.nnz / (bag_of_words.shape[0] * bag_of_words.shape[1])\n",
    "print('\\n Sparsity {} \\n'.format(sparsity))\n",
    "\n",
    "# This is a  \n",
    "print('\\n Type of bag_of_words.toarray {} \\n'.format(type(bag_of_words.toarray())))\n",
    "\n",
    "# In CountVec we have the vocabulary as an attribute\n",
    "print('\\n Type of CountVec.vocabulary {} \\n'.format(type(CountVec.vocabulary_)))\n",
    "print('A sample of CountVec.vocabulary_ {}'.format([(k, v) for k, v in CountVec.vocabulary_.items() if v < 1000]))\n",
    "\n",
    "# In bag_of_words we have the vector features representing each single document \n",
    "print('\\n Type of bag_of_words.toarray() {} \\n')\n",
    "print('\\n First feature vector, representing the first document \\n', bag_of_words[0, :])\n",
    "\n",
    "# Lets get our training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(bag_of_words.toarray(), df['sentiment'].values, test_size=0.25)\n",
    "\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "\n",
    "results = [(predicted, actual) for predicted, actual in zip(clf.predict(X_test),  y_test) \n",
    "           if  predicted == actual]\n",
    "\n",
    "print('Percentage of correct predicted values{}'.format(len(results)/len(y_test)))\n",
    "\n",
    "\n",
    "# lr_CV = Pipeline([('vect', CountVec), ('clf', LogisticRegression(random_state=0))])\n",
    "\n",
    "\n",
    "\n",
    "# lr_CV.fit(X_train, y_train)\n",
    "# print('Train accuracy {}'.format(lr_CV.score(X_train, y_train)))\n",
    "# print('Test accuracy {}'.format(lr_CV.score(X_test, y_test)))\n",
    "\n",
    "# # # Trying with cross_val_score\n",
    "# lr = LogisticRegression()\n",
    "# k_folds = 10\n",
    "# X_train_CV = CountVec.fit_transform(X_train)\n",
    "# type(X_train_CV)\n",
    "# print('Train accuracy list {} '.format(cross_val_score(lr, X_train_CV, y_train, cv= k_folds))) \n",
    "# print('Train accuracy mean {} '.format(cross_val_score(lr, X_train_CV, y_train, cv= k_folds).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regress model using TfidfVectorizer vectorisation technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 0.982\n",
      "Test accuracy 0.88\n",
      "Train accuracy list [0.89403974 0.89403974 0.86754967 0.88666667 0.85333333 0.83333333\n",
      " 0.89333333 0.86577181 0.89261745 0.89932886] \n",
      "Train accuracy mean 0.8780013926544884 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf), ('clf', LogisticRegression(random_state=0))])\n",
    "lr_tfidf.fit(X_train, y_train)\n",
    "print('Train accuracy {}'.format(lr_tfidf.score(X_train, y_train)))\n",
    "print('Test accuracy {}'.format(lr_tfidf.score(X_test, y_test)))\n",
    "\n",
    "# Trying with cross_val_score\n",
    "lr = LogisticRegression()\n",
    "k_folds = 10\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "print('Train accuracy list {} '.format(cross_val_score(lr, X_train_tfidf, y_train, cv= k_folds))) \n",
    "print('Train accuracy mean {} '.format(cross_val_score(lr, X_train_tfidf, y_train, cv= k_folds).mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regress model using TfidfVectorizer and different values for C hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_value 1.0 Test Score 0.88 Train_score 0.982\n",
      "C_value 1.1 Test Score 0.886 Train_score 0.9853333333333333\n",
      "C_value 1.2000000000000002 Test Score 0.886 Train_score 0.9866666666666667\n",
      "C_value 1.3000000000000003 Test Score 0.886 Train_score 0.9873333333333333\n",
      "C_value 1.4000000000000004 Test Score 0.886 Train_score 0.9886666666666667\n",
      "C_value 1.5000000000000004 Test Score 0.886 Train_score 0.9886666666666667\n",
      "C_value 1.6000000000000005 Test Score 0.888 Train_score 0.9893333333333333\n",
      "C_value 1.7000000000000006 Test Score 0.888 Train_score 0.99\n",
      "C_value 1.8000000000000007 Test Score 0.888 Train_score 0.99\n",
      "C_value 1.9000000000000008 Test Score 0.89 Train_score 0.99\n"
     ]
    }
   ],
   "source": [
    "C_values = np.arange(1,2,0.1)\n",
    "results = []\n",
    "\n",
    "for value in C_values:   \n",
    "    lr_tfidf = Pipeline([('vect', tfidf), ('clf', LogisticRegression(random_state=0, C=value))])\n",
    "    lr_tfidf.fit(X_train, y_train)\n",
    "    train_score = lr_tfidf.score(X_train, y_train)\n",
    "    score = lr_tfidf.score(X_test, y_test)\n",
    "    print('C_value {} Test Score {} Train_score {}'.format(value, score, train_score))\n",
    "    results.append(score)\n",
    "\n",
    "time_end_of_notebook = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "colab_type": "code",
    "id": "6siRLHQU79F7",
    "outputId": "64142e5a-67d5-408b-e067-45f2551d0d17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 1000\n",
      "Full notebook execution duration: 1743.947651386261 seconds\n",
      "Full notebook execution duration: 29.065794189771015 minutes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>Vectorisation techniques</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>Pending</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>TFIDF</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Models Vectorisation techniques    Score\n",
       "0  Logistic Regression             Bag of Words     0.89\n",
       "1  Logistic Regression                 Word2Vec  Pending\n",
       "2  Logistic Regression                    TFIDF     0.99"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "table_models_vectorization = pd.DataFrame(\n",
    "     {'Models':                   [\"Logistic Regression\", \"Logistic Regression\", \"Logistic Regression\"], \n",
    "      'Vectorisation techniques': [\"Bag of Words\",        \"Word2Vec\", \"TFIDF\"], \n",
    "      'Score':                    [score,                 \"Pending\", lr_tfidf.score(X_train, y_train) ]},\n",
    "    columns=['Models','Vectorisation techniques','Score']\n",
    ")\n",
    "print(\"Sample size:\", SAMPLE_SIZE)\n",
    "\n",
    "duration = time_end_of_notebook - time_beginning_of_notebook\n",
    "\n",
    "print(\"Full notebook execution duration:\", duration, \"seconds\")\n",
    "print(\"Full notebook execution duration:\", duration / 60, \"minutes\")\n",
    "\n",
    "table_models_vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Sentiment analysis of movies (IMDB).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
