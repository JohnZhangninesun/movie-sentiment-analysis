{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Sentiment Analysis with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "colab_type": "code",
    "id": "CML_IG6z-iwM",
    "outputId": "d9301f36-c1cf-4b3f-e639-a731880ed036"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/swami/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# uncomment these for Google collab, will have already been installed in local environment \n",
    "# if 'pip install -r requirements.txt' has been run\n",
    "#!pip install nltk\n",
    "#!pip install --upgrade gensim\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "from pdb import set_trace\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import nltk\n",
    "\n",
    "\n",
    "import glob\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FJiWamI00hBp",
    "outputId": "e3c105d5-037f-4521-b8e1-a83cb7a5edeb"
   },
   "outputs": [],
   "source": [
    "# MacOSX: See https://www.mkyong.com/mac/wget-on-mac-os-x/ for wget\n",
    "if not os.path.isdir('../aclImdb'):\n",
    "    if not os.path.isfile('../aclImdb_v1.tar.gz'):\n",
    "      !wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz \n",
    "\n",
    "    if not os.path.isdir('../aclImdb'):  \n",
    "      !tar -xf aclImdb_v1.tar.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U5Tnmoh-Dpfk"
   },
   "outputs": [],
   "source": [
    "time_beginning_of_notebook = time.time()\n",
    "SAMPLE_SIZE=3000\n",
    "positive_sample_file_list = glob.glob(os.path.join('../aclImdb/train/pos', \"*.txt\"))\n",
    "positive_sample_file_list = positive_sample_file_list[:SAMPLE_SIZE]\n",
    "\n",
    "negative_sample_file_list = glob.glob(os.path.join('../aclImdb/train/neg', \"*.txt\"))\n",
    "negative_sample_file_list = negative_sample_file_list[:SAMPLE_SIZE]\n",
    "\n",
    "import re\n",
    "\n",
    "# load doc into memory\n",
    "# regex to clean markup elements \n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', encoding='utf8')\n",
    "    # read all text\n",
    "    text = re.sub('<[^>]*>', ' ', file.read())\n",
    "    #text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "lfr3bXOgXNJJ",
    "outputId": "cc06dd0d-e886-4090-c972-cd05520adaf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive review(s): Bizarre horror movie filled with famous faces but stolen by Cristina Raines (later of TV's \"Flamingo\n",
      "Negative review(s): Well...tremors I, the original started off in 1990 and i found the movie quite enjoyable to watch. h\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df_positives = pd.DataFrame({'reviews':[load_doc(x) for x in positive_sample_file_list], 'sentiment': np.ones(SAMPLE_SIZE)})\n",
    "df_negatives = pd.DataFrame({'reviews':[load_doc(x) for x in negative_sample_file_list], 'sentiment': np.zeros(SAMPLE_SIZE)})\n",
    "\n",
    "print(\"Positive review(s):\", df_positives['reviews'][1][:100])\n",
    "print(\"Negative review(s):\", df_negatives['reviews'][1][:100])\n",
    "\n",
    "df = pd.concat([df_positives, df_negatives], ignore_index=True)\n",
    "\n",
    "df = shuffle(df)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['reviews'], df['sentiment'], test_size=0.25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logic to compute DAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML STUDY GROUP\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "class PreProcessor:\n",
    "    def __init__(self,REVIEWS,REVIEWS_VAL,LABELS,LABELS_VAL,WE_FILE):\n",
    "        self.reviews = REVIEWS\n",
    "        self.reviews_val = REVIEWS_VAL\n",
    "        self.labels = LABELS\n",
    "        self.labels_val = LABELS_VAL\n",
    "        self.we_file = WE_FILE\n",
    "\n",
    "    def tokenize(self):\n",
    "#         set_trace()\n",
    "        print(self.reviews[0])\n",
    "\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(self.reviews)\n",
    "\n",
    "        self.sequences = tokenizer.texts_to_sequences(self.reviews)\n",
    "        self.sequences_val = tokenizer.texts_to_sequences(self.reviews_val)\n",
    "\n",
    "        self.word_index = tokenizer.word_index\n",
    "        print(\"Found %s unique tokens\" %(len(self.word_index)))\n",
    "\n",
    "    def make_data(self):\n",
    "        self.MAX_SEQUENCE_LENGTH = max([len(self.sequences[i]) for i in range(len(self.sequences))])\n",
    "        print(\"self.MAX_SEQUENCE_LENGTH: {}\".format(self.MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "        data = pad_sequences(self.sequences,maxlen=self.MAX_SEQUENCE_LENGTH)\n",
    "        data_val = pad_sequences(self.sequences_val,maxlen=self.MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "        labels = np.zeros((len(self.labels),1))\n",
    "        labels_val = np.zeros((len(self.labels_val),1))\n",
    "\n",
    "        for i in range(len(self.labels)):\n",
    "            labels[i] = self.labels[i]\n",
    "\n",
    "        for i in range(len(self.labels_val)):\n",
    "            labels_val[i] = self.labels_val[i]\n",
    "        \n",
    "        labels = to_categorical(labels)\n",
    "        labels_val = to_categorical(labels_val)\n",
    "\n",
    "        print(\"Shape of data tensor: \" +str(data.shape))\n",
    "        print(\"Shape of label tensor: \" +str(labels.shape))\n",
    "\n",
    "        return data, data_val, labels, labels_val\n",
    "        \n",
    "    def get_word_embedding_matrix(self,EMBEDDING_DIM=100):\n",
    "        embeddings_index = {}\n",
    "\n",
    "        if self.we_file == \"rand\":\n",
    "            return None\n",
    "\n",
    "        f = open(self.we_file)\n",
    "\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        f.close()\n",
    "\n",
    "        print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "        self.embedding_matrix = np.zeros((len(self.word_index)+1, EMBEDDING_DIM))\n",
    "\n",
    "        for word, i in self.word_index.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                self.embedding_matrix[i] = embedding_vector\n",
    "\n",
    "        return self.embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from dan.custom_layers import AverageWords, WordDropout\n",
    "\n",
    "from keras.layers import Embedding, Dense, Input, BatchNormalization, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adagrad, Adam\n",
    "from keras import backend as K\n",
    "\n",
    "from pdb import set_trace\n",
    "\n",
    "embedding_dim = 300\n",
    "num_hidden_layers = 3\n",
    "num_hidden_units = 300\n",
    "num_epochs = 100\n",
    "batch_size = 100\n",
    "dropout_rate = 0.2\n",
    "word_dropout_rate = 0.3\n",
    "activation = 'relu'\n",
    "\n",
    "args = {}\n",
    "args['We']='data/glove.6B.300d.txt'\n",
    "args['Wels']='' ### rand or ''\n",
    "args['model']='dan'  ### nbow OR dan\n",
    "args['wd']='y'\n",
    "\n",
    "reviews=X_train.values\n",
    "reviews_val=X_test.values\n",
    "labels=y_train.values\n",
    "labels_val=y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_val.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie didn't really surprise me, as such, it just got better and better. I thought: \"Paul Rieser wrote this, huh? Well...we'll see how he does...\" Then I saw Peter Falk was in it. I appreciate Colombo. Even though I was never a big fan of the show, I've always liked watching Peter Falk.   The performances of Peter and Paul were so natural that I felt like a fly on the wall. They played off of each other so well that I practically felt giddy with enjoyment! ...And I hadn't even been drinking!  This movie was so well done that I wanted to get right on the phone to Paul and let him know how much I enjoyed it! but I couldn't find his number. Must be unlisted or something.  This was one of those movies that I had no idea what it was going to be about or who was in it or anything. It just came on and I thought:\"Eh, why not? Let's see. If I don't like it - I don't have to watch it...\" ...and I ended up just loving it!\n",
      "Found 41247 unique tokens\n",
      "self.MAX_SEQUENCE_LENGTH: 2473\n",
      "Shape of data tensor: (4500, 2473)\n",
      "Shape of label tensor: (4500, 2)\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "pp = PreProcessor(reviews,reviews_val,labels,labels_val,args['We'])\n",
    "pp.tokenize()\n",
    "\n",
    "reviews,reviews_val,labels,labels_val = pp.make_data()\n",
    "\n",
    "embedding_matrix = pp.get_word_embedding_matrix(embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "if args['Wels'] == \"rand\":\n",
    "    model.add(Embedding(len(pp.word_index) + 1,embedding_dim,input_length=pp.MAX_SEQUENCE_LENGTH,trainable=False))\n",
    "else:\n",
    "    model.add(Embedding(len(pp.word_index)+1,embedding_dim,weights=[embedding_matrix],input_length=pp.MAX_SEQUENCE_LENGTH,trainable=False))\n",
    "\n",
    "if args['wd'] == 'y':\n",
    "    model.add(WordDropout(word_dropout_rate))\n",
    "model.add(AverageWords())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2473\n",
      "reviews.shape: (4500, 2473)\n",
      "reviews_val.shape: (1500, 2473)\n",
      "labels.shape: (4500, 2)\n",
      "labels_val.shape: (1500, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4500"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('reviews.shape: ' + str(reviews.shape))\n",
    "print('reviews_val.shape: ' + str(reviews_val.shape))\n",
    "print('labels.shape: ' + str(labels.shape))\n",
    "print('labels_val.shape: ' + str(labels_val.shape))\n",
    "labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 2473, 300)         12374400  \n",
      "_________________________________________________________________\n",
      "word_dropout_12 (WordDropout (None, 2473, 300)         0         \n",
      "_________________________________________________________________\n",
      "average_words_12 (AverageWor (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_145 (Dense)            (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_145 (Bat (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation_145 (Activation)  (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_145 (Dropout)        (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_146 (Dense)            (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_146 (Bat (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation_146 (Activation)  (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_146 (Dropout)        (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_147 (Dense)            (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_147 (Bat (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation_147 (Activation)  (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_147 (Dropout)        (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_148 (Dense)            (None, 1)                 301       \n",
      "_________________________________________________________________\n",
      "batch_normalization_148 (Bat (None, 1)                 4         \n",
      "_________________________________________________________________\n",
      "dropout_148 (Dropout)        (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "activation_148 (Activation)  (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_149 (Dense)            (None, 300)               600       \n",
      "_________________________________________________________________\n",
      "batch_normalization_149 (Bat (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation_149 (Activation)  (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_149 (Dropout)        (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_150 (Dense)            (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_150 (Bat (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation_150 (Activation)  (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_150 (Dropout)        (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_151 (Dense)            (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_151 (Bat (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation_151 (Activation)  (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_151 (Dropout)        (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_152 (Dense)            (None, 2)                 602       \n",
      "_________________________________________________________________\n",
      "batch_normalization_152 (Bat (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "dropout_152 (Dropout)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "activation_152 (Activation)  (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 12,834,615\n",
      "Trainable params: 456,609\n",
      "Non-trainable params: 12,378,006\n",
      "_________________________________________________________________\n",
      "Train on 4500 samples, validate on 1500 samples\n",
      "Epoch 1/100\n",
      "4500/4500 [==============================] - 27s 6ms/step - loss: 0.8279 - categorical_accuracy: 0.4947 - val_loss: 7.7904 - val_categorical_accuracy: 0.5167\n",
      "Epoch 2/100\n",
      "4500/4500 [==============================] - 19s 4ms/step - loss: 0.7555 - categorical_accuracy: 0.4962 - val_loss: 1.6806 - val_categorical_accuracy: 0.5167\n",
      "Epoch 3/100\n",
      "2000/4500 [============>.................] - ETA: 9s - loss: 0.7442 - categorical_accuracy: 0.4885"
     ]
    }
   ],
   "source": [
    "if args['model'] == 'dan':\n",
    "    for i in range(num_hidden_layers):\n",
    "        model.add(Dense(num_hidden_units))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation(activation))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "model.add(Dense(labels.shape[1]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "adam = Adam()\n",
    "model.compile(loss='categorical_crossentropy',optimizer=adam,metrics=['categorical_accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(reviews,labels,batch_size=batch_size,epochs=num_epochs,\\\n",
    "          validation_data=(reviews_val,labels_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
