{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Sentiment Analysis with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "colab_type": "code",
    "id": "CML_IG6z-iwM",
    "outputId": "d9301f36-c1cf-4b3f-e639-a731880ed036"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/michael/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# uncomment these for Google collab, will have already been installed in local environment \n",
    "# if 'pip install -r requirements.txt' has been run\n",
    "#!pip install nltk\n",
    "#!pip install --upgrade gensim\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "from pdb import set_trace\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import nltk\n",
    "\n",
    "\n",
    "import glob\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FJiWamI00hBp",
    "outputId": "e3c105d5-037f-4521-b8e1-a83cb7a5edeb"
   },
   "outputs": [],
   "source": [
    "# MacOSX: See https://www.mkyong.com/mac/wget-on-mac-os-x/ for wget\n",
    "if not os.path.isdir('../aclImdb'):\n",
    "    if not os.path.isfile('../aclImdb_v1.tar.gz'):\n",
    "      !wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz \n",
    "\n",
    "    if not os.path.isdir('../aclImdb'):  \n",
    "      !tar -xf aclImdb_v1.tar.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U5Tnmoh-Dpfk"
   },
   "outputs": [],
   "source": [
    "time_beginning_of_notebook = time.time()\n",
    "SAMPLE_SIZE=12500\n",
    "positive_sample_file_list = glob.glob(os.path.join('../aclImdb/train/pos', \"*.txt\"))\n",
    "# positive_sample_file_list = positive_sample_file_list[:SAMPLE_SIZE]\n",
    "\n",
    "negative_sample_file_list = glob.glob(os.path.join('../aclImdb/train/neg', \"*.txt\"))\n",
    "# negative_sample_file_list = negative_sample_file_list[:SAMPLE_SIZE]\n",
    "\n",
    "import re\n",
    "\n",
    "# load doc into memory\n",
    "# regex to clean markup elements \n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', encoding='utf8')\n",
    "    # read all text\n",
    "    text = re.sub('<[^>]*>', ' ', file.read())\n",
    "    #text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "lfr3bXOgXNJJ",
    "outputId": "cc06dd0d-e886-4090-c972-cd05520adaf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive review(s): There are few really hilarious films about science fiction but this one will knock your sox off. The\n",
      "Negative review(s): I can find very little thats good to say about this film. I am sure the idea and script looked good \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df_positives = pd.DataFrame({'reviews':[load_doc(x) for x in positive_sample_file_list], 'sentiment': np.ones(SAMPLE_SIZE)})\n",
    "df_negatives = pd.DataFrame({'reviews':[load_doc(x) for x in negative_sample_file_list], 'sentiment': np.zeros(SAMPLE_SIZE)})\n",
    "\n",
    "print(\"Positive review(s):\", df_positives['reviews'][1][:100])\n",
    "print(\"Negative review(s):\", df_negatives['reviews'][1][:100])\n",
    "\n",
    "df = pd.concat([df_positives, df_negatives], ignore_index=True)\n",
    "\n",
    "df = shuffle(df)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['reviews'], df['sentiment'], test_size=0.25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logic to compute DAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML STUDY GROUP\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "class PreProcessor:\n",
    "    def __init__(self,REVIEWS,REVIEWS_VAL,LABELS,LABELS_VAL,WE_FILE):\n",
    "        self.reviews = REVIEWS\n",
    "        self.reviews_val = REVIEWS_VAL\n",
    "        self.labels = LABELS\n",
    "        self.labels_val = LABELS_VAL\n",
    "        self.we_file = WE_FILE\n",
    "\n",
    "    def tokenize(self):\n",
    "#         set_trace()\n",
    "        print(self.reviews[0])\n",
    "\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(self.reviews)\n",
    "\n",
    "        self.sequences = tokenizer.texts_to_sequences(self.reviews)\n",
    "        self.sequences_val = tokenizer.texts_to_sequences(self.reviews_val)\n",
    "\n",
    "        self.word_index = tokenizer.word_index\n",
    "        print(\"Found %s unique tokens\" %(len(self.word_index)))\n",
    "\n",
    "    def make_data(self):\n",
    "        self.MAX_SEQUENCE_LENGTH = max([len(self.sequences[i]) for i in range(len(self.sequences))])\n",
    "        print(\"self.MAX_SEQUENCE_LENGTH: {}\".format(self.MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "        review = pad_sequences(self.sequences,maxlen=self.MAX_SEQUENCE_LENGTH)\n",
    "        review_val = pad_sequences(self.sequences_val,maxlen=self.MAX_SEQUENCE_LENGTH)\n",
    "        \n",
    "        labels = to_categorical(self.labels)\n",
    "        labels_val = to_categorical(self.labels_val)\n",
    "\n",
    "        print(\"Shape of data tensor: \" +str(review.shape))\n",
    "        print(\"Shape of label tensor: \" +str(labels.shape))\n",
    "\n",
    "        return review, review_val, labels, labels_val\n",
    "        \n",
    "    def get_word_embedding_matrix(self,EMBEDDING_DIM=100):\n",
    "        embeddings_index = {}\n",
    "\n",
    "        if self.we_file == \"rand\":\n",
    "            return None\n",
    "\n",
    "        f = open(self.we_file)\n",
    "\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        f.close()\n",
    "\n",
    "        print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "        self.embedding_matrix = np.zeros((len(self.word_index)+1, EMBEDDING_DIM))\n",
    "\n",
    "        for word, i in self.word_index.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                self.embedding_matrix[i] = embedding_vector\n",
    "\n",
    "        return self.embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from dan.custom_layers import AverageWords, WordDropout\n",
    "\n",
    "from keras.layers import Embedding, Dense, Input, BatchNormalization, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adagrad, Adam\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from pdb import set_trace\n",
    "\n",
    "embedding_dim = 300\n",
    "num_hidden_layers = 3\n",
    "num_hidden_units = 300\n",
    "num_epochs = 100\n",
    "batch_size = 512\n",
    "dropout_rate = 0.2\n",
    "word_dropout_rate = 0.3\n",
    "activation = 'relu'\n",
    "\n",
    "args = {}\n",
    "args['We']='./glove.6B.300d.txt'\n",
    "args['Wels']='' ### rand or ''\n",
    "args['model']='dan'  ### nbow OR dan\n",
    "args['wd']='y'\n",
    "\n",
    "reviews=X_train.values\n",
    "reviews_val=X_test.values\n",
    "labels=y_train.values\n",
    "labels_val=y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_val.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What can one say about Elvira that hasn't already been said in the world's press? The classic comedienne that IS Elvira delivers in her first full-length big budget comedy masterpiece.  From the very first movie frame thingy, Elvira packs an acting punch that clearly says Film Great....eat your heart out, Bette Davis! See a forlorn Elvira, see an excitable Elvira, see a jealous Elvira, see a murderous Elvira. You can do nothing but marvel at her acting prowess!  At the heart of this comedy masterpiece is Elvira's desire for Las Vegas show stardom. Despite putting \"the boob back in the boobtube\" as a horror hostess (with the mostest), Elvira finds the small screen constrictive emotionally....and PHYSICALLY! Nuff said, she packs up her kitbag and heads East....a hotdog in one hand and a letter from her Aunt's lawyer outlining her inheritance 'windfall' in the other.  I've seen this movie so many times, I can almost recite it verbatim....(verbatim would just be showing off)!  Grab a copy, laugh yourself silly, learn the lines....  Why she didn't win the Best Actress Oscar for this role is beyond me.\n",
      "Found 78189 unique tokens\n",
      "self.MAX_SEQUENCE_LENGTH: 1841\n",
      "Shape of data tensor: (18750, 1841)\n",
      "Shape of label tensor: (18750, 2)\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "pp = PreProcessor(reviews,reviews_val,labels,labels_val,args['We'])\n",
    "pp.tokenize()\n",
    "\n",
    "reviews,reviews_val,labels,labels_val = pp.make_data()\n",
    "\n",
    "embedding_matrix = pp.get_word_embedding_matrix(embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "if args['Wels'] == \"rand\":\n",
    "    model.add(Embedding(len(pp.word_index) + 1,embedding_dim,input_length=pp.MAX_SEQUENCE_LENGTH,trainable=False))\n",
    "else:\n",
    "    model.add(Embedding(len(pp.word_index)+1,embedding_dim,weights=[embedding_matrix],input_length=pp.MAX_SEQUENCE_LENGTH,trainable=False))\n",
    "\n",
    "if args['wd'] == 'y':\n",
    "    model.add(WordDropout(word_dropout_rate))\n",
    "model.add(AverageWords())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviews.shape: (18750, 1841)\n",
      "reviews_val.shape: (6250, 1841)\n",
      "labels.shape: (18750, 2)\n",
      "labels_val.shape: (6250, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18750"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('reviews.shape: ' + str(reviews.shape))\n",
    "print('reviews_val.shape: ' + str(reviews_val.shape))\n",
    "print('labels.shape: ' + str(labels.shape))\n",
    "print('labels_val.shape: ' + str(labels_val.shape))\n",
    "labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 1841, 300)         23457000  \n",
      "_________________________________________________________________\n",
      "word_dropout_6 (WordDropout) (None, 1841, 300)         0         \n",
      "_________________________________________________________________\n",
      "average_words_6 (AverageWord (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 2)                 602       \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 23,732,110\n",
      "Trainable params: 273,306\n",
      "Non-trainable params: 23,458,804\n",
      "_________________________________________________________________\n",
      "Train on 18750 samples, validate on 6250 samples\n",
      "Epoch 1/100\n",
      "18750/18750 [==============================] - 5s 265us/step - loss: 0.6600 - acc: 0.6375 - categorical_accuracy: 0.6375 - val_loss: 0.5029 - val_acc: 0.7632 - val_categorical_accuracy: 0.7632\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.50290, saving model to best.weights\n",
      "Epoch 2/100\n",
      "18750/18750 [==============================] - 4s 206us/step - loss: 0.4980 - acc: 0.7626 - categorical_accuracy: 0.7626 - val_loss: 0.6187 - val_acc: 0.7176 - val_categorical_accuracy: 0.7176\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.50290\n",
      "Epoch 3/100\n",
      "18750/18750 [==============================] - 4s 203us/step - loss: 0.4540 - acc: 0.7918 - categorical_accuracy: 0.7918 - val_loss: 0.4842 - val_acc: 0.7856 - val_categorical_accuracy: 0.7856\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.50290 to 0.48423, saving model to best.weights\n",
      "Epoch 4/100\n",
      "18750/18750 [==============================] - 4s 204us/step - loss: 0.4447 - acc: 0.7998 - categorical_accuracy: 0.7998 - val_loss: 0.3898 - val_acc: 0.8288 - val_categorical_accuracy: 0.8288\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.48423 to 0.38978, saving model to best.weights\n",
      "Epoch 5/100\n",
      "18750/18750 [==============================] - 4s 204us/step - loss: 0.4360 - acc: 0.7993 - categorical_accuracy: 0.7993 - val_loss: 0.3853 - val_acc: 0.8278 - val_categorical_accuracy: 0.8278\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.38978 to 0.38532, saving model to best.weights\n",
      "Epoch 6/100\n",
      "18750/18750 [==============================] - 4s 203us/step - loss: 0.4278 - acc: 0.8064 - categorical_accuracy: 0.8064 - val_loss: 0.4455 - val_acc: 0.7850 - val_categorical_accuracy: 0.7850\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.38532\n",
      "Epoch 7/100\n",
      "18750/18750 [==============================] - 4s 204us/step - loss: 0.4275 - acc: 0.8047 - categorical_accuracy: 0.8047 - val_loss: 0.3752 - val_acc: 0.8309 - val_categorical_accuracy: 0.8309\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.38532 to 0.37522, saving model to best.weights\n",
      "Epoch 8/100\n",
      "18750/18750 [==============================] - 4s 207us/step - loss: 0.4212 - acc: 0.8100 - categorical_accuracy: 0.8100 - val_loss: 0.3707 - val_acc: 0.8342 - val_categorical_accuracy: 0.8342\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.37522 to 0.37068, saving model to best.weights\n",
      "Epoch 9/100\n",
      "18750/18750 [==============================] - 4s 207us/step - loss: 0.4225 - acc: 0.8070 - categorical_accuracy: 0.8070 - val_loss: 0.4048 - val_acc: 0.8139 - val_categorical_accuracy: 0.8139\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.37068\n",
      "Epoch 10/100\n",
      "18750/18750 [==============================] - 4s 203us/step - loss: 0.4162 - acc: 0.8108 - categorical_accuracy: 0.8108 - val_loss: 0.3956 - val_acc: 0.8171 - val_categorical_accuracy: 0.8171\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.37068\n",
      "Epoch 11/100\n",
      "18750/18750 [==============================] - 4s 202us/step - loss: 0.4072 - acc: 0.8167 - categorical_accuracy: 0.8167 - val_loss: 0.4223 - val_acc: 0.8085 - val_categorical_accuracy: 0.8085\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.37068\n",
      "Epoch 12/100\n",
      "18750/18750 [==============================] - 4s 201us/step - loss: 0.4052 - acc: 0.8196 - categorical_accuracy: 0.8196 - val_loss: 0.3621 - val_acc: 0.8418 - val_categorical_accuracy: 0.8418\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.37068 to 0.36206, saving model to best.weights\n",
      "Epoch 13/100\n",
      "18750/18750 [==============================] - 4s 199us/step - loss: 0.4049 - acc: 0.8163 - categorical_accuracy: 0.8163 - val_loss: 0.3627 - val_acc: 0.8403 - val_categorical_accuracy: 0.8403\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.36206\n",
      "Epoch 14/100\n",
      "18750/18750 [==============================] - 4s 198us/step - loss: 0.4112 - acc: 0.8142 - categorical_accuracy: 0.8142 - val_loss: 0.4290 - val_acc: 0.8099 - val_categorical_accuracy: 0.8099\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.36206\n",
      "Epoch 15/100\n",
      "18750/18750 [==============================] - 4s 199us/step - loss: 0.4056 - acc: 0.8140 - categorical_accuracy: 0.8140 - val_loss: 0.3754 - val_acc: 0.8339 - val_categorical_accuracy: 0.8339\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.36206\n",
      "Epoch 16/100\n",
      "18750/18750 [==============================] - 4s 199us/step - loss: 0.3969 - acc: 0.8189 - categorical_accuracy: 0.8189 - val_loss: 0.3628 - val_acc: 0.8387 - val_categorical_accuracy: 0.8387\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.36206\n",
      "Epoch 17/100\n",
      "18750/18750 [==============================] - 4s 196us/step - loss: 0.3949 - acc: 0.8237 - categorical_accuracy: 0.8237 - val_loss: 0.3831 - val_acc: 0.8414 - val_categorical_accuracy: 0.8414\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.36206\n",
      "Epoch 18/100\n",
      "18750/18750 [==============================] - 4s 197us/step - loss: 0.3957 - acc: 0.8237 - categorical_accuracy: 0.8237 - val_loss: 0.3796 - val_acc: 0.8299 - val_categorical_accuracy: 0.8299\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.36206\n",
      "Epoch 19/100\n",
      "18750/18750 [==============================] - 4s 196us/step - loss: 0.3878 - acc: 0.8254 - categorical_accuracy: 0.8254 - val_loss: 0.3618 - val_acc: 0.8398 - val_categorical_accuracy: 0.8398\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.36206 to 0.36183, saving model to best.weights\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18750/18750 [==============================] - 4s 203us/step - loss: 0.3908 - acc: 0.8233 - categorical_accuracy: 0.8233 - val_loss: 0.3682 - val_acc: 0.8381 - val_categorical_accuracy: 0.8381\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.36183\n",
      "Epoch 21/100\n",
      "18750/18750 [==============================] - 4s 206us/step - loss: 0.3844 - acc: 0.8280 - categorical_accuracy: 0.8280 - val_loss: 0.3633 - val_acc: 0.8366 - val_categorical_accuracy: 0.8366\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.36183\n",
      "Epoch 22/100\n",
      "18750/18750 [==============================] - 4s 203us/step - loss: 0.3844 - acc: 0.8228 - categorical_accuracy: 0.8228 - val_loss: 0.3866 - val_acc: 0.8240 - val_categorical_accuracy: 0.8240\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.36183\n",
      "Epoch 23/100\n",
      "18750/18750 [==============================] - 4s 204us/step - loss: 0.3788 - acc: 0.8272 - categorical_accuracy: 0.8272 - val_loss: 0.4132 - val_acc: 0.8224 - val_categorical_accuracy: 0.8224\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.36183\n",
      "Epoch 24/100\n",
      "18750/18750 [==============================] - 4s 201us/step - loss: 0.3790 - acc: 0.8298 - categorical_accuracy: 0.8298 - val_loss: 0.4135 - val_acc: 0.8221 - val_categorical_accuracy: 0.8221\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.36183\n",
      "Epoch 25/100\n",
      "18750/18750 [==============================] - 4s 201us/step - loss: 0.3739 - acc: 0.8315 - categorical_accuracy: 0.8315 - val_loss: 0.4221 - val_acc: 0.7920 - val_categorical_accuracy: 0.7920\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.36183\n",
      "Epoch 26/100\n",
      "18750/18750 [==============================] - 4s 203us/step - loss: 0.3771 - acc: 0.8287 - categorical_accuracy: 0.8287 - val_loss: 0.3735 - val_acc: 0.8352 - val_categorical_accuracy: 0.8352\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.36183\n",
      "Epoch 27/100\n",
      "18750/18750 [==============================] - 4s 199us/step - loss: 0.3684 - acc: 0.8357 - categorical_accuracy: 0.8357 - val_loss: 0.3854 - val_acc: 0.8320 - val_categorical_accuracy: 0.8320\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.36183\n",
      "Epoch 28/100\n",
      "18750/18750 [==============================] - 4s 201us/step - loss: 0.3691 - acc: 0.8350 - categorical_accuracy: 0.8350 - val_loss: 0.3919 - val_acc: 0.8240 - val_categorical_accuracy: 0.8240\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.36183\n",
      "Epoch 29/100\n",
      "18750/18750 [==============================] - 4s 201us/step - loss: 0.3569 - acc: 0.8405 - categorical_accuracy: 0.8405 - val_loss: 0.3673 - val_acc: 0.8352 - val_categorical_accuracy: 0.8352\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.36183\n",
      "Epoch 00029: early stopping\n"
     ]
    }
   ],
   "source": [
    "if args['model'] == 'dan':\n",
    "    for i in range(num_hidden_layers):\n",
    "        model.add(Dense(num_hidden_units))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation(activation))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "model.add(Dense(labels.shape[1]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "adam = Adam()\n",
    "model.compile(loss='categorical_crossentropy',optimizer=adam,metrics=['accuracy','categorical_accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model_checkpoint = ModelCheckpoint('best.weights', monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)\n",
    "\n",
    "callbacks = [model_checkpoint, early_stopping]\n",
    "\n",
    "history = model.fit(reviews,labels,batch_size=batch_size,epochs=num_epochs,\\\n",
    "          validation_data=(reviews_val,labels_val), callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   val_loss  val_acc  val_categorical_accuracy      loss       acc  \\\n",
      "0  0.362056  0.84176                   0.84176  0.405207  0.819573   \n",
      "\n",
      "   categorical_accuracy      title  sample_size  nb_epochs  \n",
      "0              0.819573  Keras DAN        12500         12  \n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(history.history)\n",
    "df=df[df['val_acc']==df.val_acc.max()]\n",
    "df.reset_index(inplace=True)\n",
    "df[\"title\"]=[\"Keras DAN\"]\n",
    "df[\"sample_size\"]=[SAMPLE_SIZE]\n",
    "df[\"nb_epochs\"]=[df.iloc[0][\"index\"]+1]\n",
    "df.drop(labels=\"index\",axis=1,inplace=True)\n",
    "print(df)\n",
    "df.to_csv(path_or_buf=df.iloc[0].title+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlsg",
   "language": "python",
   "name": "mlsg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
