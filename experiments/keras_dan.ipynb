{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Sentiment Analysis with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "colab_type": "code",
    "id": "CML_IG6z-iwM",
    "outputId": "d9301f36-c1cf-4b3f-e639-a731880ed036"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/swami/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# uncomment these for Google collab, will have already been installed in local environment \n",
    "# if 'pip install -r requirements.txt' has been run\n",
    "#!pip install nltk\n",
    "#!pip install --upgrade gensim\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "from pdb import set_trace\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import nltk\n",
    "\n",
    "\n",
    "import glob\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FJiWamI00hBp",
    "outputId": "e3c105d5-037f-4521-b8e1-a83cb7a5edeb"
   },
   "outputs": [],
   "source": [
    "# MacOSX: See https://www.mkyong.com/mac/wget-on-mac-os-x/ for wget\n",
    "if not os.path.isdir('../aclImdb'):\n",
    "    if not os.path.isfile('../aclImdb_v1.tar.gz'):\n",
    "      !wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz \n",
    "\n",
    "    if not os.path.isdir('../aclImdb'):  \n",
    "      !tar -xf aclImdb_v1.tar.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U5Tnmoh-Dpfk"
   },
   "outputs": [],
   "source": [
    "time_beginning_of_notebook = time.time()\n",
    "SAMPLE_SIZE=3000\n",
    "positive_sample_file_list = glob.glob(os.path.join('../aclImdb/train/pos', \"*.txt\"))\n",
    "positive_sample_file_list = positive_sample_file_list[:SAMPLE_SIZE]\n",
    "\n",
    "negative_sample_file_list = glob.glob(os.path.join('../aclImdb/train/neg', \"*.txt\"))\n",
    "negative_sample_file_list = negative_sample_file_list[:SAMPLE_SIZE]\n",
    "\n",
    "import re\n",
    "\n",
    "# load doc into memory\n",
    "# regex to clean markup elements \n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', encoding='utf8')\n",
    "    # read all text\n",
    "    text = re.sub('<[^>]*>', ' ', file.read())\n",
    "    #text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "lfr3bXOgXNJJ",
    "outputId": "cc06dd0d-e886-4090-c972-cd05520adaf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive review(s): Bizarre horror movie filled with famous faces but stolen by Cristina Raines (later of TV's \"Flamingo\n",
      "Negative review(s): Well...tremors I, the original started off in 1990 and i found the movie quite enjoyable to watch. h\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df_positives = pd.DataFrame({'reviews':[load_doc(x) for x in positive_sample_file_list], 'sentiment': np.ones(SAMPLE_SIZE)})\n",
    "df_negatives = pd.DataFrame({'reviews':[load_doc(x) for x in negative_sample_file_list], 'sentiment': np.zeros(SAMPLE_SIZE)})\n",
    "\n",
    "print(\"Positive review(s):\", df_positives['reviews'][1][:100])\n",
    "print(\"Negative review(s):\", df_negatives['reviews'][1][:100])\n",
    "\n",
    "df = pd.concat([df_positives, df_negatives], ignore_index=True)\n",
    "\n",
    "df = shuffle(df)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['reviews'], df['sentiment'], test_size=0.25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logic to compute DAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML STUDY GROUP\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "class PreProcessor:\n",
    "    def __init__(self,REVIEWS,REVIEWS_VAL,LABELS,LABELS_VAL,WE_FILE):\n",
    "        self.reviews = REVIEWS\n",
    "        self.reviews_val = REVIEWS_VAL\n",
    "        self.labels = LABELS\n",
    "        self.labels_val = LABELS_VAL\n",
    "        self.we_file = WE_FILE\n",
    "\n",
    "    def tokenize(self):\n",
    "#         set_trace()\n",
    "        print(self.reviews[0])\n",
    "\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(self.reviews)\n",
    "\n",
    "        self.sequences = tokenizer.texts_to_sequences(self.reviews)\n",
    "        self.sequences_val = tokenizer.texts_to_sequences(self.reviews_val)\n",
    "\n",
    "        self.word_index = tokenizer.word_index\n",
    "        print(\"Found %s unique tokens\" %(len(self.word_index)))\n",
    "\n",
    "    def make_data(self):\n",
    "        self.MAX_SEQUENCE_LENGTH = max([len(self.sequences[i]) for i in range(len(self.sequences))])\n",
    "        print(\"self.MAX_SEQUENCE_LENGTH: {}\".format(self.MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "        review = pad_sequences(self.sequences,maxlen=self.MAX_SEQUENCE_LENGTH)\n",
    "        review_val = pad_sequences(self.sequences_val,maxlen=self.MAX_SEQUENCE_LENGTH)\n",
    "        \n",
    "        labels = to_categorical(self.labels)\n",
    "        labels_val = to_categorical(self.labels_val)\n",
    "\n",
    "        print(\"Shape of data tensor: \" +str(review.shape))\n",
    "        print(\"Shape of label tensor: \" +str(labels.shape))\n",
    "\n",
    "        return review, review_val, labels, labels_val\n",
    "        \n",
    "    def get_word_embedding_matrix(self,EMBEDDING_DIM=100):\n",
    "        embeddings_index = {}\n",
    "\n",
    "        if self.we_file == \"rand\":\n",
    "            return None\n",
    "\n",
    "        f = open(self.we_file)\n",
    "\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        f.close()\n",
    "\n",
    "        print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "        self.embedding_matrix = np.zeros((len(self.word_index)+1, EMBEDDING_DIM))\n",
    "\n",
    "        for word, i in self.word_index.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                self.embedding_matrix[i] = embedding_vector\n",
    "\n",
    "        return self.embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from dan.custom_layers import AverageWords, WordDropout\n",
    "\n",
    "from keras.layers import Embedding, Dense, Input, BatchNormalization, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adagrad, Adam\n",
    "from keras import backend as K\n",
    "\n",
    "from pdb import set_trace\n",
    "\n",
    "embedding_dim = 300\n",
    "num_hidden_layers = 3\n",
    "num_hidden_units = 300\n",
    "num_epochs = 100\n",
    "batch_size = 512\n",
    "dropout_rate = 0.2\n",
    "word_dropout_rate = 0.3\n",
    "activation = 'relu'\n",
    "\n",
    "args = {}\n",
    "args['We']='data/glove.6B.300d.txt'\n",
    "args['Wels']='' ### rand or ''\n",
    "args['model']='dan'  ### nbow OR dan\n",
    "args['wd']='y'\n",
    "\n",
    "reviews=X_train.values\n",
    "reviews_val=X_test.values\n",
    "labels=y_train.values\n",
    "labels_val=y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_val.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie didn't really surprise me, as such, it just got better and better. I thought: \"Paul Rieser wrote this, huh? Well...we'll see how he does...\" Then I saw Peter Falk was in it. I appreciate Colombo. Even though I was never a big fan of the show, I've always liked watching Peter Falk.   The performances of Peter and Paul were so natural that I felt like a fly on the wall. They played off of each other so well that I practically felt giddy with enjoyment! ...And I hadn't even been drinking!  This movie was so well done that I wanted to get right on the phone to Paul and let him know how much I enjoyed it! but I couldn't find his number. Must be unlisted or something.  This was one of those movies that I had no idea what it was going to be about or who was in it or anything. It just came on and I thought:\"Eh, why not? Let's see. If I don't like it - I don't have to watch it...\" ...and I ended up just loving it!\n",
      "Found 41247 unique tokens\n",
      "self.MAX_SEQUENCE_LENGTH: 2473\n",
      "Shape of data tensor: (4500, 2473)\n",
      "Shape of label tensor: (4500, 2)\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "pp = PreProcessor(reviews,reviews_val,labels,labels_val,args['We'])\n",
    "pp.tokenize()\n",
    "\n",
    "reviews,reviews_val,labels,labels_val = pp.make_data()\n",
    "\n",
    "embedding_matrix = pp.get_word_embedding_matrix(embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "if args['Wels'] == \"rand\":\n",
    "    model.add(Embedding(len(pp.word_index) + 1,embedding_dim,input_length=pp.MAX_SEQUENCE_LENGTH,trainable=False))\n",
    "else:\n",
    "    model.add(Embedding(len(pp.word_index)+1,embedding_dim,weights=[embedding_matrix],input_length=pp.MAX_SEQUENCE_LENGTH,trainable=False))\n",
    "\n",
    "if args['wd'] == 'y':\n",
    "    model.add(WordDropout(word_dropout_rate))\n",
    "model.add(AverageWords())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviews.shape: (4500, 2473)\n",
      "reviews_val.shape: (1500, 2473)\n",
      "labels.shape: (4500, 2)\n",
      "labels_val.shape: (1500, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4500"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('reviews.shape: ' + str(reviews.shape))\n",
    "print('reviews_val.shape: ' + str(reviews_val.shape))\n",
    "print('labels.shape: ' + str(labels.shape))\n",
    "print('labels_val.shape: ' + str(labels_val.shape))\n",
    "labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 2473, 300)         12374400  \n",
      "_________________________________________________________________\n",
      "word_dropout_13 (WordDropout (None, 2473, 300)         0         \n",
      "_________________________________________________________________\n",
      "average_words_13 (AverageWor (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_165 (Dense)            (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_165 (Bat (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation_165 (Activation)  (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_165 (Dropout)        (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_166 (Dense)            (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_166 (Bat (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation_166 (Activation)  (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_166 (Dropout)        (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_167 (Dense)            (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_167 (Bat (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation_167 (Activation)  (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_167 (Dropout)        (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_168 (Dense)            (None, 2)                 602       \n",
      "_________________________________________________________________\n",
      "batch_normalization_168 (Bat (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "dropout_168 (Dropout)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "activation_168 (Activation)  (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 12,649,510\n",
      "Trainable params: 273,306\n",
      "Non-trainable params: 12,376,204\n",
      "_________________________________________________________________\n",
      "Train on 4500 samples, validate on 1500 samples\n",
      "Epoch 1/100\n",
      "4500/4500 [==============================] - 25s 6ms/step - loss: 0.7505 - acc: 0.5767 - categorical_accuracy: 0.5767 - val_loss: 0.6191 - val_acc: 0.6980 - val_categorical_accuracy: 0.6980\n",
      "Epoch 2/100\n",
      "4500/4500 [==============================] - 15s 3ms/step - loss: 0.6396 - acc: 0.6484 - categorical_accuracy: 0.6484 - val_loss: 0.5416 - val_acc: 0.7513 - val_categorical_accuracy: 0.7513\n",
      "Epoch 3/100\n",
      "4500/4500 [==============================] - 15s 3ms/step - loss: 0.5649 - acc: 0.7162 - categorical_accuracy: 0.7162 - val_loss: 0.5309 - val_acc: 0.7353 - val_categorical_accuracy: 0.7353\n",
      "Epoch 4/100\n",
      "4500/4500 [==============================] - 15s 3ms/step - loss: 0.5228 - acc: 0.7436 - categorical_accuracy: 0.7436 - val_loss: 0.5887 - val_acc: 0.7120 - val_categorical_accuracy: 0.7120\n",
      "Epoch 5/100\n",
      "4500/4500 [==============================] - 15s 3ms/step - loss: 0.4796 - acc: 0.7756 - categorical_accuracy: 0.7756 - val_loss: 0.5226 - val_acc: 0.7580 - val_categorical_accuracy: 0.7580\n",
      "Epoch 6/100\n",
      "4500/4500 [==============================] - 15s 3ms/step - loss: 0.4635 - acc: 0.7793 - categorical_accuracy: 0.7793 - val_loss: 0.5852 - val_acc: 0.7427 - val_categorical_accuracy: 0.7427\n",
      "Epoch 7/100\n",
      "4500/4500 [==============================] - 16s 3ms/step - loss: 0.4492 - acc: 0.7967 - categorical_accuracy: 0.7967 - val_loss: 0.5687 - val_acc: 0.7527 - val_categorical_accuracy: 0.7527\n",
      "Epoch 8/100\n",
      "4500/4500 [==============================] - 15s 3ms/step - loss: 0.4336 - acc: 0.8056 - categorical_accuracy: 0.8056 - val_loss: 0.4638 - val_acc: 0.7867 - val_categorical_accuracy: 0.7867\n",
      "Epoch 9/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.4211 - acc: 0.8127 - categorical_accuracy: 0.8127 - val_loss: 0.3994 - val_acc: 0.8093 - val_categorical_accuracy: 0.8093\n",
      "Epoch 10/100\n",
      "4500/4500 [==============================] - 24s 5ms/step - loss: 0.4199 - acc: 0.8178 - categorical_accuracy: 0.8178 - val_loss: 0.4656 - val_acc: 0.7827 - val_categorical_accuracy: 0.7827\n",
      "Epoch 11/100\n",
      "4500/4500 [==============================] - 16s 4ms/step - loss: 0.4065 - acc: 0.8169 - categorical_accuracy: 0.8169 - val_loss: 0.4723 - val_acc: 0.7713 - val_categorical_accuracy: 0.7713\n",
      "Epoch 12/100\n",
      "4500/4500 [==============================] - 16s 4ms/step - loss: 0.4058 - acc: 0.8211 - categorical_accuracy: 0.8211 - val_loss: 0.4441 - val_acc: 0.7887 - val_categorical_accuracy: 0.7887\n",
      "Epoch 13/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.3943 - acc: 0.8244 - categorical_accuracy: 0.8244 - val_loss: 0.4458 - val_acc: 0.7873 - val_categorical_accuracy: 0.7873\n",
      "Epoch 14/100\n",
      "4500/4500 [==============================] - 16s 4ms/step - loss: 0.3982 - acc: 0.8340 - categorical_accuracy: 0.8340 - val_loss: 0.4232 - val_acc: 0.7967 - val_categorical_accuracy: 0.7967\n",
      "Epoch 15/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.3973 - acc: 0.8264 - categorical_accuracy: 0.8264 - val_loss: 0.3800 - val_acc: 0.8300 - val_categorical_accuracy: 0.8300\n",
      "Epoch 16/100\n",
      "4500/4500 [==============================] - 16s 4ms/step - loss: 0.3936 - acc: 0.8318 - categorical_accuracy: 0.8318 - val_loss: 0.4362 - val_acc: 0.7960 - val_categorical_accuracy: 0.7960\n",
      "Epoch 17/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.3922 - acc: 0.8278 - categorical_accuracy: 0.8278 - val_loss: 0.4086 - val_acc: 0.8200 - val_categorical_accuracy: 0.8200\n",
      "Epoch 18/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.3879 - acc: 0.8287 - categorical_accuracy: 0.8287 - val_loss: 0.3766 - val_acc: 0.8380 - val_categorical_accuracy: 0.8380\n",
      "Epoch 19/100\n",
      "4500/4500 [==============================] - 16s 4ms/step - loss: 0.3923 - acc: 0.8193 - categorical_accuracy: 0.8193 - val_loss: 0.4405 - val_acc: 0.7893 - val_categorical_accuracy: 0.7893\n",
      "Epoch 20/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.3862 - acc: 0.8293 - categorical_accuracy: 0.8293 - val_loss: 0.5137 - val_acc: 0.7453 - val_categorical_accuracy: 0.7453\n",
      "Epoch 21/100\n",
      "4500/4500 [==============================] - 16s 4ms/step - loss: 0.3832 - acc: 0.8264 - categorical_accuracy: 0.8264 - val_loss: 0.4827 - val_acc: 0.7713 - val_categorical_accuracy: 0.7713\n",
      "Epoch 22/100\n",
      "4500/4500 [==============================] - 18s 4ms/step - loss: 0.3787 - acc: 0.8327 - categorical_accuracy: 0.8327 - val_loss: 0.4314 - val_acc: 0.8000 - val_categorical_accuracy: 0.8000\n",
      "Epoch 23/100\n",
      "4500/4500 [==============================] - 19s 4ms/step - loss: 0.3697 - acc: 0.8422 - categorical_accuracy: 0.8422 - val_loss: 0.3854 - val_acc: 0.8327 - val_categorical_accuracy: 0.8327\n",
      "Epoch 24/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.3699 - acc: 0.8364 - categorical_accuracy: 0.8364 - val_loss: 0.3863 - val_acc: 0.8333 - val_categorical_accuracy: 0.8333\n",
      "Epoch 25/100\n",
      "4500/4500 [==============================] - 18s 4ms/step - loss: 0.3571 - acc: 0.8496 - categorical_accuracy: 0.8496 - val_loss: 0.3919 - val_acc: 0.8260 - val_categorical_accuracy: 0.8260\n",
      "Epoch 26/100\n",
      "4500/4500 [==============================] - 23s 5ms/step - loss: 0.3599 - acc: 0.8429 - categorical_accuracy: 0.8429 - val_loss: 0.3821 - val_acc: 0.8393 - val_categorical_accuracy: 0.8393\n",
      "Epoch 27/100\n",
      "4500/4500 [==============================] - 19s 4ms/step - loss: 0.3559 - acc: 0.8447 - categorical_accuracy: 0.8447 - val_loss: 0.3862 - val_acc: 0.8287 - val_categorical_accuracy: 0.8287\n",
      "Epoch 28/100\n",
      "4500/4500 [==============================] - 19s 4ms/step - loss: 0.3475 - acc: 0.8496 - categorical_accuracy: 0.8496 - val_loss: 0.5532 - val_acc: 0.7300 - val_categorical_accuracy: 0.7300\n",
      "Epoch 29/100\n",
      "4500/4500 [==============================] - 16s 3ms/step - loss: 0.3478 - acc: 0.8509 - categorical_accuracy: 0.8509 - val_loss: 0.5670 - val_acc: 0.7073 - val_categorical_accuracy: 0.7073\n",
      "Epoch 30/100\n",
      "4500/4500 [==============================] - 16s 4ms/step - loss: 0.3498 - acc: 0.8491 - categorical_accuracy: 0.8491 - val_loss: 0.4326 - val_acc: 0.8167 - val_categorical_accuracy: 0.8167\n",
      "Epoch 31/100\n",
      "4500/4500 [==============================] - 16s 4ms/step - loss: 0.3346 - acc: 0.8640 - categorical_accuracy: 0.8640 - val_loss: 0.3969 - val_acc: 0.8347 - val_categorical_accuracy: 0.8347\n",
      "Epoch 32/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.3335 - acc: 0.8556 - categorical_accuracy: 0.8556 - val_loss: 0.4079 - val_acc: 0.8287 - val_categorical_accuracy: 0.8287\n",
      "Epoch 33/100\n",
      "4500/4500 [==============================] - 16s 4ms/step - loss: 0.3297 - acc: 0.8691 - categorical_accuracy: 0.8691 - val_loss: 0.4161 - val_acc: 0.8140 - val_categorical_accuracy: 0.8140\n",
      "Epoch 34/100\n",
      "4500/4500 [==============================] - 15s 3ms/step - loss: 0.3189 - acc: 0.8720 - categorical_accuracy: 0.8720 - val_loss: 0.4573 - val_acc: 0.7800 - val_categorical_accuracy: 0.7800\n",
      "Epoch 35/100\n",
      "4500/4500 [==============================] - 16s 3ms/step - loss: 0.3255 - acc: 0.8611 - categorical_accuracy: 0.8611 - val_loss: 0.4024 - val_acc: 0.8320 - val_categorical_accuracy: 0.8320\n",
      "Epoch 36/100\n",
      "4500/4500 [==============================] - 16s 4ms/step - loss: 0.3246 - acc: 0.8656 - categorical_accuracy: 0.8656 - val_loss: 0.4025 - val_acc: 0.8233 - val_categorical_accuracy: 0.8233\n",
      "Epoch 37/100\n",
      "4500/4500 [==============================] - 16s 4ms/step - loss: 0.3238 - acc: 0.8658 - categorical_accuracy: 0.8658 - val_loss: 0.4020 - val_acc: 0.8260 - val_categorical_accuracy: 0.8260\n",
      "Epoch 38/100\n",
      "4500/4500 [==============================] - 16s 3ms/step - loss: 0.3132 - acc: 0.8722 - categorical_accuracy: 0.8722 - val_loss: 0.4198 - val_acc: 0.8140 - val_categorical_accuracy: 0.8140\n",
      "Epoch 39/100\n",
      "4500/4500 [==============================] - 16s 4ms/step - loss: 0.2972 - acc: 0.8798 - categorical_accuracy: 0.8798 - val_loss: 0.4162 - val_acc: 0.8287 - val_categorical_accuracy: 0.8287\n",
      "Epoch 40/100\n",
      "4500/4500 [==============================] - 16s 4ms/step - loss: 0.3181 - acc: 0.8704 - categorical_accuracy: 0.8704 - val_loss: 0.4384 - val_acc: 0.8173 - val_categorical_accuracy: 0.8173\n",
      "Epoch 41/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.3108 - acc: 0.8720 - categorical_accuracy: 0.8720 - val_loss: 0.4125 - val_acc: 0.8253 - val_categorical_accuracy: 0.8253\n",
      "Epoch 42/100\n",
      "4500/4500 [==============================] - 16s 4ms/step - loss: 0.3061 - acc: 0.8738 - categorical_accuracy: 0.8738 - val_loss: 0.4165 - val_acc: 0.8180 - val_categorical_accuracy: 0.8180\n",
      "Epoch 43/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.3009 - acc: 0.8720 - categorical_accuracy: 0.8720 - val_loss: 0.3977 - val_acc: 0.8327 - val_categorical_accuracy: 0.8327\n",
      "Epoch 44/100\n",
      "4500/4500 [==============================] - 16s 4ms/step - loss: 0.2982 - acc: 0.8798 - categorical_accuracy: 0.8798 - val_loss: 0.3939 - val_acc: 0.8407 - val_categorical_accuracy: 0.8407\n",
      "Epoch 45/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2821 - acc: 0.8873 - categorical_accuracy: 0.8873 - val_loss: 0.4492 - val_acc: 0.8073 - val_categorical_accuracy: 0.8073\n",
      "Epoch 46/100\n",
      "4500/4500 [==============================] - 16s 4ms/step - loss: 0.2796 - acc: 0.8873 - categorical_accuracy: 0.8873 - val_loss: 0.4254 - val_acc: 0.8273 - val_categorical_accuracy: 0.8273\n",
      "Epoch 47/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2739 - acc: 0.8887 - categorical_accuracy: 0.8887 - val_loss: 0.4165 - val_acc: 0.8367 - val_categorical_accuracy: 0.8367\n",
      "Epoch 48/100\n",
      "4500/4500 [==============================] - 18s 4ms/step - loss: 0.2774 - acc: 0.8871 - categorical_accuracy: 0.8871 - val_loss: 0.4217 - val_acc: 0.8180 - val_categorical_accuracy: 0.8180\n",
      "Epoch 49/100\n",
      "4500/4500 [==============================] - 16s 4ms/step - loss: 0.2728 - acc: 0.8867 - categorical_accuracy: 0.8867 - val_loss: 0.3963 - val_acc: 0.8380 - val_categorical_accuracy: 0.8380\n",
      "Epoch 50/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2709 - acc: 0.8927 - categorical_accuracy: 0.8927 - val_loss: 0.4231 - val_acc: 0.8227 - val_categorical_accuracy: 0.8227\n",
      "Epoch 51/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2680 - acc: 0.8920 - categorical_accuracy: 0.8920 - val_loss: 0.4217 - val_acc: 0.8207 - val_categorical_accuracy: 0.8207\n",
      "Epoch 52/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2664 - acc: 0.8964 - categorical_accuracy: 0.8964 - val_loss: 0.3964 - val_acc: 0.8353 - val_categorical_accuracy: 0.8353\n",
      "Epoch 53/100\n",
      "4500/4500 [==============================] - 16s 4ms/step - loss: 0.2728 - acc: 0.8900 - categorical_accuracy: 0.8900 - val_loss: 0.5323 - val_acc: 0.7333 - val_categorical_accuracy: 0.7333\n",
      "Epoch 54/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2763 - acc: 0.8847 - categorical_accuracy: 0.8847 - val_loss: 0.5916 - val_acc: 0.7033 - val_categorical_accuracy: 0.7033\n",
      "Epoch 55/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2616 - acc: 0.9031 - categorical_accuracy: 0.9031 - val_loss: 0.4930 - val_acc: 0.7540 - val_categorical_accuracy: 0.7540\n",
      "Epoch 56/100\n",
      "4500/4500 [==============================] - 16s 4ms/step - loss: 0.2558 - acc: 0.9000 - categorical_accuracy: 0.9000 - val_loss: 0.4360 - val_acc: 0.8020 - val_categorical_accuracy: 0.8020\n",
      "Epoch 57/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2601 - acc: 0.8976 - categorical_accuracy: 0.8976 - val_loss: 0.5402 - val_acc: 0.7280 - val_categorical_accuracy: 0.7280\n",
      "Epoch 58/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2420 - acc: 0.9087 - categorical_accuracy: 0.9087 - val_loss: 0.5002 - val_acc: 0.7640 - val_categorical_accuracy: 0.7640\n",
      "Epoch 59/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2471 - acc: 0.9020 - categorical_accuracy: 0.9020 - val_loss: 0.4139 - val_acc: 0.8227 - val_categorical_accuracy: 0.8227\n",
      "Epoch 60/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2498 - acc: 0.9053 - categorical_accuracy: 0.9053 - val_loss: 0.4666 - val_acc: 0.7767 - val_categorical_accuracy: 0.7767\n",
      "Epoch 61/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2540 - acc: 0.8980 - categorical_accuracy: 0.8980 - val_loss: 0.9028 - val_acc: 0.6100 - val_categorical_accuracy: 0.6100\n",
      "Epoch 62/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2668 - acc: 0.8953 - categorical_accuracy: 0.8953 - val_loss: 0.6914 - val_acc: 0.6627 - val_categorical_accuracy: 0.6627\n",
      "Epoch 63/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2420 - acc: 0.9011 - categorical_accuracy: 0.9011 - val_loss: 0.5032 - val_acc: 0.7547 - val_categorical_accuracy: 0.7547\n",
      "Epoch 64/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2341 - acc: 0.9067 - categorical_accuracy: 0.9067 - val_loss: 0.4136 - val_acc: 0.8147 - val_categorical_accuracy: 0.8147\n",
      "Epoch 65/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2293 - acc: 0.9087 - categorical_accuracy: 0.9087 - val_loss: 0.4072 - val_acc: 0.8180 - val_categorical_accuracy: 0.8180\n",
      "Epoch 66/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2332 - acc: 0.9073 - categorical_accuracy: 0.9073 - val_loss: 0.4349 - val_acc: 0.8020 - val_categorical_accuracy: 0.8020\n",
      "Epoch 67/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2316 - acc: 0.9133 - categorical_accuracy: 0.9133 - val_loss: 0.5461 - val_acc: 0.7420 - val_categorical_accuracy: 0.7420\n",
      "Epoch 68/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2212 - acc: 0.9144 - categorical_accuracy: 0.9144 - val_loss: 0.5521 - val_acc: 0.7133 - val_categorical_accuracy: 0.7133\n",
      "Epoch 69/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2268 - acc: 0.9116 - categorical_accuracy: 0.9116 - val_loss: 0.5921 - val_acc: 0.6987 - val_categorical_accuracy: 0.6987\n",
      "Epoch 70/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2236 - acc: 0.9120 - categorical_accuracy: 0.9120 - val_loss: 0.4187 - val_acc: 0.8180 - val_categorical_accuracy: 0.8180\n",
      "Epoch 71/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2162 - acc: 0.9187 - categorical_accuracy: 0.9187 - val_loss: 0.4201 - val_acc: 0.8213 - val_categorical_accuracy: 0.8213\n",
      "Epoch 72/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2239 - acc: 0.9122 - categorical_accuracy: 0.9122 - val_loss: 0.4231 - val_acc: 0.8113 - val_categorical_accuracy: 0.8113\n",
      "Epoch 73/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2087 - acc: 0.9193 - categorical_accuracy: 0.9193 - val_loss: 0.4216 - val_acc: 0.8167 - val_categorical_accuracy: 0.8167\n",
      "Epoch 74/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2230 - acc: 0.9124 - categorical_accuracy: 0.9124 - val_loss: 0.3945 - val_acc: 0.8333 - val_categorical_accuracy: 0.8333\n",
      "Epoch 75/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2201 - acc: 0.9191 - categorical_accuracy: 0.9191 - val_loss: 0.4391 - val_acc: 0.7980 - val_categorical_accuracy: 0.7980\n",
      "Epoch 76/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2172 - acc: 0.9169 - categorical_accuracy: 0.9169 - val_loss: 0.3974 - val_acc: 0.8320 - val_categorical_accuracy: 0.8320\n",
      "Epoch 77/100\n",
      "4500/4500 [==============================] - 18s 4ms/step - loss: 0.2158 - acc: 0.9167 - categorical_accuracy: 0.9167 - val_loss: 0.4785 - val_acc: 0.7827 - val_categorical_accuracy: 0.7827\n",
      "Epoch 78/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2075 - acc: 0.9160 - categorical_accuracy: 0.9160 - val_loss: 0.5189 - val_acc: 0.7587 - val_categorical_accuracy: 0.7587\n",
      "Epoch 79/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2052 - acc: 0.9209 - categorical_accuracy: 0.9209 - val_loss: 0.6072 - val_acc: 0.7180 - val_categorical_accuracy: 0.7180\n",
      "Epoch 80/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2035 - acc: 0.9200 - categorical_accuracy: 0.9200 - val_loss: 0.6517 - val_acc: 0.7073 - val_categorical_accuracy: 0.7073\n",
      "Epoch 81/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2039 - acc: 0.9198 - categorical_accuracy: 0.9198 - val_loss: 0.7171 - val_acc: 0.6820 - val_categorical_accuracy: 0.6820\n",
      "Epoch 82/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2008 - acc: 0.9209 - categorical_accuracy: 0.9209 - val_loss: 0.5059 - val_acc: 0.7687 - val_categorical_accuracy: 0.7687\n",
      "Epoch 83/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2094 - acc: 0.9158 - categorical_accuracy: 0.9158 - val_loss: 0.4330 - val_acc: 0.8027 - val_categorical_accuracy: 0.8027\n",
      "Epoch 84/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2003 - acc: 0.9220 - categorical_accuracy: 0.9220 - val_loss: 0.4921 - val_acc: 0.7793 - val_categorical_accuracy: 0.7793\n",
      "Epoch 85/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2094 - acc: 0.9193 - categorical_accuracy: 0.9193 - val_loss: 0.4160 - val_acc: 0.8127 - val_categorical_accuracy: 0.8127\n",
      "Epoch 86/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2070 - acc: 0.9196 - categorical_accuracy: 0.9196 - val_loss: 0.4177 - val_acc: 0.8153 - val_categorical_accuracy: 0.8153\n",
      "Epoch 87/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.1961 - acc: 0.9218 - categorical_accuracy: 0.9218 - val_loss: 0.4292 - val_acc: 0.8033 - val_categorical_accuracy: 0.8033\n",
      "Epoch 88/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.1952 - acc: 0.9233 - categorical_accuracy: 0.9233 - val_loss: 0.4820 - val_acc: 0.7827 - val_categorical_accuracy: 0.7827\n",
      "Epoch 89/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.1992 - acc: 0.9209 - categorical_accuracy: 0.9209 - val_loss: 0.4532 - val_acc: 0.8053 - val_categorical_accuracy: 0.8053\n",
      "Epoch 90/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.2076 - acc: 0.9187 - categorical_accuracy: 0.9187 - val_loss: 0.4996 - val_acc: 0.7907 - val_categorical_accuracy: 0.7907\n",
      "Epoch 91/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.1874 - acc: 0.9229 - categorical_accuracy: 0.9229 - val_loss: 0.4856 - val_acc: 0.7947 - val_categorical_accuracy: 0.7947\n",
      "Epoch 92/100\n",
      "4500/4500 [==============================] - 18s 4ms/step - loss: 0.1897 - acc: 0.9260 - categorical_accuracy: 0.9260 - val_loss: 0.4379 - val_acc: 0.8020 - val_categorical_accuracy: 0.8020\n",
      "Epoch 93/100\n",
      "4500/4500 [==============================] - 19s 4ms/step - loss: 0.1930 - acc: 0.9287 - categorical_accuracy: 0.9287 - val_loss: 0.4440 - val_acc: 0.7987 - val_categorical_accuracy: 0.7987\n",
      "Epoch 94/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.1849 - acc: 0.9280 - categorical_accuracy: 0.9280 - val_loss: 0.4288 - val_acc: 0.8060 - val_categorical_accuracy: 0.8060\n",
      "Epoch 95/100\n",
      "4500/4500 [==============================] - 18s 4ms/step - loss: 0.1906 - acc: 0.9284 - categorical_accuracy: 0.9284 - val_loss: 0.5446 - val_acc: 0.7833 - val_categorical_accuracy: 0.7833\n",
      "Epoch 96/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.1721 - acc: 0.9333 - categorical_accuracy: 0.9333 - val_loss: 0.4629 - val_acc: 0.7947 - val_categorical_accuracy: 0.7947\n",
      "Epoch 97/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.1750 - acc: 0.9351 - categorical_accuracy: 0.9351 - val_loss: 0.5050 - val_acc: 0.7820 - val_categorical_accuracy: 0.7820\n",
      "Epoch 98/100\n",
      "4500/4500 [==============================] - 17s 4ms/step - loss: 0.1752 - acc: 0.9327 - categorical_accuracy: 0.9327 - val_loss: 0.4861 - val_acc: 0.7793 - val_categorical_accuracy: 0.7793\n",
      "Epoch 99/100\n",
      "4500/4500 [==============================] - 18s 4ms/step - loss: 0.1768 - acc: 0.9302 - categorical_accuracy: 0.9302 - val_loss: 0.4330 - val_acc: 0.8147 - val_categorical_accuracy: 0.8147\n",
      "Epoch 100/100\n",
      "4500/4500 [==============================] - 18s 4ms/step - loss: 0.1800 - acc: 0.9287 - categorical_accuracy: 0.9287 - val_loss: 0.4700 - val_acc: 0.8033 - val_categorical_accuracy: 0.8033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ab0230400>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if args['model'] == 'dan':\n",
    "    for i in range(num_hidden_layers):\n",
    "        model.add(Dense(num_hidden_units))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation(activation))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "model.add(Dense(labels.shape[1]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "adam = Adam()\n",
    "model.compile(loss='categorical_crossentropy',optimizer=adam,metrics=['accuracy','categorical_accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(reviews,labels,batch_size=batch_size,epochs=num_epochs,\\\n",
    "          validation_data=(reviews_val,labels_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
