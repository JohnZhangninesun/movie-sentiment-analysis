{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Sentiment Analysis with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "colab_type": "code",
    "id": "CML_IG6z-iwM",
    "outputId": "d9301f36-c1cf-4b3f-e639-a731880ed036"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/swami/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# uncomment these for Google collab, will have already been installed in local environment \n",
    "# if 'pip install -r requirements.txt' has been run\n",
    "#!pip install nltk\n",
    "#!pip install --upgrade gensim\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import nltk\n",
    "\n",
    "\n",
    "import glob\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FJiWamI00hBp",
    "outputId": "e3c105d5-037f-4521-b8e1-a83cb7a5edeb"
   },
   "outputs": [],
   "source": [
    "# MacOSX: See https://www.mkyong.com/mac/wget-on-mac-os-x/ for wget\n",
    "if not os.path.isdir('../aclImdb'):\n",
    "    if not os.path.isfile('../aclImdb_v1.tar.gz'):\n",
    "      !wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz \n",
    "\n",
    "    if not os.path.isdir('../aclImdb'):  \n",
    "      !tar -xf aclImdb_v1.tar.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U5Tnmoh-Dpfk"
   },
   "outputs": [],
   "source": [
    "time_beginning_of_notebook = time.time()\n",
    "SAMPLE_SIZE=3000\n",
    "positive_sample_file_list = glob.glob(os.path.join('../aclImdb/train/pos', \"*.txt\"))\n",
    "positive_sample_file_list = positive_sample_file_list[:SAMPLE_SIZE]\n",
    "\n",
    "negative_sample_file_list = glob.glob(os.path.join('../aclImdb/train/neg', \"*.txt\"))\n",
    "negative_sample_file_list = negative_sample_file_list[:SAMPLE_SIZE]\n",
    "\n",
    "import re\n",
    "\n",
    "# load doc into memory\n",
    "# regex to clean markup elements \n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', encoding='utf8')\n",
    "    # read all text\n",
    "    text = re.sub('<[^>]*>', ' ', file.read())\n",
    "    #text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "lfr3bXOgXNJJ",
    "outputId": "cc06dd0d-e886-4090-c972-cd05520adaf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive review(s): Bizarre horror movie filled with famous faces but stolen by Cristina Raines (later of TV's \"Flamingo\n",
      "Negative review(s): Well...tremors I, the original started off in 1990 and i found the movie quite enjoyable to watch. h\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df_positives = pd.DataFrame({'reviews':[load_doc(x) for x in positive_sample_file_list], 'sentiment': np.ones(SAMPLE_SIZE)})\n",
    "df_negatives = pd.DataFrame({'reviews':[load_doc(x) for x in negative_sample_file_list], 'sentiment': np.zeros(SAMPLE_SIZE)})\n",
    "\n",
    "print(\"Positive review(s):\", df_positives['reviews'][1][:100])\n",
    "print(\"Negative review(s):\", df_negatives['reviews'][1][:100])\n",
    "\n",
    "df = pd.concat([df_positives, df_negatives], ignore_index=True)\n",
    "\n",
    "df = shuffle(df)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['reviews'], df['sentiment'], test_size=0.25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logic to compute DAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from dan.custom_layers import AverageWords, WordDropout\n",
    "from dan.preprocess import PreProcessor\n",
    "\n",
    "from keras.layers import Embedding, Dense, Input, BatchNormalization, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adagrad, Adam\n",
    "from keras import backend as K\n",
    "\n",
    "from pdb import set_trace\n",
    "\n",
    "embedding_dim = 300\n",
    "num_hidden_layers = 3\n",
    "num_hidden_units = 300\n",
    "num_epochs = 100\n",
    "batch_size = 100\n",
    "dropout_rate = 0.2\n",
    "word_dropout_rate = 0.3\n",
    "activation = 'relu'\n",
    "\n",
    "args = {}\n",
    "args['We']='data/glove.6B.300d.txt'\n",
    "args['Wels']='' ### rand or ''\n",
    "args['model']='dan'  ### nbow OR dan\n",
    "args['wd']='y'\n",
    "\n",
    "reviews=X_train\n",
    "reviews_val=X_test\n",
    "labels=y_train\n",
    "labels_val=y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "class PreProcessor:\n",
    "    def __init__(self,TRAIN_DATA,VAL_DATA,WE_FILE):\n",
    "        self.reviews = TRAIN_DATA\n",
    "        self.reviews_val = VAL_DATA\n",
    "        self.we_file = WE_FILE\n",
    "\n",
    "    def tokenize(self):\n",
    "        print(self.reviews[0])\n",
    "\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(self.reviews)\n",
    "\n",
    "        self.sequences = tokenizer.texts_to_sequences(self.reviews)\n",
    "        self.sequences_val = tokenizer.texts_to_sequences(self.reviews_val)\n",
    "\n",
    "        self.word_index = tokenizer.word_index\n",
    "        print(\"Found %s unique tokens\" %(len(self.word_index)))\n",
    "\n",
    "    def make_data(self):\n",
    "        self.MAX_SEQUENCE_LENGTH = max([len(self.sequences[i]) for i in range(len(self.sequences))])\n",
    "        print(\"self.MAX_SEQUENCE_LENGTH: {}\".format(self.MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "        data = pad_sequences(self.sequences,maxlen=self.MAX_SEQUENCE_LENGTH)\n",
    "        data_val = pad_sequences(self.sequences_val,maxlen=self.MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "        answers_train = set(self.reviews[:,1])\n",
    "        answers_val = set(self.reviews_val[:,1])\n",
    "        answers = answers_train.union(answers_val)\n",
    "\n",
    "        labels_index = {} # labels_index[\"Henry IV of France\"]\n",
    "        answers_index = {} # answers_index[0]\n",
    "\n",
    "        for i,j in enumerate(answers):\n",
    "            labels_index[j] = i\n",
    "            answers_index[i] = j\n",
    "\n",
    "        labels = np.zeros((len(self.sequences),1))\n",
    "        labels_val = np.zeros((len(self.sequences_val),1))\n",
    "\n",
    "        for i in range(len(self.sequences)):\n",
    "            labels[i] = labels_index[self.reviews_data[i,1]]\n",
    "\n",
    "        for i in range(len(self.sequences_val)):\n",
    "            labels_val[i] = labels_index[self.val_data[i,1]]\n",
    "\n",
    "        labels = to_categorical(labels,num_classes=len(answers))\n",
    "        labels_val = to_categorical(labels_val,num_classes=len(answers))\n",
    "\n",
    "        print(\"Shape of data tensor: \" +str(data.shape))\n",
    "        print(\"Shape of label tensor: \" +str(labels.shape))\n",
    "\n",
    "        return data, labels, data_val, labels_val\n",
    "        \n",
    "    def get_word_embedding_matrix(self,EMBEDDING_DIM=100):\n",
    "        embeddings_index = {}\n",
    "\n",
    "        if self.we_file == \"rand\":\n",
    "            return None\n",
    "\n",
    "        f = open(self.we_file)\n",
    "\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        f.close()\n",
    "\n",
    "        print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "        self.embedding_matrix = np.zeros((len(self.word_index)+1, EMBEDDING_DIM))\n",
    "\n",
    "        for word, i in self.word_index.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                self.embedding_matrix[i] = embedding_vector\n",
    "\n",
    "        return self.embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "if args['Wels'] == \"rand\":\n",
    "    model.add(Embedding(len(pp.word_index) + 1,embedding_dim,input_length=pp.MAX_SEQUENCE_LENGTH,trainable=False))\n",
    "else:\n",
    "    model.add(Embedding(len(pp.word_index)+1,embedding_dim,weights=[embedding_matrix],input_length=pp.MAX_SEQUENCE_LENGTH,trainable=False))\n",
    "\n",
    "if args['wd'] == 'y':\n",
    "    model.add(WordDropout(word_dropout_rate))\n",
    "model.add(AverageWords())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2473, 300)         12356100  \n",
      "_________________________________________________________________\n",
      "word_dropout_1 (WordDropout) (None, 2473, 300)         0         \n",
      "_________________________________________________________________\n",
      "average_words_1 (AverageWord (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4500)              1354500   \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 4500)              18000     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 4500)              0         \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 4500)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 300)               1350300   \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 4500)              1354500   \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 4500)              18000     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4500)              0         \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 4500)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 300)               1350300   \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 4500)              1354500   \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 4500)              18000     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 4500)              0         \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 4500)              0         \n",
      "=================================================================\n",
      "Total params: 20,091,600\n",
      "Trainable params: 7,701,300\n",
      "Non-trainable params: 12,390,300\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected embedding_1_input to have shape (2473,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-52663c93a9df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m          \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    951\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    135\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected embedding_1_input to have shape (2473,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "if args['model'] == 'dan':\n",
    "    for i in range(num_hidden_layers):\n",
    "        model.add(Dense(num_hidden_units))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation(activation))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "model.add(Dense(labels.shape[0]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "adam = Adam()\n",
    "model.compile(loss='categorical_crossentropy',optimizer=adam,metrics=['categorical_accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(reviews,labels,batch_size=batch_size,epochs=num_epochs,\\\n",
    "          validation_data=(reviews_val,labels_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
