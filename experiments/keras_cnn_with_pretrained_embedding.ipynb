{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_cnn_with_pretrained_embedding.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "STVavNFnNCEr"
      },
      "cell_type": "markdown",
      "source": [
        "# Movie Sentiment Analysis with Keras"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "CML_IG6z-iwM",
        "outputId": "25996133-981f-4b3d-a6c0-e78e14383a9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        }
      },
      "cell_type": "code",
      "source": [
        "# uncomment these for Google collab, will have already been installed in local environment \n",
        "# if 'pip install -r requirements.txt' has been run\n",
        "!pip install nltk\n",
        "!pip install --upgrade gensim\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import os.path\n",
        "\n",
        "from pdb import set_trace\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import nltk\n",
        "\n",
        "\n",
        "import glob\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.11.0)\n",
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/b9/6c93685bed0026b6a1cce55ab173f6b617f6db0d1325d25489c2fd43e711/gensim-3.7.1-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 24.2MB 1.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.8.0)\n",
            "Requirement already satisfied, skipping upgrade: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (2.49.0)\n",
            "Requirement already satisfied, skipping upgrade: bz2file in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (0.98)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (2.18.4)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (1.9.86)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (2018.11.29)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (1.22)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (2.6)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.13.0,>=1.12.86 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (1.12.86)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (0.9.3)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.2.0,>=0.1.10 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (0.1.13)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.86->boto3->smart-open>=1.7.0->gensim) (2.5.3)\n",
            "Requirement already satisfied, skipping upgrade: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.86->boto3->smart-open>=1.7.0->gensim) (0.14)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-3.7.1\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FJiWamI00hBp",
        "outputId": "0d6f166b-106f-48e2-de2e-4e832c3e8848",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "cell_type": "code",
      "source": [
        "# MacOSX: See https://www.mkyong.com/mac/wget-on-mac-os-x/ for wget\n",
        "if not os.path.isdir('./aclImdb'):\n",
        "    if not os.path.isfile('./aclImdb_v1.tar.gz'):\n",
        "      !wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz \n",
        "\n",
        "    if not os.path.isdir('./aclImdb'):  \n",
        "      !tar -xf aclImdb_v1.tar.gz "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-02-06 18:04:44--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  19.7MB/s    in 6.1s    \n",
            "\n",
            "2019-02-06 18:04:50 (13.2 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "U5Tnmoh-Dpfk",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "time_beginning_of_notebook = time.time()\n",
        "# SAMPLE_SIZE=12500\n",
        "SAMPLE_SIZE=4000\n",
        "\n",
        "positive_sample_file_list = glob.glob(os.path.join('./aclImdb/train/pos', \"*.txt\"))\n",
        "positive_sample_file_list = positive_sample_file_list[:SAMPLE_SIZE]\n",
        "\n",
        "negative_sample_file_list = glob.glob(os.path.join('./aclImdb/train/neg', \"*.txt\"))\n",
        "negative_sample_file_list = negative_sample_file_list[:SAMPLE_SIZE]\n",
        "\n",
        "import re\n",
        "\n",
        "# load doc into memory\n",
        "# regex to clean markup elements \n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r', encoding='utf8')\n",
        "    # read all text\n",
        "    text = re.sub('<[^>]*>', ' ', file.read())\n",
        "    #text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "lfr3bXOgXNJJ",
        "outputId": "39188666-639d-4b86-c317-a39401616851",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "df_positives = pd.DataFrame({'reviews':[load_doc(x) for x in positive_sample_file_list], 'sentiment': np.ones(SAMPLE_SIZE)})\n",
        "df_negatives = pd.DataFrame({'reviews':[load_doc(x) for x in negative_sample_file_list], 'sentiment': np.zeros(SAMPLE_SIZE)})\n",
        "\n",
        "print(\"Positive review(s):\", df_positives['reviews'][1][:100])\n",
        "print(\"Negative review(s):\", df_negatives['reviews'][1][:100])\n",
        "\n",
        "df = pd.concat([df_positives, df_negatives], ignore_index=True)\n",
        "\n",
        "df = shuffle(df)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['reviews'], df['sentiment'], test_size=0.25)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive review(s): Japanese indie film with humor and philosophy where the three main characters run literally almost t\n",
            "Negative review(s): The film is severely awful and is demeaning to rape victims. On the surface, it may be a daring film\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "PA4PuIKtNCGh",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#ML STUDY GROUP\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "class PreProcessor:\n",
        "    def __init__(self,REVIEWS,REVIEWS_VAL,LABELS,LABELS_VAL,WE_FILE):\n",
        "        self.reviews = REVIEWS\n",
        "        self.reviews_val = REVIEWS_VAL\n",
        "        self.labels = LABELS\n",
        "        self.labels_val = LABELS_VAL\n",
        "        self.we_file = WE_FILE\n",
        "\n",
        "    def tokenize(self):\n",
        "#         set_trace()\n",
        "        print(self.reviews[0])\n",
        "\n",
        "        tokenizer = Tokenizer()\n",
        "        tokenizer.fit_on_texts(self.reviews)\n",
        "\n",
        "        self.sequences = tokenizer.texts_to_sequences(self.reviews)\n",
        "        self.sequences_val = tokenizer.texts_to_sequences(self.reviews_val)\n",
        "\n",
        "        self.word_index = tokenizer.word_index\n",
        "        print(\"Found %s unique tokens\" %(len(self.word_index)))\n",
        "\n",
        "    def make_data(self):\n",
        "        self.MAX_SEQUENCE_LENGTH = max([len(self.sequences[i]) for i in range(len(self.sequences))])\n",
        "        print(\"self.MAX_SEQUENCE_LENGTH: {}\".format(self.MAX_SEQUENCE_LENGTH))\n",
        "\n",
        "        review = pad_sequences(self.sequences,maxlen=self.MAX_SEQUENCE_LENGTH)\n",
        "        review_val = pad_sequences(self.sequences_val,maxlen=self.MAX_SEQUENCE_LENGTH)\n",
        "        \n",
        "        labels = to_categorical(self.labels)\n",
        "        labels_val = to_categorical(self.labels_val)\n",
        "\n",
        "        print(\"Shape of data tensor: \" +str(review.shape))\n",
        "        print(\"Shape of label tensor: \" +str(labels.shape))\n",
        "\n",
        "        return review, review_val, labels, labels_val\n",
        "        \n",
        "    def get_word_embedding_matrix(self,EMBEDDING_DIM=100):\n",
        "        embeddings_index = {}\n",
        "\n",
        "        if self.we_file == \"rand\":\n",
        "            return None\n",
        "\n",
        "        f = open(self.we_file)\n",
        "\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "        f.close()\n",
        "\n",
        "        print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "        self.embedding_matrix = np.zeros((len(self.word_index)+1, EMBEDDING_DIM))\n",
        "\n",
        "        for word, i in self.word_index.items():\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                # words not found in embedding index will be all-zeros.\n",
        "                self.embedding_matrix[i] = embedding_vector\n",
        "\n",
        "        return self.embedding_matrix\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "VEIPYnFIPMxB",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# MacOSX: See https://www.mkyong.com/mac/wget-on-mac-os-x/ for wget\n",
        "if not os.path.isfile('./glove.6B.300d.txt'):\n",
        "    if not os.path.isfile('./glove.6B.zip'):\n",
        "      !wget http://nlp.stanford.edu/data/glove.6B.zip \n",
        "\n",
        "    if not os.path.isfile('./glove.6B.300d.txt'):  \n",
        "      !unzip glove.6B.zip \n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Sl6W6_92NCGu",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "from keras.layers import Embedding, Dense, Input, BatchNormalization, Activation, Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adagrad, Adam\n",
        "from keras import backend as K\n",
        "\n",
        "from pdb import set_trace\n",
        "\n",
        "embedding_dim = 300\n",
        "num_hidden_layers = 3\n",
        "num_hidden_units = 300\n",
        "num_epochs = 100\n",
        "batch_size = 512\n",
        "dropout_rate = 0.2\n",
        "word_dropout_rate = 0.3\n",
        "activation = 'relu'\n",
        "\n",
        "args = {}\n",
        "args['We']='./glove.6B.300d.txt'\n",
        "args['Wels']='' ### rand or ''\n",
        "args['model']='dan'  ### nbow OR dan\n",
        "args['wd']='y'\n",
        "\n",
        "# reviews=X_train.values\n",
        "# reviews_val=X_test.values\n",
        "# labels=y_train.values\n",
        "# labels_val=y_test.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "OO66Wuy-NCHP",
        "outputId": "22b4b015-80ec-49bb-8c3d-4ba721898347",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "cell_type": "code",
      "source": [
        "pp = PreProcessor(X_train,X_test,y_train,y_test,args['We'])\n",
        "pp.tokenize()\n",
        "\n",
        "encoded_X_train,encoded_X_test,y_train,y_test = pp.make_data()\n",
        "\n",
        "embedding_matrix = pp.get_word_embedding_matrix(embedding_dim)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First, nobody can understand why this movie is rated so poorly. Not only is this the first real horrific movie since a very long time for me who am pretty hard-boiled with a decades long experience of horror starting with driving through dark rides (ghost trains) as a child. Second, the main actress Cheri Christian has a face that lets you hope she will be the leading actress in major pictures of the future. Third, this woman is that tremendously beautiful that I suggest the directors retire all those Cameron Diazes, Eva Mendezes, and how ever the names of these ephemeral bulb-lights are. Mrs. Christian is not a light, but a sun.  However, \"Dark remains\" is also of considerable metaphysical importance. They idea that photographs shows creatures of the intermediary reign between reality and \"imagination\" that are not visible with one' own eyes is not new. But I have never seen in a movie before that those creatures are visible on the photographs only for certain people and only to certain times. This means that the photo is not just an iconic picture of reality (by which reality turns into a sign), but becomes an alternative form of reality which can change as the \"real\" reality can. Being a sign, the changing of the picture means that it influences the photographed objects, i.e. the sign behaves like an object. Now, in our usual world of perception, it is common that objects change signs. F.ex., if someone grows a bird, his photograph will show him with beard, not without, as it did before. But the opposite, the changing of objects by signs would imply that the photo with beard is first and only then the beard grows on the man. This is, very simply expressed, the case that happen with the photos taken by the main character in the prison, in this movie. This is new, and we must be thankful for everything new in horror movies which usually just repeat and reorder effects and features that are already well-known, mostly since the silent time.\n",
            "Found 46039 unique tokens\n",
            "self.MAX_SEQUENCE_LENGTH: 2473\n",
            "Shape of data tensor: (6000, 2473)\n",
            "Shape of label tensor: (6000, 2)\n",
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ab3wHsrPNCHp",
        "outputId": "b141ce7f-038e-4f45-c896-3fb06b7190d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "embedding_matrix.shape\n",
        "# pp.MAX_SEQUENCE_LENGTH\n",
        "len(pp.word_index)+1\n",
        "embedding_dim"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "OSGu8ksfNCIB"
      },
      "cell_type": "markdown",
      "source": [
        "https://machinelearningmastery.com/develop-n-gram-multichannel-convolutional-neural-network-sentiment-analysis/"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "XOJqpJ0nNCIj",
        "outputId": "63a65bff-8ced-4e17-9613-74927daa30b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 840
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Concatenate\n",
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.merge import concatenate\n",
        "# create the model\n",
        "\n",
        "channels = []\n",
        "inputs = []\n",
        "encoded_X_trains= []\n",
        "encoded_X_tests = []\n",
        "for filter_len in [3,4,5]:\n",
        "    inputs1 = Input(shape=(pp.MAX_SEQUENCE_LENGTH,))\n",
        "    inputs.append(inputs1)\n",
        "    embedding1 = Embedding(len(pp.word_index)+1,embedding_dim,weights=[embedding_matrix],\\\n",
        "                           input_length=pp.MAX_SEQUENCE_LENGTH,trainable=True)(inputs1)\n",
        "    conv1 = Conv1D(filters=128, kernel_size=filter_len, padding='same', activation='relu')(embedding1)\n",
        "    drop1 = Dropout(0.5)(conv1)\n",
        "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "    flat1 = Flatten()(pool1)\n",
        "    channels.append(flat1)\n",
        "    encoded_X_trains.append(encoded_X_train)\n",
        "    encoded_X_tests.append(encoded_X_test)\n",
        "    \n",
        "# merge\n",
        "merged = concatenate(channels)\n",
        "# interpretation\n",
        "outputs = Dense(2, activation='softmax')(merged)\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "# compile\n",
        "    \n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy','categorical_accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            (None, 2473)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            (None, 2473)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            (None, 2473)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, 2473, 300)    13812000    input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_5 (Embedding)         (None, 2473, 300)    13812000    input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_6 (Embedding)         (None, 2473, 300)    13812000    input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 2473, 128)    115328      embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, 2473, 128)    153728      embedding_5[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 2473, 128)    192128      embedding_6[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 2473, 128)    0           conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 2473, 128)    0           conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 2473, 128)    0           conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1D)  (None, 1236, 128)    0           dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1D)  (None, 1236, 128)    0           dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_6 (MaxPooling1D)  (None, 1236, 128)    0           dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_4 (Flatten)             (None, 158208)       0           max_pooling1d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_5 (Flatten)             (None, 158208)       0           max_pooling1d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_6 (Flatten)             (None, 158208)       0           max_pooling1d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 474624)       0           flatten_4[0][0]                  \n",
            "                                                                 flatten_5[0][0]                  \n",
            "                                                                 flatten_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 2)            949250      concatenate_2[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 42,846,434\n",
            "Trainable params: 42,846,434\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "rxiSQ40iNCI7",
        "outputId": "0f0050be-0c02-4c90-ea0a-12554f395263",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "cell_type": "code",
      "source": [
        "%pdb off\n",
        "batch_size = 64\n",
        "# num_epochs = 3\n",
        "num_epochs = 4\n",
        "\n",
        "history = model.fit(encoded_X_trains,y_train,batch_size=batch_size,epochs=num_epochs,\\\n",
        "          validation_data=(encoded_X_tests,y_test))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Automatic pdb calling has been turned OFF\n",
            "Train on 6000 samples, validate on 2000 samples\n",
            "Epoch 1/4\n",
            "6000/6000 [==============================] - 39s 6ms/step - loss: 0.1307 - acc: 0.9552 - categorical_accuracy: 0.9552 - val_loss: 0.3472 - val_acc: 0.8605 - val_categorical_accuracy: 0.8605\n",
            "Epoch 2/4\n",
            "6000/6000 [==============================] - 38s 6ms/step - loss: 0.0489 - acc: 0.9897 - categorical_accuracy: 0.9897 - val_loss: 0.4320 - val_acc: 0.8465 - val_categorical_accuracy: 0.8465\n",
            "Epoch 3/4\n",
            " 576/6000 [=>............................] - ETA: 31s - loss: 0.0205 - acc: 0.9965 - categorical_accuracy: 0.9965"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "S62Y-vjAQPo-"
      },
      "cell_type": "markdown",
      "source": [
        "y_train"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "U1L_TrbfNCJQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "a536d586-931e-4b94-f41f-093976eb3226"
      },
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(history.history)\n",
        "df=df[df['val_acc']==df.val_acc.max()]\n",
        "df.reset_index(inplace=True)\n",
        "df[\"title\"]=[\"Keras CNN with pretrained embedding\"]\n",
        "df[\"sample_size\"]=[SAMPLE_SIZE]\n",
        "df[\"nb_epochs\"]=[df.iloc[0][\"index\"]+1]\n",
        "df.drop(labels=\"index\",axis=1,inplace=True)\n",
        "print(df)\n",
        "df.to_csv(path_or_buf=df.iloc[0].title+\".csv\")"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'val_loss': [0.5589282789230346, 0.3646219997406006], 'val_acc': [0.7025, 0.8445], 'val_categorical_accuracy': [0.7025, 0.8445], 'loss': [0.775305295308431, 0.31206544399261477], 'acc': [0.5896666668256124, 0.8736666668256123], 'categorical_accuracy': [0.5896666668256124, 0.8736666668256123]}\n",
            "[0, 1]\n",
            "{'batch_size': 64, 'epochs': 2, 'steps': None, 'samples': 6000, 'verbose': 1, 'do_validation': True, 'metrics': ['loss', 'acc', 'categorical_accuracy', 'val_loss', 'val_acc', 'val_categorical_accuracy']}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>acc</th>\n",
              "      <th>categorical_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>val_categorical_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.873667</td>\n",
              "      <td>0.873667</td>\n",
              "      <td>0.312065</td>\n",
              "      <td>0.8445</td>\n",
              "      <td>0.8445</td>\n",
              "      <td>0.364622</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index       acc  categorical_accuracy      loss  val_acc  \\\n",
              "0      1  0.873667              0.873667  0.312065   0.8445   \n",
              "\n",
              "   val_categorical_accuracy  val_loss  \n",
              "0                    0.8445  0.364622  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "metadata": {
        "id": "uuOPm6z9KHnJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xz-m2ZgHKv4y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "a1a14a31-316a-43bf-ba42-5e65a5ddd87f"
      },
      "cell_type": "code",
      "source": [
        "df"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>acc</th>\n",
              "      <th>categorical_accuracy</th>\n",
              "      <th>loss</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>val_categorical_accuracy</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>title</th>\n",
              "      <th>sample_size</th>\n",
              "      <th>nb_epochs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.873667</td>\n",
              "      <td>0.873667</td>\n",
              "      <td>0.312065</td>\n",
              "      <td>0.8445</td>\n",
              "      <td>0.8445</td>\n",
              "      <td>0.364622</td>\n",
              "      <td>Keras CNN with pretrained embedding</td>\n",
              "      <td>4000</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index       acc  categorical_accuracy      loss  val_acc  \\\n",
              "0      1  0.873667              0.873667  0.312065   0.8445   \n",
              "\n",
              "   val_categorical_accuracy  val_loss                                title  \\\n",
              "0                    0.8445  0.364622  Keras CNN with pretrained embedding   \n",
              "\n",
              "   sample_size  nb_epochs  \n",
              "0         4000          2  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "metadata": {
        "id": "-dBRudlAKs3G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HRw6nHSdD0AM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-pG_hc2xQRMx",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}