{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_cnn_with_pretrained_embedding.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "STVavNFnNCEr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Movie Sentiment Analysis with Keras"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "CML_IG6z-iwM",
        "outputId": "682a6fdb-bbb6-4f4d-de8d-39165901f3bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "cell_type": "code",
      "source": [
        "# uncomment these for Google collab, will have already been installed in local environment \n",
        "# if 'pip install -r requirements.txt' has been run\n",
        "!pip install nltk\n",
        "!pip install --upgrade gensim\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import os.path\n",
        "\n",
        "from pdb import set_trace\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import nltk\n",
        "\n",
        "\n",
        "import glob\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.11.0)\n",
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/8e/719275cb952330a14752655141ceda5f6ec4c065eb3f54993943f89faa4e/gensim-3.7.0-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 24.2MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: smart-open>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.8.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (2.49.0)\n",
            "Requirement already satisfied, skipping upgrade: bz2file in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (0.98)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (2.18.4)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (1.9.86)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (2.6)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (2018.11.29)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (1.22)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.13.0,>=1.12.86 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (1.12.86)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (0.9.3)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.2.0,>=0.1.10 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (0.1.13)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.86->boto3->smart-open>=1.7.0->gensim) (2.5.3)\n",
            "Requirement already satisfied, skipping upgrade: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.86->boto3->smart-open>=1.7.0->gensim) (0.14)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-3.7.0\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FJiWamI00hBp",
        "outputId": "48cd7d2e-e671-4adf-d3cf-21314d1368af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "# MacOSX: See https://www.mkyong.com/mac/wget-on-mac-os-x/ for wget\n",
        "if not os.path.isdir('./aclImdb'):\n",
        "    if not os.path.isfile('./aclImdb_v1.tar.gz'):\n",
        "      !wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz \n",
        "\n",
        "    if not os.path.isdir('./aclImdb'):  \n",
        "      !tar -xf aclImdb_v1.tar.gz "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-30 19:40:19--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  41.1MB/s    in 2.0s    \n",
            "\n",
            "2019-01-30 19:40:21 (41.1 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "U5Tnmoh-Dpfk",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "time_beginning_of_notebook = time.time()\n",
        "SAMPLE_SIZE=12500\n",
        "positive_sample_file_list = glob.glob(os.path.join('./aclImdb/train/pos', \"*.txt\"))\n",
        "positive_sample_file_list = positive_sample_file_list[:SAMPLE_SIZE]\n",
        "\n",
        "negative_sample_file_list = glob.glob(os.path.join('./aclImdb/train/neg', \"*.txt\"))\n",
        "negative_sample_file_list = negative_sample_file_list[:SAMPLE_SIZE]\n",
        "\n",
        "import re\n",
        "\n",
        "# load doc into memory\n",
        "# regex to clean markup elements \n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r', encoding='utf8')\n",
        "    # read all text\n",
        "    text = re.sub('<[^>]*>', ' ', file.read())\n",
        "    #text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "lfr3bXOgXNJJ",
        "outputId": "606458f4-ec46-4432-dabf-dd22483e2155",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "df_positives = pd.DataFrame({'reviews':[load_doc(x) for x in positive_sample_file_list], 'sentiment': np.ones(SAMPLE_SIZE)})\n",
        "df_negatives = pd.DataFrame({'reviews':[load_doc(x) for x in negative_sample_file_list], 'sentiment': np.zeros(SAMPLE_SIZE)})\n",
        "\n",
        "print(\"Positive review(s):\", df_positives['reviews'][1][:100])\n",
        "print(\"Negative review(s):\", df_negatives['reviews'][1][:100])\n",
        "\n",
        "df = pd.concat([df_positives, df_negatives], ignore_index=True)\n",
        "\n",
        "df = shuffle(df)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['reviews'], df['sentiment'], test_size=0.25)\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive review(s): This movie is awesome for three main reasons. It is esthetically beautiful. I absolutely loved that.\n",
            "Negative review(s): What? You were not aware that Scooby-Doo battled zombies? Well, you might also not be aware of this \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PA4PuIKtNCGh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#ML STUDY GROUP\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "class PreProcessor:\n",
        "    def __init__(self,REVIEWS,REVIEWS_VAL,LABELS,LABELS_VAL,WE_FILE):\n",
        "        self.reviews = REVIEWS\n",
        "        self.reviews_val = REVIEWS_VAL\n",
        "        self.labels = LABELS\n",
        "        self.labels_val = LABELS_VAL\n",
        "        self.we_file = WE_FILE\n",
        "\n",
        "    def tokenize(self):\n",
        "#         set_trace()\n",
        "        print(self.reviews[0])\n",
        "\n",
        "        tokenizer = Tokenizer()\n",
        "        tokenizer.fit_on_texts(self.reviews)\n",
        "\n",
        "        self.sequences = tokenizer.texts_to_sequences(self.reviews)\n",
        "        self.sequences_val = tokenizer.texts_to_sequences(self.reviews_val)\n",
        "\n",
        "        self.word_index = tokenizer.word_index\n",
        "        print(\"Found %s unique tokens\" %(len(self.word_index)))\n",
        "\n",
        "    def make_data(self):\n",
        "        self.MAX_SEQUENCE_LENGTH = max([len(self.sequences[i]) for i in range(len(self.sequences))])\n",
        "        print(\"self.MAX_SEQUENCE_LENGTH: {}\".format(self.MAX_SEQUENCE_LENGTH))\n",
        "\n",
        "        review = pad_sequences(self.sequences,maxlen=self.MAX_SEQUENCE_LENGTH)\n",
        "        review_val = pad_sequences(self.sequences_val,maxlen=self.MAX_SEQUENCE_LENGTH)\n",
        "        \n",
        "        labels = to_categorical(self.labels)\n",
        "        labels_val = to_categorical(self.labels_val)\n",
        "\n",
        "        print(\"Shape of data tensor: \" +str(review.shape))\n",
        "        print(\"Shape of label tensor: \" +str(labels.shape))\n",
        "\n",
        "        return review, review_val, labels, labels_val\n",
        "        \n",
        "    def get_word_embedding_matrix(self,EMBEDDING_DIM=100):\n",
        "        embeddings_index = {}\n",
        "\n",
        "        if self.we_file == \"rand\":\n",
        "            return None\n",
        "\n",
        "        f = open(self.we_file)\n",
        "\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "        f.close()\n",
        "\n",
        "        print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "        self.embedding_matrix = np.zeros((len(self.word_index)+1, EMBEDDING_DIM))\n",
        "\n",
        "        for word, i in self.word_index.items():\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                # words not found in embedding index will be all-zeros.\n",
        "                self.embedding_matrix[i] = embedding_vector\n",
        "\n",
        "        return self.embedding_matrix\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VEIPYnFIPMxB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# MacOSX: See https://www.mkyong.com/mac/wget-on-mac-os-x/ for wget\n",
        "if not os.path.isfile('./glove.6B.300d.txt'):\n",
        "    if not os.path.isfile('./glove.6B.zip'):\n",
        "      !wget http://nlp.stanford.edu/data/glove.6B.zip \n",
        "\n",
        "    if not os.path.isfile('./glove.6B.300d.txt'):  \n",
        "      !unzip glove.6B.zip \n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sl6W6_92NCGu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "from keras.layers import Embedding, Dense, Input, BatchNormalization, Activation, Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adagrad, Adam\n",
        "from keras import backend as K\n",
        "\n",
        "from pdb import set_trace\n",
        "\n",
        "embedding_dim = 300\n",
        "num_hidden_layers = 3\n",
        "num_hidden_units = 300\n",
        "num_epochs = 100\n",
        "batch_size = 512\n",
        "dropout_rate = 0.2\n",
        "word_dropout_rate = 0.3\n",
        "activation = 'relu'\n",
        "\n",
        "args = {}\n",
        "args['We']='./glove.6B.300d.txt'\n",
        "args['Wels']='' ### rand or ''\n",
        "args['model']='dan'  ### nbow OR dan\n",
        "args['wd']='y'\n",
        "\n",
        "# reviews=X_train.values\n",
        "# reviews_val=X_test.values\n",
        "# labels=y_train.values\n",
        "# labels_val=y_test.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OO66Wuy-NCHP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "b9659f8d-d005-4673-eb77-5b500c8466e1"
      },
      "cell_type": "code",
      "source": [
        "pp = PreProcessor(X_train,X_test,y_train,y_test,args['We'])\n",
        "pp.tokenize()\n",
        "\n",
        "encoded_X_train,encoded_X_test,y_train,y_test = pp.make_data()\n",
        "\n",
        "embedding_matrix = pp.get_word_embedding_matrix(embedding_dim)\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "An unusual take on time travel: instead of traveling to Earth's past, the main trio get stuck in the past history of another planet. They beam down to this planet, whose sun is scheduled to go nova in 3 or 4 hours (that's cutting it close!). In some kind of futuristic library, they meet Mr. Atoz (A to Z, get it? ha-ha) and his duplicates. It turns out, instead of escaping their planet's destruction via space travel, the usual way, the inhabitants have all escaped into their planet's various past time eras. Mr. Atoz uses a time machine to send people on their way after they make a selection (check out the discs we see here, another Trek prognostication of CDs and DVDs!). When Mr. Atoz prepares the machine (the Atavachron-what-sis), gallant Kirk hears a woman's scream and runs into the planet's version of Earth's 17th century, where he gets into a sword fight and is arrested for witchery. There's an eccentric but good performance here by the actress playing a female of ill repute in this time, using phrasing of the time (\"...you're a bully fine coo.. Witch! Witch! They'll burn ye...!\"). Spock & McCoy follow Kirk, but end up in an ice age, 5000 years earlier.  Kirk manages to get back to the library first. The real story here is Spock's reversion to the barbaric tendencies of his ancestors, the warlike Vulcans of 5000 years ago. This doesn't really make sense, except that maybe this time machine is responsible for the change (even so, Spock & McCoy weren't 'prepared' by Atoz - oh, well; it also seems to me Spock was affected by the transition almost immediately - he mentions being from 'millions of light years' away, instead of the correct hundreds or thousands - a gross error for a logical Vulcan). In any case, Spock really shows his nasty side here - forget \"Day of the Dove\" and remember \"This Side of Paradise\" - McCoy quickly finds out that his Vulcan buddy will not stand for any of his usual baiting and nearly gets his face rearranged. Spock also gets it on with Zarabeth, a comely female who had been exiled to this cold past as punishment (a couple of Trek novels were written about Spock's son, the result of this union). All these scenes are eye-openers, a reminder of just how much Spock conceals or holds in. It's also ironic that, only a few episodes earlier (\"Requiem for Methuselah\"), McCoy was pointing out to Spock how he would never know the pain of love - and now all this happens. Kirk, meanwhile, tussles with the elderly Atoz, who insists that Kirk head back to some past era (\"You are evidently a suicidal maniac\" - great stuff from actor Wolfe, last seen in \"Bread and Circuses\"). It all works out in the end, but, like I mentioned earlier, they cut it very close. A neat little Trek adventure, with a definite cosmic slant.\n",
            "Found 77805 unique tokens\n",
            "self.MAX_SEQUENCE_LENGTH: 2473\n",
            "Shape of data tensor: (18750, 2473)\n",
            "Shape of label tensor: (18750, 2)\n",
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ab3wHsrPNCHp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "07935dc6-ad72-457d-f9c8-0ff63f4abab2"
      },
      "cell_type": "code",
      "source": [
        "embedding_matrix.shape\n",
        "# pp.MAX_SEQUENCE_LENGTH\n",
        "len(pp.word_index)+1\n",
        "embedding_dim"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "metadata": {
        "id": "OSGu8ksfNCIB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://machinelearningmastery.com/develop-n-gram-multichannel-convolutional-neural-network-sentiment-analysis/"
      ]
    },
    {
      "metadata": {
        "id": "XOJqpJ0nNCIj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "153da794-6b24-4116-ca8b-f66325863230"
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Concatenate\n",
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.merge import concatenate\n",
        "# create the model\n",
        "\n",
        "channels = []\n",
        "inputs = []\n",
        "encoded_X_trains= []\n",
        "encoded_X_tests = []\n",
        "for filter_len in [3,4,5]:\n",
        "    inputs1 = Input(shape=(pp.MAX_SEQUENCE_LENGTH,))\n",
        "    inputs.append(inputs1)\n",
        "    embedding1 = Embedding(len(pp.word_index)+1,embedding_dim,weights=[embedding_matrix],\\\n",
        "                           input_length=pp.MAX_SEQUENCE_LENGTH,trainable=True)(inputs1)\n",
        "    conv1 = Conv1D(filters=128, kernel_size=filter_len, padding='same', activation='relu')(embedding1)\n",
        "    drop1 = Dropout(0.5)(conv1)\n",
        "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "    flat1 = Flatten()(pool1)\n",
        "    channels.append(flat1)\n",
        "    encoded_X_trains.append(encoded_X_train)\n",
        "    encoded_X_tests.append(encoded_X_test)\n",
        "    \n",
        "# merge\n",
        "merged = concatenate(channels)\n",
        "# interpretation\n",
        "outputs = Dense(2, activation='softmax')(merged)\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "# compile\n",
        "    \n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy','categorical_accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            (None, 2473)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            (None, 2473)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            (None, 2473)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, 2473, 300)    23341800    input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_5 (Embedding)         (None, 2473, 300)    23341800    input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_6 (Embedding)         (None, 2473, 300)    23341800    input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 2473, 128)    115328      embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, 2473, 128)    153728      embedding_5[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 2473, 128)    192128      embedding_6[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 2473, 128)    0           conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 2473, 128)    0           conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 2473, 128)    0           conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1D)  (None, 1236, 128)    0           dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1D)  (None, 1236, 128)    0           dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_6 (MaxPooling1D)  (None, 1236, 128)    0           dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_4 (Flatten)             (None, 158208)       0           max_pooling1d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_5 (Flatten)             (None, 158208)       0           max_pooling1d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_6 (Flatten)             (None, 158208)       0           max_pooling1d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 474624)       0           flatten_4[0][0]                  \n",
            "                                                                 flatten_5[0][0]                  \n",
            "                                                                 flatten_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 2)            949250      concatenate_2[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 71,435,834\n",
            "Trainable params: 71,435,834\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rxiSQ40iNCI7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1050
        },
        "outputId": "e17e9c36-2070-441f-c9d5-f21630171bcb"
      },
      "cell_type": "code",
      "source": [
        "%pdb off\n",
        "batch_size = 128\n",
        "num_epochs = 3\n",
        "\n",
        "model.fit(encoded_X_trains,y_train,batch_size=batch_size,epochs=num_epochs,\\\n",
        "          validation_data=(encoded_X_tests,y_test))\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Automatic pdb calling has been turned OFF\n",
            "Train on 18750 samples, validate on 6250 samples\n",
            "Epoch 1/10\n",
            "18750/18750 [==============================] - 127s 7ms/step - loss: 0.6009 - acc: 0.6988 - categorical_accuracy: 0.6988 - val_loss: 0.3382 - val_acc: 0.8637 - val_categorical_accuracy: 0.8637\n",
            "Epoch 2/10\n",
            "18750/18750 [==============================] - 123s 7ms/step - loss: 0.2385 - acc: 0.9066 - categorical_accuracy: 0.9066 - val_loss: 0.2779 - val_acc: 0.8877 - val_categorical_accuracy: 0.8877\n",
            "Epoch 3/10\n",
            "18750/18750 [==============================] - 123s 7ms/step - loss: 0.1157 - acc: 0.9581 - categorical_accuracy: 0.9581 - val_loss: 0.2701 - val_acc: 0.8926 - val_categorical_accuracy: 0.8926\n",
            "Epoch 4/10\n",
            "18750/18750 [==============================] - 123s 7ms/step - loss: 0.0469 - acc: 0.9874 - categorical_accuracy: 0.9874 - val_loss: 0.3219 - val_acc: 0.8838 - val_categorical_accuracy: 0.8838\n",
            "Epoch 5/10\n",
            "18750/18750 [==============================] - 123s 7ms/step - loss: 0.0195 - acc: 0.9955 - categorical_accuracy: 0.9955 - val_loss: 0.3868 - val_acc: 0.8723 - val_categorical_accuracy: 0.8723\n",
            "Epoch 6/10\n",
            " 1920/18750 [==>...........................] - ETA: 1:41 - loss: 0.0133 - acc: 0.9969 - categorical_accuracy: 0.9969"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-302e21500624>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_X_trains\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m          \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_X_tests\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "S62Y-vjAQPo-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "y_train"
      ]
    },
    {
      "metadata": {
        "id": "U1L_TrbfNCJQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_train\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-pG_hc2xQRMx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}