{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Sentiment Analysis with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "colab_type": "code",
    "id": "CML_IG6z-iwM",
    "outputId": "d9301f36-c1cf-4b3f-e639-a731880ed036"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jeremie/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# uncomment these for Google collab, will have already been installed in local environment \n",
    "# if 'pip install -r requirements.txt' has been run\n",
    "#!pip install nltk\n",
    "#!pip install --upgrade gensim\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "from pdb import set_trace\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import nltk\n",
    "\n",
    "\n",
    "import glob\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FJiWamI00hBp",
    "outputId": "e3c105d5-037f-4521-b8e1-a83cb7a5edeb"
   },
   "outputs": [],
   "source": [
    "# MacOSX: See https://www.mkyong.com/mac/wget-on-mac-os-x/ for wget\n",
    "if not os.path.isdir('../aclImdb'):\n",
    "    if not os.path.isfile('../aclImdb_v1.tar.gz'):\n",
    "      !wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz \n",
    "\n",
    "    if not os.path.isdir('../aclImdb'):  \n",
    "      !tar -xf aclImdb_v1.tar.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U5Tnmoh-Dpfk"
   },
   "outputs": [],
   "source": [
    "time_beginning_of_notebook = time.time()\n",
    "SAMPLE_SIZE=3000\n",
    "positive_sample_file_list = glob.glob(os.path.join('../aclImdb/train/pos', \"*.txt\"))\n",
    "positive_sample_file_list = positive_sample_file_list[:SAMPLE_SIZE]\n",
    "\n",
    "negative_sample_file_list = glob.glob(os.path.join('../aclImdb/train/neg', \"*.txt\"))\n",
    "negative_sample_file_list = negative_sample_file_list[:SAMPLE_SIZE]\n",
    "\n",
    "import re\n",
    "\n",
    "# load doc into memory\n",
    "# regex to clean markup elements \n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', encoding='utf8')\n",
    "    # read all text\n",
    "    text = re.sub('<[^>]*>', ' ', file.read())\n",
    "    #text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "lfr3bXOgXNJJ",
    "outputId": "cc06dd0d-e886-4090-c972-cd05520adaf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive review(s): NYC model Alison Parker (Cristina Raines) rents a room in an old brownstone where she meets a few bi\n",
      "Negative review(s): The reason the DVD releases of this film are in black and white is because nobody can get their hand\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df_positives = pd.DataFrame({'reviews':[load_doc(x) for x in positive_sample_file_list], 'sentiment': np.ones(SAMPLE_SIZE)})\n",
    "df_negatives = pd.DataFrame({'reviews':[load_doc(x) for x in negative_sample_file_list], 'sentiment': np.zeros(SAMPLE_SIZE)})\n",
    "\n",
    "print(\"Positive review(s):\", df_positives['reviews'][1][:100])\n",
    "print(\"Negative review(s):\", df_negatives['reviews'][1][:100])\n",
    "\n",
    "df = pd.concat([df_positives, df_negatives], ignore_index=True)\n",
    "\n",
    "df = shuffle(df)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['reviews'], df['sentiment'], test_size=0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#ML STUDY GROUP\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "class PreProcessor:\n",
    "    def __init__(self,REVIEWS,REVIEWS_VAL,LABELS,LABELS_VAL,WE_FILE):\n",
    "        self.reviews = REVIEWS\n",
    "        self.reviews_val = REVIEWS_VAL\n",
    "        self.labels = LABELS\n",
    "        self.labels_val = LABELS_VAL\n",
    "        self.we_file = WE_FILE\n",
    "\n",
    "    def tokenize(self):\n",
    "#         set_trace()\n",
    "        print(self.reviews[0])\n",
    "\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(self.reviews)\n",
    "\n",
    "        self.sequences = tokenizer.texts_to_sequences(self.reviews)\n",
    "        self.sequences_val = tokenizer.texts_to_sequences(self.reviews_val)\n",
    "\n",
    "        self.word_index = tokenizer.word_index\n",
    "        print(\"Found %s unique tokens\" %(len(self.word_index)))\n",
    "\n",
    "    def make_data(self):\n",
    "        self.MAX_SEQUENCE_LENGTH = max([len(self.sequences[i]) for i in range(len(self.sequences))])\n",
    "        print(\"self.MAX_SEQUENCE_LENGTH: {}\".format(self.MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "        review = pad_sequences(self.sequences,maxlen=self.MAX_SEQUENCE_LENGTH)\n",
    "        review_val = pad_sequences(self.sequences_val,maxlen=self.MAX_SEQUENCE_LENGTH)\n",
    "        \n",
    "        labels = to_categorical(self.labels)\n",
    "        labels_val = to_categorical(self.labels_val)\n",
    "\n",
    "        print(\"Shape of data tensor: \" +str(review.shape))\n",
    "        print(\"Shape of label tensor: \" +str(labels.shape))\n",
    "\n",
    "        return review, review_val, labels, labels_val\n",
    "        \n",
    "    def get_word_embedding_matrix(self,EMBEDDING_DIM=100):\n",
    "        embeddings_index = {}\n",
    "\n",
    "        if self.we_file == \"rand\":\n",
    "            return None\n",
    "\n",
    "        f = open(self.we_file)\n",
    "\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        f.close()\n",
    "\n",
    "        print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "        self.embedding_matrix = np.zeros((len(self.word_index)+1, EMBEDDING_DIM))\n",
    "\n",
    "        for word, i in self.word_index.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # words not found in embedding index will be all-zeros.\n",
    "                self.embedding_matrix[i] = embedding_vector\n",
    "\n",
    "        return self.embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from dan.custom_layers import AverageWords, WordDropout\n",
    "\n",
    "from keras.layers import Embedding, Dense, Input, BatchNormalization, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adagrad, Adam\n",
    "from keras import backend as K\n",
    "\n",
    "from pdb import set_trace\n",
    "\n",
    "embedding_dim = 300\n",
    "num_hidden_layers = 3\n",
    "num_hidden_units = 300\n",
    "num_epochs = 100\n",
    "batch_size = 512\n",
    "dropout_rate = 0.2\n",
    "word_dropout_rate = 0.3\n",
    "activation = 'relu'\n",
    "\n",
    "args = {}\n",
    "args['We']='data/glove.6B.300d.txt'\n",
    "args['Wels']='' ### rand or ''\n",
    "args['model']='dan'  ### nbow OR dan\n",
    "args['wd']='y'\n",
    "\n",
    "# reviews=X_train.values\n",
    "# reviews_val=X_test.values\n",
    "# labels=y_train.values\n",
    "# labels_val=y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_val.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I thought I should qualify my position after reading other reviews. The movie is not great, but it has a lot of great elements. The lighting and scenes along with the camera work are great. The story is slow and weak, but entertaining. The acting is bad, but no worse than you will find on the SyFy Channel. The music is pretty good and the gore is good. It has the great Leather Face in the film and is produced by Bruce Campbell. I watched the complete movie and while mostly predictable, it was still enjoyable. The women are attractive enough and the lead actor does a good job of being brooding and creepy. The movie was remarkably clean for a modern film and the violence appropriate for children 13 and up. There was no sex scenes. I gave it 7 out of 10 and I think that is fair. I would watch it again if I had nothing better to do. The gay sounding angel was the most annoying aspect of the film, the devil is quite creepy.\n",
      "Found 40729 unique tokens\n",
      "self.MAX_SEQUENCE_LENGTH: 1580\n",
      "Shape of data tensor: (4500, 1580)\n",
      "Shape of label tensor: (4500, 2)\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "pp = PreProcessor(X_train,X_test,y_train,y_test,args['We'])\n",
    "pp.tokenize()\n",
    "\n",
    "encoded_X_train,encoded_X_test,y_train,y_test = pp.make_data()\n",
    "\n",
    "embedding_matrix = pp.get_word_embedding_matrix(embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40730"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape\n",
    "# pp.MAX_SEQUENCE_LENGTH\n",
    "len(pp.word_index)+1\n",
    "embedding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/develop-n-gram-multichannel-convolutional-neural-network-sentiment-analysis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           (None, 1580)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           (None, 1580)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           (None, 1580)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_15 (Embedding)        (None, 1580, 300)    12219000    input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_16 (Embedding)        (None, 1580, 300)    12219000    input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_17 (Embedding)        (None, 1580, 300)    12219000    input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 1580, 128)    115328      embedding_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 1580, 128)    153728      embedding_16[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 1580, 128)    192128      embedding_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 1580, 128)    0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 1580, 128)    0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 1580, 128)    0           conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 790, 128)     0           dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 790, 128)     0           dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 790, 128)     0           dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 101120)       0           max_pooling1d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 101120)       0           max_pooling1d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 101120)       0           max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 303360)       0           flatten_16[0][0]                 \n",
      "                                                                 flatten_17[0][0]                 \n",
      "                                                                 flatten_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 2)            606722      concatenate_8[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 37,724,906\n",
      "Trainable params: 1,067,906\n",
      "Non-trainable params: 36,657,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Concatenate\n",
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "# create the model\n",
    "\n",
    "channels = []\n",
    "inputs = []\n",
    "encoded_X_trains= []\n",
    "encoded_X_tests = []\n",
    "for filter_len in [3,4,5]:\n",
    "    inputs1 = Input(shape=(pp.MAX_SEQUENCE_LENGTH,))\n",
    "    inputs.append(inputs1)\n",
    "    embedding1 = Embedding(len(pp.word_index)+1,embedding_dim,weights=[embedding_matrix],\\\n",
    "                           input_length=pp.MAX_SEQUENCE_LENGTH,trainable=True)(inputs1)\n",
    "    conv1 = Conv1D(filters=128, kernel_size=filter_len, padding='same', activation='relu')(embedding1)\n",
    "    drop1 = Dropout(0.5)(conv1)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    channels.append(flat1)\n",
    "    encoded_X_trains.append(encoded_X_train)\n",
    "    encoded_X_tests.append(encoded_X_test)\n",
    "    \n",
    "# merge\n",
    "merged = concatenate(channels)\n",
    "# interpretation\n",
    "outputs = Dense(2, activation='softmax')(merged)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "# compile\n",
    "    \n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy','categorical_accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "Train on 4500 samples, validate on 1500 samples\n",
      "Epoch 1/5\n",
      "4500/4500 [==============================] - 202s 45ms/step - loss: 0.4913 - acc: 0.7789 - categorical_accuracy: 0.7789 - val_loss: 0.4981 - val_acc: 0.7733 - val_categorical_accuracy: 0.7733\n",
      "Epoch 2/5\n",
      "4500/4500 [==============================] - 203s 45ms/step - loss: 0.3115 - acc: 0.8736 - categorical_accuracy: 0.8736 - val_loss: 0.4344 - val_acc: 0.8000 - val_categorical_accuracy: 0.8000\n",
      "Epoch 3/5\n",
      "1280/4500 [=======>......................] - ETA: 2:15 - loss: 0.2110 - acc: 0.9406 - categorical_accuracy: 0.9406"
     ]
    }
   ],
   "source": [
    "%pdb off\n",
    "batch_size = 128\n",
    "num_epochs = 5\n",
    "\n",
    "model.fit(encoded_X_trains,y_train,batch_size=batch_size,epochs=num_epochs,\\\n",
    "          validation_data=(encoded_X_tests,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
