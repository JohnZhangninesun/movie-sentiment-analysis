{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_cnn_with_multiple_channels.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "-rcv7L0dRSCc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Movie Sentiment Analysis with Keras"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "CML_IG6z-iwM",
        "outputId": "2db9531d-ccbc-42ae-fe74-af07cbf66346",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "cell_type": "code",
      "source": [
        "# uncomment these for Google collab, will have already been installed in local environment \n",
        "# if 'pip install -r requirements.txt' has been run\n",
        "!pip install nltk\n",
        "!pip install --upgrade gensim\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import os.path\n",
        "\n",
        "from pdb import set_trace\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import nltk\n",
        "\n",
        "\n",
        "import glob\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import time"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.11.0)\n",
            "Requirement already up-to-date: gensim in /usr/local/lib/python3.6/dist-packages (3.7.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.8.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (2.49.0)\n",
            "Requirement already satisfied, skipping upgrade: bz2file in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (0.98)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (2.18.4)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.7.0->gensim) (1.9.86)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (1.22)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (2018.11.29)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.7.0->gensim) (2.6)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.13.0,>=1.12.86 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (1.12.86)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (0.9.3)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.2.0,>=0.1.10 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.7.0->gensim) (0.1.13)\n",
            "Requirement already satisfied, skipping upgrade: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.86->boto3->smart-open>=1.7.0->gensim) (0.14)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.86->boto3->smart-open>=1.7.0->gensim) (2.5.3)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "FJiWamI00hBp",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# MacOSX: See https://www.mkyong.com/mac/wget-on-mac-os-x/ for wget\n",
        "if not os.path.isdir('./aclImdb'):\n",
        "    if not os.path.isfile('./aclImdb_v1.tar.gz'):\n",
        "      !wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz \n",
        "\n",
        "    if not os.path.isdir('./aclImdb'):  \n",
        "      !tar -xf aclImdb_v1.tar.gz "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "U5Tnmoh-Dpfk",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "time_beginning_of_notebook = time.time()\n",
        "SAMPLE_SIZE=12500\n",
        "positive_sample_file_list = glob.glob(os.path.join('./aclImdb/train/pos', \"*.txt\"))\n",
        "positive_sample_file_list = positive_sample_file_list[:SAMPLE_SIZE]\n",
        "\n",
        "negative_sample_file_list = glob.glob(os.path.join('./aclImdb/train/neg', \"*.txt\"))\n",
        "negative_sample_file_list = negative_sample_file_list[:SAMPLE_SIZE]\n",
        "\n",
        "import re\n",
        "\n",
        "# load doc into memory\n",
        "# regex to clean markup elements \n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r', encoding='utf8')\n",
        "    # read all text\n",
        "    text = re.sub('<[^>]*>', ' ', file.read())\n",
        "    #text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "lfr3bXOgXNJJ",
        "outputId": "d2fdd042-2ff1-4e96-c735-599f78e763df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "df_positives = pd.DataFrame({'reviews':[load_doc(x) for x in positive_sample_file_list], 'sentiment': np.ones(SAMPLE_SIZE)})\n",
        "df_negatives = pd.DataFrame({'reviews':[load_doc(x) for x in negative_sample_file_list], 'sentiment': np.zeros(SAMPLE_SIZE)})\n",
        "\n",
        "print(\"Positive review(s):\", df_positives['reviews'][1][:100])\n",
        "print(\"Negative review(s):\", df_negatives['reviews'][1][:100])\n",
        "\n",
        "df = pd.concat([df_positives, df_negatives], ignore_index=True)\n",
        "\n",
        "df = shuffle(df)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['reviews'], df['sentiment'], test_size=0.25)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive review(s): This movie is awesome for three main reasons. It is esthetically beautiful. I absolutely loved that.\n",
            "Negative review(s): What? You were not aware that Scooby-Doo battled zombies? Well, you might also not be aware of this \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nDCSpt3tRSDL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# def lstm_keras():\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Embedding, LSTM, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zgpo5KA8RSDU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "vocab_size = 1000\n",
        "\n",
        "# Tokenizer(num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',\\\n",
        "#           lower=True, split=' ', char_level=False, oov_token=None, document_count=0)\n",
        "\n",
        "tokenize = Tokenizer(num_words=vocab_size)\n",
        "tokenize.fit_on_texts(X_train)\n",
        "\n",
        "tokenized_X_train = tokenize.texts_to_sequences(X_train)\n",
        "tokenized_X_test = tokenize.texts_to_sequences(X_test)\n",
        "\n",
        "max_document_length = max([len(x) for x in np.concatenate((tokenized_X_train,tokenized_X_test),axis=0)])\n",
        "encoded_X_train = sequence.pad_sequences(tokenized_X_train, maxlen=max_document_length)\n",
        "encoded_X_test = sequence.pad_sequences(tokenized_X_test, maxlen=max_document_length)\n",
        "\n",
        "\n",
        "encoder = LabelBinarizer()\n",
        "encoder.fit(y_train)\n",
        "encoded_y_train = encoder.transform(y_train)\n",
        "encoded_y_test = encoder.transform(y_test)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I-soTHAmRSDc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0V155QJxRSDj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k4UdfCfqRSDp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1G6rYjPnRSDw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://machinelearningmastery.com/develop-n-gram-multichannel-convolutional-neural-network-sentiment-analysis/"
      ]
    },
    {
      "metadata": {
        "id": "tOMGc4wCRSDy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "dbd6cdac-6076-4874-9d6c-109e92a89e5b"
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Concatenate\n",
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.utils import to_categorical\n",
        "# create the model\n",
        "\n",
        "\n",
        "channels = []\n",
        "inputs = []\n",
        "\n",
        "encoded_X_trains= []\n",
        "encoded_X_tests = []\n",
        "for filter_len in [3,4,5]:\n",
        "# for filter_len in [3,4]:\n",
        "    inputs1 = Input(shape=(max_document_length,))\n",
        "    inputs.append(inputs1)\n",
        "    embedding1 = Embedding(vocab_size, 128, input_length=max_document_length)(inputs1)\n",
        "    conv1 = Conv1D(filters=128, kernel_size=filter_len, padding='same', activation='relu')(embedding1)\n",
        "    drop1 = Dropout(0.5)(conv1)\n",
        "    pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "    flat1 = Flatten()(pool1)\n",
        "    channels.append(flat1)\n",
        "    encoded_X_trains.append(encoded_X_train)\n",
        "    encoded_X_tests.append(encoded_X_test)\n",
        "    \n",
        "# merge\n",
        "merged = concatenate(channels)\n",
        "# interpretation\n",
        "outputs = Dense(2, activation='softmax')(merged)\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "# compile\n",
        "    \n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy','categorical_accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_10 (InputLayer)           (None, 1495)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_11 (InputLayer)           (None, 1495)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_12 (InputLayer)           (None, 1495)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_10 (Embedding)        (None, 1495, 128)    128000      input_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_11 (Embedding)        (None, 1495, 128)    128000      input_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_12 (Embedding)        (None, 1495, 128)    128000      input_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_10 (Conv1D)              (None, 1495, 128)    49280       embedding_10[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_11 (Conv1D)              (None, 1495, 128)    65664       embedding_11[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_12 (Conv1D)              (None, 1495, 128)    82048       embedding_12[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 1495, 128)    0           conv1d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 1495, 128)    0           conv1d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 1495, 128)    0           conv1d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_10 (MaxPooling1D) (None, 747, 128)     0           dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_11 (MaxPooling1D) (None, 747, 128)     0           dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_12 (MaxPooling1D) (None, 747, 128)     0           dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "flatten_10 (Flatten)            (None, 95616)        0           max_pooling1d_10[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "flatten_11 (Flatten)            (None, 95616)        0           max_pooling1d_11[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "flatten_12 (Flatten)            (None, 95616)        0           max_pooling1d_12[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 286848)       0           flatten_10[0][0]                 \n",
            "                                                                 flatten_11[0][0]                 \n",
            "                                                                 flatten_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 2)            573698      concatenate_4[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 1,154,690\n",
            "Trainable params: 1,154,690\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VTkjFCgpRSD7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "bd4d3c59-9540-44cf-ae9b-5bbcb90d7b3f"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "batch_size = 64\n",
        "num_epochs = 2\n",
        "\n",
        "model.fit(encoded_X_trains,to_categorical(y_train),batch_size=batch_size,epochs=num_epochs,\\\n",
        "          validation_data=(encoded_X_tests,to_categorical(y_test)))\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 18750 samples, validate on 6250 samples\n",
            "Epoch 1/5\n",
            "18750/18750 [==============================] - 44s 2ms/step - loss: 0.4377 - acc: 0.7772 - categorical_accuracy: 0.7772 - val_loss: 0.3443 - val_acc: 0.8502 - val_categorical_accuracy: 0.8502\n",
            "Epoch 2/5\n",
            "18750/18750 [==============================] - 44s 2ms/step - loss: 0.2669 - acc: 0.8892 - categorical_accuracy: 0.8892 - val_loss: 0.3154 - val_acc: 0.8656 - val_categorical_accuracy: 0.8656\n",
            "Epoch 3/5\n",
            "18750/18750 [==============================] - 44s 2ms/step - loss: 0.1723 - acc: 0.9332 - categorical_accuracy: 0.9332 - val_loss: 0.3528 - val_acc: 0.8613 - val_categorical_accuracy: 0.8613\n",
            "Epoch 4/5\n",
            "18750/18750 [==============================] - 44s 2ms/step - loss: 0.0840 - acc: 0.9731 - categorical_accuracy: 0.9731 - val_loss: 0.3943 - val_acc: 0.8589 - val_categorical_accuracy: 0.8589\n",
            "Epoch 5/5\n",
            "18750/18750 [==============================] - 44s 2ms/step - loss: 0.0395 - acc: 0.9894 - categorical_accuracy: 0.9894 - val_loss: 0.5065 - val_acc: 0.8507 - val_categorical_accuracy: 0.8507\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff62da90f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "oCrVfxr7RSED",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}