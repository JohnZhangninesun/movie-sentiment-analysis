{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lMofUnI1i05g"
   },
   "source": [
    "Sentiment analysis of movie (IMDB) reviews using dataset provided by the ACL 2011 paper, \n",
    "see http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "This notebook uses neural net models\n",
    "\n",
    "The plan is to compare a variety of hyperparameters, vectorization techniques, neural net based models:\n",
    "* dense neural network with bag of words\n",
    "* dense neural network with fixed size input and words mapped to integers\n",
    "* LSTM\n",
    "* CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L-FXDJ17i05n"
   },
   "source": [
    "### Table of Contents<a class=\"anchor\" id=\"table\"></a>\n",
    "* [Load data](#load)\n",
    "* [Train different architectures](#train)\n",
    "    * [Train NN 50 - 10 - 1](#train1)\n",
    "    * [Train NN 256 - 128 - 1](#train2)\n",
    "    * [Train NN with K-Fold cross validation](#kfold)\n",
    "    * [Train RNN](#rnn)\n",
    "* [Optimize](#opti)\n",
    "    * [Optimize on dropouts](#opti_d)\n",
    "        * no dropout\n",
    "        * low dropout on 1 layer\n",
    "        * high dropout on 1 layer\n",
    "        * low dropout on 2 layers\n",
    "        * high dropout on 2 layers\n",
    "        * [Observation](#opti_d_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hFAxjxjImFl2",
    "outputId": "4b397b76-1a96-42f0-a875-03e626135057"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "\u001b[33m  Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x107294550>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known',)': /simple/wget/\u001b[0m\n",
      "\u001b[33m  Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x1072942d0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known',)': /simple/wget/\u001b[0m\n",
      "\u001b[33m  Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x107294310>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known',)': /simple/wget/\u001b[0m\n",
      "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
      "Building wheels for collected packages: wget\n",
      "  Running setup.py bdist_wheel for wget ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/swami/Library/Caches/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "eqIsNp2Yi05v",
    "outputId": "8b80cba9-3a53-491e-bd9e-ddafae6a4542"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/swami/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "nltk.download('punkt')\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tafRpfeyi06R"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "x_LeQpbQi06j",
    "outputId": "4b1dcfaa-99bf-4c82-e6eb-ddadf2bb7036"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory exists, taking no action\n"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "import tarfile\n",
    "\n",
    "# By checking if the directory exists first, we allow people to delete the tarfile without the notebook re-downloading it\n",
    "if os.path.isdir('aclImdb'):\n",
    "    print(\"Dataset directory exists, taking no action\")\n",
    "else:    \n",
    "    if not os.path.isfile('aclImdb_v1.tar.gz'):\n",
    "        print(\"Downloading dataset\")\n",
    "        #!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "        wget.download('http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz')\n",
    "    else:\n",
    "        print(\"Dataset already downloaded\")\n",
    "    \n",
    "    print(\"Unpacking dataset\")\n",
    "    #!tar -xf aclImdb_v1.tar.gz \n",
    "    tar = tarfile.open(\"aclImdb_v1.tar.gz\")\n",
    "    tar.extractall()\n",
    "    tar.close()\n",
    "    print(\"Dataset unpacked in aclImdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PQAl9GvRi067"
   },
   "outputs": [],
   "source": [
    "# configuration\n",
    "SAMPLE_SIZE=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasGPU():\n",
    "  device_name = tf.test.gpu_device_name()\n",
    "  if device_name != '/device:GPU:0':\n",
    "    print('No GPU found')\n",
    "    return false\n",
    "  else:\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "    return true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'false' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-cc09d105985a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhasGPU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-3dd5a9dc61e3>\u001b[0m in \u001b[0;36mhasGPU\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'/device:GPU:0'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No GPU found'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found GPU at: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'false' is not defined"
     ]
    }
   ],
   "source": [
    "hasGPU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vMYBkcdIB9uc"
   },
   "source": [
    "<a href='#table'>Back</a>\n",
    "# Load data<a class=\"anchor\" id=\"load\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dense vector from reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bcTVSZK-i07N"
   },
   "outputs": [],
   "source": [
    "time_beginning_of_notebook = time.time()\n",
    "positive_file_list = glob.glob(os.path.join('aclImdb/train/pos', \"*.txt\"))\n",
    "positive_sample_file_list = positive_file_list[:SAMPLE_SIZE]\n",
    "\n",
    "negative_file_list = glob.glob(os.path.join('aclImdb/train/neg', \"*.txt\"))\n",
    "negative_sample_file_list = negative_file_list[:SAMPLE_SIZE]\n",
    "\n",
    "import re\n",
    "\n",
    "# load doc into memory\n",
    "# regex to clean markup elements \n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', encoding='utf8')\n",
    "    # read all text\n",
    "    text = re.sub('<[^>]*>', ' ', file.read())\n",
    "    #text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dcjl_fogi07W"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'encoding' is an invalid keyword argument for this function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-99a80cb64bf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpositive_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mload_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpositive_sample_file_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnegative_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mload_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnegative_sample_file_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpositive_tokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpositive_strings\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnegative_tokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnegative_strings\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-22bb1e89e3b5>\u001b[0m in \u001b[0;36mload_doc\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# open the file as read only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;31m# read all text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<[^>]*>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'encoding' is an invalid keyword argument for this function"
     ]
    }
   ],
   "source": [
    "positive_strings = [load_doc(x) for x in positive_sample_file_list]\n",
    "negative_strings = [load_doc(x) for x in negative_sample_file_list]\n",
    "\n",
    "positive_tokenized = [word_tokenize(s) for s in positive_strings]\n",
    "negative_tokenized = [word_tokenize(s) for s in negative_strings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DrxFWNmFi07d"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TdNEv_DKi07l"
   },
   "outputs": [],
   "source": [
    "total_counts = Counter()\n",
    "all_reviews = positive_tokenized + negative_tokenized\n",
    "for r in all_reviews:\n",
    "    for word in r:\n",
    "        total_counts[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IiMud9kdi07u"
   },
   "outputs": [],
   "source": [
    "vocab = set(total_counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-WKETgG_i070",
    "outputId": "518d6b56-0884-493e-c7b5-aafd8ad5e621"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I8DCTKf7i078"
   },
   "outputs": [],
   "source": [
    "# Create a dictionary of words in the vocabulary mapped to index positions\n",
    "# (to be used in layer_0)\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1y_wEoNPi08E",
    "outputId": "2754d5c9-a9cf-48f6-9310-ae6dbeaa1f6d"
   },
   "outputs": [],
   "source": [
    "print(\"ID of 'movie' = {}\".format(word2index['movie']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rnt7_kXii08M"
   },
   "outputs": [],
   "source": [
    "def convert_to_bag(review):\n",
    "    bag = np.zeros(vocab_size)\n",
    "    for word in review:\n",
    "        i = word2index[word]\n",
    "        bag[i]+=1\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BLn_ywWwi08S"
   },
   "outputs": [],
   "source": [
    "test_bag = convert_to_bag(all_reviews[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BWXsLsrTi08X"
   },
   "outputs": [],
   "source": [
    "all_reviews_encoded = [convert_to_bag(x) for x in all_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hlrsPRWWi08f",
    "outputId": "b71f0004-a4b6-4008-b62f-0632c1941176"
   },
   "outputs": [],
   "source": [
    "all_reviews_encoded[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hbGb8T-Ei08n"
   },
   "outputs": [],
   "source": [
    "#all_reviews_trunc = np.trunc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "1t-_7UmVi08s",
    "outputId": "f98ff701-a56b-44b1-e802-c58fd0bc5e5e"
   },
   "outputs": [],
   "source": [
    "# display the map of words to indices\n",
    "# print(\"word indexes = {}\".format(word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZPEW66FQi08z"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "positive_labels = []\n",
    "for i in range(len(positive_tokenized)):\n",
    "    positive_labels.append('POSITIVE')\n",
    "negative_labels = []\n",
    "for i in range(len(negative_tokenized)):\n",
    "    negative_labels.append('NEGATIVE')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w__rOGc7i085"
   },
   "outputs": [],
   "source": [
    "labels = positive_labels + negative_labels\n",
    "\n",
    "num_lables = []\n",
    "\n",
    "for val in labels:\n",
    "    if val == 'POSITIVE':\n",
    "       num_lables.append(1)\n",
    "    else:\n",
    "       num_lables.append(0) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V75H_F9Gi08-"
   },
   "outputs": [],
   "source": [
    "reviews_and_labels = list(zip(all_reviews_encoded, num_lables))\n",
    "random.shuffle(reviews_and_labels)\n",
    "reviews, labels = zip(*reviews_and_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HFOEHypri09D"
   },
   "outputs": [],
   "source": [
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a sparse matrix from reviews (where we keep the order of the words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_strings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews=[]\n",
    "for sentence in positive_strings:\n",
    "    reviews.append([sentence,1])\n",
    "for sentence in negative_strings:\n",
    "    reviews.append([sentence,0])\n",
    "random.shuffle(reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',\\\n",
    "                                   lower=True, split=' ', char_level=False, oov_token=None, document_count=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#table'>Back</a>\n",
    "# Train models<a class=\"anchor\" id=\"train\"></a>\n",
    "## Train NN 50 - 10 - 1 <a class=\"anchor\" id=\"train1\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RsW4fg5wi09R"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(reviews), np.array(labels), test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "94uBoS1xi09X",
    "outputId": "9da410a7-f695-45b2-da68-d6fa1e9224ac"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(50, activation = \"relu\", input_shape=(vocab_size, )))\n",
    "model.add(layers.Dense(10, activation = \"relu\"))\n",
    "model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bJDNCzSui09e"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    " optimizer = \"adam\",\n",
    " loss = \"binary_crossentropy\",\n",
    " metrics = [\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "YvcEJsRxi09j",
    "outputId": "dedcc712-8fc6-41b2-d2d3-516c5bcc81b7"
   },
   "outputs": [],
   "source": [
    "results = model.fit(\n",
    " X_train, y_train,\n",
    " epochs= 20,\n",
    " validation_data=(X_test, y_test),\n",
    "batch_size=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train NN 256 - 128 - 1 <a class=\"anchor\" id=\"train2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IG7nratoi09w"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(256, activation = \"relu\", input_shape=(vocab_size, )),\n",
    "    layers.Dense(128, activation = \"relu\"),\n",
    "    layers.Dense(1, activation = \"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "IOI6rAd_i091",
    "outputId": "a6b9d82f-84a6-48a9-a7a6-aa1a61b1f13d"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tHpN5VM8i097"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    " optimizer = \"adam\",\n",
    " loss = \"binary_crossentropy\",\n",
    " metrics = [\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "FhcPKZnFi0-D",
    "outputId": "d097a476-93b0-477a-c108-203e8a467ddb"
   },
   "outputs": [],
   "source": [
    "results = model.fit(\n",
    " X_train, y_train,\n",
    " epochs= 1,\n",
    " validation_data=(X_test, y_test),\n",
    "batch_size=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "i2T7Tom0i0-O",
    "outputId": "8e4e8522-210e-41c0-dbf3-afc1936e0eb2"
   },
   "outputs": [],
   "source": [
    "results.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train NN with K-Fold cross validation <a class=\"anchor\" id=\"kfold\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_LwIgaq-i0-a"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(reviews), np.array(labels), test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c60zERWRi0-e"
   },
   "outputs": [],
   "source": [
    "kfold = KFold(3, True, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OR8fKYxai0-j"
   },
   "outputs": [],
   "source": [
    "train_data = list(zip(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pV3wG31Qi0-o",
    "outputId": "ea4f389a-27bf-43f8-c3ac-c99bee868e29"
   },
   "outputs": [],
   "source": [
    "train_data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "id": "5sw1JTMli0-u",
    "outputId": "bdabe653-4518-4634-f52f-a74d25bcf036"
   },
   "outputs": [],
   "source": [
    "histories=[]\n",
    "for train_indices, test_indices in kfold.split(X_train,y=y_train):\n",
    "    model = keras.Sequential([\n",
    "    layers.Dense(256, activation = \"relu\", input_shape=(vocab_size, )),\n",
    "    layers.Dense(128, activation = \"relu\"),\n",
    "    layers.Dense(1, activation = \"sigmoid\")\n",
    "    ])\n",
    "    model.compile(\n",
    "     optimizer = \"adam\",\n",
    "     loss = \"binary_crossentropy\",\n",
    "     metrics = [\"accuracy\"]\n",
    "    )\n",
    "    K_X_train = X_train[train_indices]\n",
    "    K_y_train = y_train[train_indices]\n",
    "    K_X_test = X_train[test_indices]\n",
    "    K_y_test = y_train[test_indices]\n",
    "    results=model.fit(\n",
    "        K_X_train, K_y_train,\n",
    "        epochs= 5,\n",
    "        validation_data=(K_X_test, K_y_test),\n",
    "        batch_size=1000\n",
    "    )\n",
    "    histories.append(results.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "colab_type": "code",
    "id": "s5ouE4n7i0-3",
    "outputId": "b3130c3d-eb48-4626-e21d-4c3ea7041bb2"
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(data=histories)\n",
    "for col in df.columns:\n",
    "    df[col] =  df[col].apply(lambda x: x[-1])\n",
    "plot=df[[\"acc\",\"val_acc\"]].plot()\n",
    "plot.set_ylim([0,1])\n",
    "\n",
    "means=df[[\"acc\",\"val_acc\"]].mean()\n",
    "print(\"mean acc: {}, mean val_acc: {}\".format(means[\"acc\"],means[\"val_acc\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train RNN <a class=\"anchor\" id=\"rnn\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#table'>Back</a>\n",
    "# OPTIMIZE<a class=\"anchor\" id=\"opti\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize on dropout<a class=\"anchor\" id=\"opti_d\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PEMT1mvwEUXq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wn6ISenGq7Aa"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(reviews), np.array(labels), test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nzc1nQ7wq7Am"
   },
   "outputs": [],
   "source": [
    "kfold = KFold(3, True, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V2iZzqKVq7Ax"
   },
   "outputs": [],
   "source": [
    "train_data = list(zip(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tRc7u0amq7A-",
    "outputId": "75b4259b-9958-4510-ddc0-67d1011deccf"
   },
   "outputs": [],
   "source": [
    "# train_data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8VNkRI_f36lv"
   },
   "outputs": [],
   "source": [
    "from pdb import set_trace\n",
    "\n",
    "def getMeansFromResultsHistory(histories):\n",
    "  df = pd.DataFrame(data=histories)\n",
    "  for col in df.columns:\n",
    "      df[col] =  df[col].apply(lambda x: x[-1])\n",
    "  means=df[[\"acc\",\"val_acc\"]].mean()\n",
    "  return means\n",
    "\n",
    "def trainModelWithDropoutOn1Layer(epochs_nb=5,rate=0.0):\n",
    "  histories=[]\n",
    "  for train_indices, test_indices in kfold.split(X_train,y=y_train):\n",
    "      model = keras.Sequential([\n",
    "      layers.Dense(256, activation = \"relu\", input_shape=(vocab_size, )),\n",
    "      layers.Dropout(rate),\n",
    "      layers.Dense(128, activation = \"relu\"),\n",
    "      layers.Dense(1, activation = \"sigmoid\")\n",
    "      ])\n",
    "      model.compile(\n",
    "       optimizer = \"adam\",\n",
    "       loss = \"binary_crossentropy\",\n",
    "       metrics = [\"accuracy\"]\n",
    "      )\n",
    "      K_X_train = X_train[train_indices]\n",
    "      K_y_train = y_train[train_indices]\n",
    "      K_X_test = X_train[test_indices]\n",
    "      K_y_test = y_train[test_indices]\n",
    "      results=model.fit(\n",
    "          K_X_train, K_y_train,\n",
    "          epochs= epochs_nb,\n",
    "          validation_data=(K_X_test, K_y_test),\n",
    "          batch_size=1000\n",
    "      )\n",
    "      histories.append(results.history)\n",
    "#       set_trace()\n",
    "  \n",
    "  means= getMeansFromResultsHistory(histories)\n",
    "  print(means) \n",
    "  return means\n",
    "\n",
    "\n",
    "def trainModelWithDropoutOn2Layers(epochs_nb=5,rate=0.0):\n",
    "  histories=[]\n",
    "  for train_indices, test_indices in kfold.split(X_train,y=y_train):\n",
    "      model = keras.Sequential([\n",
    "      layers.Dense(256, activation = \"relu\", input_shape=(vocab_size, )),\n",
    "      layers.Dropout(rate),\n",
    "      layers.Dense(128, activation = \"relu\"),\n",
    "      layers.Dropout(rate),\n",
    "      layers.Dense(1, activation = \"sigmoid\")\n",
    "      ])\n",
    "      model.compile(\n",
    "       optimizer = \"adam\",\n",
    "       loss = \"binary_crossentropy\",\n",
    "       metrics = [\"accuracy\"]\n",
    "      )\n",
    "      K_X_train = X_train[train_indices]\n",
    "      K_y_train = y_train[train_indices]\n",
    "      K_X_test = X_train[test_indices]\n",
    "      K_y_test = y_train[test_indices]\n",
    "      results=model.fit(\n",
    "          K_X_train, K_y_train,\n",
    "          epochs= epochs_nb,\n",
    "          validation_data=(K_X_test, K_y_test),\n",
    "          batch_size=1000\n",
    "      )\n",
    "      histories.append(results.history)\n",
    "#       set_trace()\n",
    "  \n",
    "  means= getMeansFromResultsHistory(histories)\n",
    "  print(means) \n",
    "  return means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9s4XxkFy2gaU"
   },
   "outputs": [],
   "source": [
    "dropout_means=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1289
    },
    "colab_type": "code",
    "id": "jOicSA33xPyy",
    "outputId": "5250306c-805d-413a-c124-779b2ab67f15"
   },
   "outputs": [],
   "source": [
    "rate=0.0\n",
    "means=trainModelWithDropoutOn1Layer(epochs_nb=5,rate=rate)\n",
    "dropout_means.append([means[\"acc\"],means[\"val_acc\"], rate,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low dropout on 1 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q5uhaJ-_rOO2"
   },
   "outputs": [],
   "source": [
    "\n",
    "rate=0.2\n",
    "means=trainModelWithDropoutOn1Layer(epochs_nb=5,rate=rate)\n",
    "dropout_means.append([means[\"acc\"],means[\"val_acc\"], rate,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High dropout on 1 layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1CblTzT_uul1"
   },
   "outputs": [],
   "source": [
    "rate=0.4\n",
    "means=trainModelWithDropoutOn1Layer(epochs_nb=5,rate=rate)\n",
    "dropout_means.append([means[\"acc\"],means[\"val_acc\"], rate,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low dropout on 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q5uhaJ-_rOO2"
   },
   "outputs": [],
   "source": [
    "\n",
    "rate=0.2\n",
    "means=trainModelWithDropoutOn1Layer(epochs_nb=5,rate=rate)\n",
    "dropout_means.append([means[\"acc\"],means[\"val_acc\"], rate,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High dropout on 2 layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1CblTzT_uul1"
   },
   "outputs": [],
   "source": [
    "rate=0.4\n",
    "means=trainModelWithDropoutOn1Layer(epochs_nb=5,rate=rate)\n",
    "dropout_means.append([means[\"acc\"],means[\"val_acc\"], rate,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gz8yOkHOzD0k"
   },
   "source": [
    "Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kWtdXNAAvW_c"
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(data=dropout_means,columns=['acc','val_acc','rate','nb_layers'])\n",
    "plt.rcParams[\"figure.figsize\"] = [17,2]\n",
    "plot=df[[\"acc\",\"val_acc\"]].plot()\n",
    "plot.set_ylim([0.7,1])\n",
    "plot.grid()\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [17,1.5]\n",
    "plot=df[[\"rate\"]].plot()\n",
    "plot.set_ylim([0,0.5])\n",
    "plot.grid()\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [17,1.5]\n",
    "plot=df[[\"nb_layers\"]].plot()\n",
    "plot.set_ylim([0,2.2])\n",
    "plot.grid()\n",
    "\n",
    "# means=df[[\"acc\",\"val_acc\"]].mean()\n",
    "# print(\"mean acc: {}, mean val_acc: {}\".format(means[\"acc\"],means[\"val_acc\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "46vBFEGrzpgG"
   },
   "outputs": [],
   "source": [
    "dropout_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation<a class=\"anchor\" id=\"opti_d_o\"></a>\n",
    "we have similar results, but got a higher test accuracy with low dropout on all layers and also less overfit (training and test accuracies are closer)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Sentiment_analysis_of_movies_(IMDB)_with_neural_net_.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
